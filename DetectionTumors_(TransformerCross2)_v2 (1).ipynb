{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DetectionTumors_(TransformerCross2)_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "too0gbC0IxX1",
        "outputId": "45966a01-dc43-48b5-e0a5-f65051695769"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNvlIIkG1qbX",
        "outputId": "05b18d9a-be29-4643-ec53-794f710f6531"
      },
      "source": [
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import tensorflow as tf\n",
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import time\n",
        "import cv2\n",
        "from skimage.transform import resize\n",
        "from IPython.display import clear_output\n",
        "os.system('pip install tensorflow-addons')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6tJG1bp1EyB"
      },
      "source": [
        "def one_hot(vec):\n",
        "  items=np.sort(pd.unique(vec))\n",
        "  n_class=np.shape(items)[0]\n",
        "  zeros=np.zeros((vec.size, n_class))\n",
        "  for n,i in enumerate(items):\n",
        "    rows=np.where(vec==i)[0]\n",
        "    zeros[rows,n]=1\n",
        "  return zeros.astype('float32'), n_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AgKeBDM1HQA"
      },
      "source": [
        "def get_index(vec_lab, seed=1996, split_train=0.6, validation=False):\n",
        "  train=[]\n",
        "  valid=[]\n",
        "  test =[]\n",
        "\n",
        "  spv=(1-split_train)/2.\n",
        "  for i in range(2):\n",
        "    ind_clss=np.where(vec_lab[:,i]==1)[0]\n",
        "    sz=len(ind_clss)\n",
        "    np.random.seed(seed)\n",
        "    ramd=np.random.choice(sz, sz, replace=False)\n",
        "    train=np.append(train, ind_clss[ramd[:int(sz*split_train)]])\n",
        "    valid=np.append(valid, ind_clss[ramd[int(sz*split_train):int(sz*(split_train+spv))]])\n",
        "    test =np.append(test, ind_clss[ramd[int(sz*(split_train+spv)):]])\n",
        "  train=train[np.random.choice(len(train), len(train), replace=False)].astype('int')\n",
        "  valid=valid[np.random.choice(len(valid), len(valid), replace=False)].astype('int')\n",
        "  test= test[np.random.choice(len(test), len(test), replace=False)].astype('int')\n",
        "  if validation:\n",
        "    return train, valid, test\n",
        "  else:\n",
        "    return np.concatenate((train,valid)), test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAUWhm6R1StX"
      },
      "source": [
        "def augmentation_f(data, labx):\n",
        "  images=tf.concat([data,\n",
        "                  tf.image.rot90(data, 1),\n",
        "                  tf.image.rot90(data, 2),\n",
        "                  tf.image.rot90(data, 3)], 0)\n",
        "  lab=tf.concat([labx,labx,labx,labx], 0)\n",
        "  sc=np.shape(images)[0]\n",
        "  idr=np.random.choice(sc,sc,replace=False)\n",
        "  images=tf.convert_to_tensor(np.array(images)[idr])\n",
        "  lab=tf.convert_to_tensor(np.array(lab)[idr])\n",
        "  return images, lab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxCqrwDz1U3P"
      },
      "source": [
        "def get_data_training(indx, ima_tens, labels_v, k_fold='none', number_folds=10, augm=False):\n",
        "  if k_fold!='none':\n",
        "    sz=len(indx)/number_folds\n",
        "    valid=indx[int(k_fold*sz):int((k_fold+1)*sz)]\n",
        "    train=np.concatenate((indx[:int(k_fold*sz)],indx[int((k_fold+1)*sz):])) \n",
        "\n",
        "    x_data=tf.image.grayscale_to_rgb(tf.convert_to_tensor(np.array(ima_tens)[train]))\n",
        "    v_data=tf.image.grayscale_to_rgb(tf.convert_to_tensor(np.array(ima_tens)[valid]))\n",
        "    y_data=tf.convert_to_tensor(np.array(labels_v)[train])\n",
        "    vy_dat=tf.convert_to_tensor(np.array(labels_v)[valid])\n",
        "\n",
        "    if augm:\n",
        "      x_data, y_data=augmentation_f(x_data, y_data)\n",
        "      v_data, vy_dat=augmentation_f(v_data, vy_dat)\n",
        "\n",
        "    return x_data, v_data, y_data, vy_dat\n",
        "  else:\n",
        "    x_data=tf.image.grayscale_to_rgb(tf.convert_to_tensor(np.array(ima_tens)[indx]))\n",
        "    y_data=tf.convert_to_tensor(np.array(labels_v)[indx])\n",
        "    if augm:\n",
        "      x_data, y_data=augmentation_f(x_data, y_data)\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "796xLgdF1bCo"
      },
      "source": [
        "def get_data(sequences='T1'): #'FLAIR','T1_Gd'\n",
        "  data_dir=('/content/drive/MyDrive/Brain_tumors_v2/Datasets/TCIA_LGG/x_x.mat')\n",
        "  mats=sio.loadmat(data_dir.replace('x_x', sequences))\n",
        "  images=mats['images']\n",
        "  images=images.reshape((np.shape(images)[0],np.shape(images)[1],np.shape(images)[2],1))\n",
        "  labels=(mats['size_tumor'][0]!=0).astype('int')\n",
        "  images=tf.image.resize(images, [240,240], method='nearest')\n",
        "  labels, n_class=one_hot(labels)\n",
        "  return images, labels, n_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXrAFGXC1ffg"
      },
      "source": [
        "from tensorflow.keras import applications as ap\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "1NXUPTVB1ktZ",
        "outputId": "10877326-654d-496a-9bf3-10c3b95d0590"
      },
      "source": [
        "df = pd.DataFrame(columns=('run_n', 'k_fold', 'network', 'optimizer', 'loss', 'epochs', 'total_parameters', 'time', 'transfer', 'augm', 'Class', 'TP', 'TN', 'FP', 'FN','result_mat'))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run_n</th>\n",
              "      <th>k_fold</th>\n",
              "      <th>network</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>loss</th>\n",
              "      <th>epochs</th>\n",
              "      <th>total_parameters</th>\n",
              "      <th>time</th>\n",
              "      <th>transfer</th>\n",
              "      <th>augm</th>\n",
              "      <th>Class</th>\n",
              "      <th>TP</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>result_mat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [run_n, k_fold, network, optimizer, loss, epochs, total_parameters, time, transfer, augm, Class, TP, TN, FP, FN, result_mat]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WqiMXgj19QA"
      },
      "source": [
        "TP=tf.keras.metrics.TruePositives()\n",
        "TN=tf.keras.metrics.TrueNegatives()\n",
        "FP=tf.keras.metrics.FalsePositives()\n",
        "FN=tf.keras.metrics.FalseNegatives()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3IzJgCO4IQZ",
        "outputId": "998b3228-55b8-4e8b-d953-5da647ed4aa3"
      },
      "source": [
        "seq='FLAIR'#,\n",
        "imas, labs, no_cls=get_data(seq)\n",
        "\n",
        "train_id, test_id=get_index(labs)\n",
        "x_train, y_trai=get_data_training(train_id, imas, labs)\n",
        "x_test, y_test0=get_data_training(test_id,  imas, labs)\n",
        "\n",
        "num_classes = np.shape(y_trai)[1]\n",
        "input_shape = np.shape(x_train)[1:]\n",
        "\n",
        "y_train=np.argmax(y_trai, axis=1).reshape(np.shape(y_trai)[0],1)\n",
        "y_test=np.argmax(y_test0, axis=1).reshape(np.shape(y_test0)[0],1)\n",
        "\n",
        "print(np.shape(x_train), np.shape(y_train))\n",
        "print(np.shape(x_test), np.shape(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3142, 240, 240, 3) (3142, 1)\n",
            "(787, 240, 240, 3) (787, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mue62zrf5gQJ"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a3xedpc4-7h"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dsJQcFb5V_x"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        #layers.RandomFlip(\"horizontal\"),\n",
        "        #layers.RandomRotation(factor=0.02),\n",
        "        #layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF5Pq_HW5qfB"
      },
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hutjc105sZ6"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "Eum4XDJj5u-w",
        "outputId": "2725b39a-e9ec-44f9-bbaa-898f0b3a110e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "resized_image = tf.image.resize(\n",
        "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
        ")\n",
        "patches = Patches(patch_size)(resized_image)\n",
        "print(f\"Image size: {image_size} X {image_size}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = int(np.sqrt(patches.shape[1]))\n",
        "plt.figure(figsize=(4, 4))\n",
        "for i, patch in enumerate(patches[0]):\n",
        "    ax = plt.subplot(n, n, i + 1)\n",
        "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
        "    plt.imshow(patch_img.numpy())\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 72 X 72\n",
            "Patch size: 6 X 6\n",
            "Patches per image: 144\n",
            "Elements per patch: 108\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO0AAADnCAYAAADy1tHpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO19WZMbR3b1KayFrYFe2U1SJEWJojTaRqMlHJ4J+5uJmBc/zIMf/Sv8Exx+92/wf/CLIyb8OHaMrcVaxiK1kZTYJHsH0NgKKAD1PbRO9kUis1BoUiKqu05ER3cDVVlZVXny3rxbOkEQIEGCBPFB6nl3IEGCBPMhIW2CBDFDQtoECWKGhLQJEsQMCWkTJIgZMmFfOo7zVKblIAicpK3n29Yi9ilpa/62JBJJmyBBzJCQNkGCmCEhbYIEMUNC2gQJYoaEtAkSxAwJaRMkiBkS0iZIEDMkpE2QIGZISJsgQcyQkDZBgpghIW2CBDFDQtoECWKGhLTnAI7jwHGMseUJziFCs3wSLDZ0ovL/qHW/TERPaoYtPhLSnkOESd1EIscfTjKzJkgQLyRr2gQJYoaEtAkSxAxJuZkYtjXL4OQ4Dsbj8ZnLzch1L9sBgFQqFTzNciruz/15tiWRSNoECWKGhLQJEsQMictnQWDyuf6Ulv2zuH4ST8NiICHtBQQjqBzHQTqd5hoY4/EYQRAk5FxwJKS9YJCETaVSSKfTSKVSGI1GCIJAETgh7uIiIe2CQCdJmGVYfifVXJPKm06njW3opOVnQRBgNBphPB5PnZfL5QBgQirPc08Jng0S0p4DpFIp9Zt/EySaCel0Gq7rIp1OIwgCRUTf9zEcDqeOr1arAIB+v49erzdxToKfDwlpYwKbRNXVXZ20/N8mhTOZjCItfyhJdXACGI1GSKVSxmMSAv/0SEi74CARJTnlTy6XQz6fRyqVQj6fRzabnTh/a2tLtUNVmeqvNESNRiP4vj/xuY7RaATghJicDEhy/p3gp0dC2p8YUsLNM6h5HqWh4zjIZDKKwPy7WCyiUqkgm82iXC6jUChMtHP9+nUAQDabVZJyMBhgOBxiPB4rolLlHY/H6po6SFoAqh/y3hLS/jxISPsMEdW3Ost4BAD5fF6RM5vNIpVKTZCWv3O5nFJxpUSe1b4E+yzdQLY+AUAmk0Emk8F4PMZwOFQTwHA4nFCzZdsJnh0S0p4B8wQmmAattNqm02kjSX7xi1/AcRy4rotSqaRIm06nMRwO0ev1MBwO4fs++v0+fN9Hs9lEs9mcaGd7e1v1mf2W1mGdsKlUCoVCYUrNBoDXX3994hmMx2N0u114nofBYIBmswnf91WfKMnpTkrwbJCQ9mcGiUGpmcvljAS5ceMGAKBcLmNpaWnCaNTv93F0dIR+v49Go4F+v4/hcKjIK7G/vw9g0k1D8kqJnc/n4bouMpkM8vk8SqWStU9S0jabTbTbbfR6PWSzWfR6PXiep3y/vHaSfP/skJD2OcM2mG0WYvljQphEe1qVNUqfTP0yLRv04xJJHB1J5YoECWKGJMsnQYKYISFtggQxQ1K5wtJW1HKksq1UKjV1MNd5mUwGpVIJ2WwWtVoNGxsbcF0XN27cwObmJgDgn//5n1Vbf/jDHwIA2NnZwQ8//IDBYKAMSTL8kP5X3aj14YcfOgDw29/+NgCgXEdaVQpl2fV9H4PBAKPRSBmWAODevXvqhGq1GgBQFu9cLocXX3wRW1tbKBaLWF9fR6FQUAEhg8EAX375JR48eIAgCPDpp5+qtra2tgIA6PV66HQ6cwdpxHlsnaUticQQZYF0hcj/iTBrqIxicl0X2WwWxWJRDW5JoJ2dHezt7U210Ww2lb9zZWVFDWhp+dUNPPSXSnQ6HQAwkpsuJ04E2WwW4/FYBWrouHr16sT/jKZqtVrodDpoNptIp9MoFArK+lwul3Hr1q2ptm7evIkgCNBoNLC3tzdh/dZ9vQkmkZBWIEy6zuOySKVSyGazSKfTqFarKJVKWFlZweuvv45arYZ2u41GowHP83Dv3j08fvx4qo39/X04joNCoYBLly4pN1Emk1EDfDQawfM8FclkIm2r1QJwQtp8Pq/aYMSTLWLLdL8vv/wyAEyEPHqeh6OjIwwGAxwfH8P3faysrGBjYwOFQgE3btzApUuXptp7/fXXEQQBHj16pO6j1WopiStDJhNMIiHtMwQHJn2dmUwGhUIBxWIRrusqyUif6Wg0UoEONkkuAzAo2WQ7pvYkpN82lUopVdj3/Sk3DfugJx3I8wleV4Yzsh/D4RCDwUAFgpgSCwqFAoIgQLFYRLlcVv7nfr8/kdOb+HenkZBW4GkHCVXelZUVbG1tIZ/PY2VlBUtLSxgMBrh3755am5JclUoFS0tLU21xnUtSca05GAzg+z663a6SeGEqZaPRAICJcMhWq6VUZcY1y3MZraXj4OBA3Wcul4PjOCiVSiiVShiNRqjVahiNRqq9breLr776Ct99991UW1Sfr127hrW1NXieh7t37+Lhw4cYDAbodDrqvkykv8hISKthlooYBhJgaWkJly9fRqFQwNraGqrVKnZ3d/Hll19ib28PuVwOrusil8vh8uXLWFtbm7rWysoKgiDAYDCA53kYjUbodDpotVoYDofodDpKikmppLfTbrcBnBqPUqkU2u22kuAksiSHLbSSE0CpVFIRWq7rqphkPjuq/77vY3d3V6noEtQ8lpaW4LouPM9Do9FAvV6H53lK/U8IO42EtAbMIqst8mdpaUkZn6jiHR8fw/M8NJtNZLNZlZHDkMFMJmOUkN1uFwDUmnU4HKoYX0ppmUMbVumC0C20JDpzY/md1AQkPM+bqnrBfnEdTynLv13XNRJvZ2dHSepqtYrhcIhyuYzNzU0lZR3HUWvnZG17ioS0AlEkK903Jkl08+ZNACfrNcdx0O/38ejRIzQaDWSzWSwtLWF1dRWZTEapl47jYDAYTLW1s7MD4IS8rVZLqcJUP2X8MvtiWj/KfvI7SfrhcKjUb+lSMj2Ler0O4ESjqNfrcBxHGdzy+TxqtRry+TzS6TRKpRKCIIDrusYqGP/5n/8Jx3GwtbWFGzduKK3j+vXrODg4QCqVwsHBAdrtNobDYWjSwUULiUxI+yPmtQ6b8k0rlQoAqO8oaff391GpVLC2toZKpaLWl1R/9SB/4FTSdjoddDodNWiZgE7S67mvOkG4NpXSSkpZ3rs0YtkC/Pv9PgAony7vlZlBrusCgPIhc0IxSdq9vT2VqLC2toZisYharYbV1VUAJyp4p9NRhqmoaY8XAReetFHJSnJkMhmsr6+jWq1a14+5XA65XA5BEKBWq6n1K9Vm3/fVOrXVaqHb7U4NSPpXgyBQUksST6qp/J9VLCRo5JKWaibBA5hQl3UfsA6qvLSO6wkEx8fH6HQ6KkuIQSCcYCTo83VdF/V6He12G77vq7Xw1atXsbGxgYcPHyqrsuz3RcaFIu3TWIbpvmEU07Vr16aOYS6r67ooFAqK4KVSaSK3tN/vo9VqKUNNo9GYIi3bKpfLqFarE2VOSTxdgpkIQslFKzMnCtmWPhHYSMsJgcETqVRKqa6+7+Po6Ai+78N1XRSLReRyORX5pbdHny+DK8bjMfb395HNZrG6uoo333wTtVoNn376Kfb29tSEaFprXzRcGNKelbA8j2F7uVxuwl8qoUstqb5ycDNxfTAYTPzYri2NTfIaus9UVrSQkBFPsraxdPPolmcbabk+lj5jXo/rad4j74n3rbfH5YFcP9PIxnNp3c7n8yqgJPHbXiDSnsUHKytMVKtVbG5uIpPJoNfr4d69e8bjqaYyYICSzfM8HB4eKmswXTaMZtJRq9UAQBl29PuQarCUlPr6WK5TdSu1TlTeA9fcOuhb5RpaThY8h1bobrerqlmErY9d18Xm5iZGo5FSk+v1Oh48eIDDw0P0ej1cuXIFnudhe3tblbQJw3lf/14Y0gLzE5eSktbQ1dVVpFIpHB4eTpV1AU6lXzabVYHz/X4fnueh0+ng4OAA3W4X/X5frWNtMbaM/ZURR9IQRQOQNC5Rqun3LH/LezN9JgmoPytOFPJz3jOfE7UKGtg6nY7R0s61aS6Xw9LSEobDoZrcUqkUdnZ20Gw21RKDEwB9xbNwnol7oUj7tDCF/M0LvehZFKkx6/tnMTj165iua+tLWB91S3VUzJOgYYPuoz4vSCpXJEgQMyRJ8AkSxAwJaRMkiBnOfeUK01poPB5HqjZBI0kul0OpVEKlUoHjOOj1esr6+dVXX6m2/uEf/iEATjeo8n0fOzs7qNfryGQyKgGeLh8aX0ScsWrr5ZdfDn68b7UeY8jgj/cw4arRXUOff/65AwBXrlwJAChXk7Q0ywLjhUJBWbyz2axKfvjiiy9Un955550AwET0FI/tdrvY3d1Ft9udCrrg/+12W31x+/btwHEc1Go1rK2tqdDQdDqNVquF77//Hp1OB2+//TZ+85vfAAD++7//G3fu3EEQBLhz545qK5fLqWcV5se1LQUXYZzOakviQhmizrJ+t/ksoxhGol7v57Z06qR6ltfW2w6DbpSztTUr8+ppjYNxw4UibRTIhABmnZRKJZUiBwDFYhErKytT59Lf2u12VXTQaDSC67oTUpD+X/pbTX5aukTksdL9o2f22AYs9/YZDocqo4hbech9gvL5vMqxteXTyn7y+myL7dOvTF8un4EOmTJIac2IrvF4jEqlopIN7t+/r7KIbvxYMF2CMc+zStWcF3Kfe9JShZxH6jHJe2lpCdevX0etVsOjR49weHgI4CRBfWtryxgRFQQB2u02njx5gtFohFKphGKxOHEcByDVOZP0IGmlqmqKWmIUkk2q0d9L9ZiBD6PRaKJAHKO9wrbGlHWqSFrWdWJtKWYwkUidTgee51mT8xkPzbTFYrEIx3GwvLwM4CRK6s6dO8hkMrh69SquXr061Tc+X6Ywzkrli7sP99yTFph/ZuXAY1QQAxpkwDyJJKGH7unpZCa11BR6KKGT1OZ7tEkXfcNpuQ6mVGWghr4TXhikRJMVL4DTbUMYpmjbjZ7tMD2QE4vcAoXJFdyGxNQW35NeQifOxAzDhSBtVDACqlarYWVlBY7j4OHDh9je3ka5XMbt27eVOkdVWeL7778HcJIsThVzOByi2+2qnFMZAwxgKrKJ4LaUHKScOJiwLlP15LpQV7V1kpO0svIEq1fI+GaTyi4/4zW5Ly7/llFkAIzbbwKniQwA1M7zQRDA8zz1nJgkQRWbObz6xLK+vo4gCCb2NTJFh+l9jysS0mpIpVIol8tYW1tDu93G9vY2Op0Obt++jRdeeAHZbBaNRgPtdnvq5e/u7gKYDKinhZaB73qwvk0SyWqJpnhhE7FMRLORllKWajG/ZzumgW36nuSSmoh+HybpzZRBz/NUSCf3yWUVC8Yyc6lgKlsDANVqFQBUah/b0nOIzwsuBGmjqHxSZaRBZTgcqqqBcgAxllgfCHoWDNuVUgyYHkBPa9XWXT86dFXWlDigh1bO6pNJ7bctBWxqO7UHmShPLYJqsuM4as3K5Asd1WoVQRCg0+lM7Jt7XnEhSBsFUmUsl8sqeZ0kpQU5CAIcHBwYC4xzcJnWVRzEetFxm1QLG/T6Oliqqvr6WKYLUvqYisFJldumHtv6xDaojsp+2Mgv1X5ej2qyrMZYLpexurqK8XiM7e1ttXWnxEsvvaQmmr29PbUs4RrXFPscZ8l7bkk7b4C5bpyhZC0Wi+p/GpekSichpastAN8kxZ6V/9gmaeV1bdJWPyZqv3SJL9sJg8n9xXUtCcy2i8WievYmDadSqSAITmooZ7NZDAaDUONe3HFuSTsvaFBJp9NoNpv44YcflPGoUCggnU7D8zz4vq82TraRFphOiTPN9vzcJNWk60X+bSOWTT2m9KMEo+pJDYJVIqVvlVqHDVzH8lp0OcniALPUY6lxcIKUkp+uJJmiWC6Xjf5x7hVUr9fV2pr9m/X844iEtD+CltRUKoX9/X0cHh5ieXkZr732Gmq1GjqdjiqH2ul0jJJWFnSTlktpwAEmpZNNFdXzVKl+ymR2XfWWlRkJ9kPWpaJ1dTAYoN/vI5VKoVQqqR3nbcYxggQPgkBNANJNI+/ZBvksWE+LE9RoNFK+ZBqZMpmMKrujt33nzh0AUBKW1UXYvom4cSbwuSXtPDMrpQTrK7GeElU0lmxhaRhbOc95LZVh6qzun30azGNUinpNk1Xa9P2sa8v7lEYpkwWYBkId3OGPxwCYkrTnCeeWtFFBKVapVHD58mU4joPd3V21qRSlW6fTwc7OjqrIwIgfCVMpVN3FIyWKvL7NTyvVZ30QE2yTx5oIR1WfbhUAE0kM+XzeqFbaoBOLxizer57YoMNkSJPhk5Tk8rkWi0Wjz5cW5aWlJaytraktVPQqF3GWrhIXnrQcpKVSSTnpGTcsQ+J6vZ76nBFTOuhmICl1K7Je+RA4Ja0OSgyqs7pVN8wAJEkiLctcO1JTYPYS1UlJ2DDSyuuYSCvXtrZtPaR1WRar4zlyG87hcKhqK5skLTOLarUaqtUqxuPx1HE2e0IccX5NbBbMo/aZoBt+9O/iNhDkfSyKJDrLkuAsKn5ckZSbSZAgZrhwkjZBgrgjIW2CBDHDuSg3M8uRr6/ZRqOROuHKlSuB4zi4efMm3njjDQRBgE8++QTffPMNyuUyrl+/jlKphL29PWxvb6ukdhqi7t+/r9q6detWAJwmdgOYMh7xt2kdKUu7vP/++4E8nwYZ2w4Derts61e/+lUQBMGUQSsITpIVuH2JNAjJcjKyT2+//fbUc2cOred5ODg4gOd5aiMt1n1myqJs6913350qpyOtxQxjrNVqWF9fRzabRa1WQ6lUguM4+Nd//VfV1htvvBEAJ/sDvfTSSxgOh/if//kf3LlzZ8LwZoMsP7SoY17iXFiPo6zLbbmi6+vr6u/Hjx8jCAJkMhlcvnxZbcDMAuPFYlEN9rDgA5KEf9v6F2YAoiXaFkgh25gVsscQQQaEkGj6M5FxumHWVlPUEwnM5yMnB1Nb8t51a7dMGRyNRjg8PFSJALYdBhlFJbchkZlWsv9xx7kgrQ7bIDER7cqVKwBOfH3c6mN1dRU3b95Eu93G999/j3a7jWKxqDaNtu1QLiOfbCTTYQryBya3rJSDzURaGYVkC3LwfV+VeCmXy6pKhWkys00EUspLlxZw6gdmlpT0F4c9A73/bIvEb7fbODw8hOM4EyV/JFqtFhzHQbfbnShel8lklNZAIp8H4p5L0ppgIwdD6ABM7IWaz+dV2J++mbNNetrU9LO4HuYdXFEGpAzumOUSieIa048h0aJGQ81qn21wAmP4pQ6+Gz34hAUDzhsuBGkdx7EGRDDCptFowPM8NRhd11XhcVwXMbbVJonkQNNVPhlwIT+flXBuUkN1dZmSSq5FCTloqS7Kneipyss17az7s0VccfmgB1+Y7o9qru1aUmshKRkvrV+f5Wr6/b7a1xeAkvxR6kbFCeeStLpqyoFt2gnOdV1FKhpMKGlZiUFPzGa4nw6dtLpUk5JahibOWj+afttIHVbNQpJWV6mlwYzrXRtMklpWrmBd5zCS6NFjpnuRpKfEJUElSGomRfB7TtKy3fNA3NiTNooax7Q7VvqTIFFpAAFOrZeyuh9neapcPFbCtD+t6UeqkDY1Ncp9zVLRJWQGkO16NvXe1La0+OrrVz0TKYpFn23pElpafjlx6uAWnKlUSq15+T7DKlXGFbEmbVQjD90Fa2trU+e0Wi01SMrlMoIgQLfbxaNHj9But9W6lrm0tJKaSKtvhiyJIK2ZNGbJPtruTUoI3eIaxYAkDUVUifX0NqkFEFHrTfFv+ZktsUFC7nGrJ0TweY9GI/T7fdWO53nGfm1tban+sbLFaDRCoVCYeO7y2cl+xw2xJW3U2VNKWs7IElLSMqCeFkjP85RvU7oS5ECTIBFNyeDS2vqsJa1NVZbt6JLWpn7L9sL6xHvQJa4kbth9yMqPPF+ea6qoaFKNgdMyP9z3Fzgt42pLyIgzYktam/9Pgu6HXC6H5eVlbG5uTp1zcHAA4IRoS0tLys1wfHwM3/eVa0TO2DY/LTNL9MoOYWsqG2nl9/rfupQzGaXkM2CfuKY3rWlt7dv6Yjpefh/mx9bbkFZnuR6VBjZ+b5osS6WSmli5lnZdV03StvRHW394T4uK2JIWmCSuyS9KldB1Xayvr+P69etTA+/Ro0dwHAcbGxvY2NhAEARoNps4PDxUFfhZJ8pU6V+CObazjEe6ZAyTtGGE1teMJvVY5qfyWrKYuHShzLOm5d96mZh5NCC9TarHuhSmtiM/k6hUKgBOgixYBmhtbU1NwnIps8hkjIpYkxYIfwmcxTlgTRJERh5xxgcwYcAgGfQcWR1ha9N5Mc95YVqH/rkkmFRxn4UK+aza4PPm+6Pxz6YBmIxeDPaQQSS6Sh9XxJ60gF1tc10Xy8vLyOfzqNfruHv37tS5XCfJQSH9ndI1YSrGLWGS9nKA6IYeXSWcB/I8Wm9Nk4YslUMfJusoyXP1fke9P7lmlq6iWX5ameSvxwbrE22/358IctFB41On01GGx5WVFWxtbSGXy+HRo0eqVJC0Ls9aliwquc8FaSXkAMzlcqhUKshkMmi1Wjg+Pp46XlZdIEhcYNK4dBaYDD4ma+s8MElP2wRACUOLLADl+tLX3PMMVN36LEkrJZ5MctD7BUwmRMiyqrJel/T/mtpqNpsAoIrUZTIZVCoVrK2tqTK47XZbveuoEndRiXvuSGuDTbWKcp7t86dRBxdxMDwLPEtDTtTne16fpQ1J5YoECWKGJAk+QYKYISFtggQxQywrV6RSKWtbtPKmUincunULb731FtLpNB48eICdnR0AwN27d1VbrHpAYw1rHPd6PWSzWZTLZWQyGfi+r1L3SqWS8sl+9tlnqq233npLtSX7Y/rb9Nmnn36q/mHFiVnRRey3HvTxySefOADwwQcfBMCJkYZZS8ViEa7rqugv3SIrjVSyT++9995UtQlbn0w2BNnWb37zm4kqH/pamMapdruNZrM5EQ8OAK1WS7W1vLwcOM5J7erl5WUUi0W89dZbuHnzJnZ3d/GnP/0Ju7u7apcIfVcHWclkUce8RCwNUaaoItPfpv9t5y7a2n7R+hMnzAoSiTtiSdowyIAK13VRLpeVS4eVGySWl5cnZvTRaIR0Oo1yuTyxoxud9cBpcvWsCUH/zOb+mWUltU1Ks65n+k4PPdT7JwMaTO4jYDJe2HTtKJZ603ORkP5f+pozmYwxvdLUd+Y/j8dj5HI5FAoFlfARdt044FySNpvNIpvNolgsolarKRXz+Ph46kUxdHE0GmF3dxej0Qjlclklx1NNkwPGFgNrq/5vku5RI3SiBADMEyQQFlVE2HKPbVlAUQlogt4ffYKS6jNrRtlAH206ncZoNFKplUwWkbm2cUbsSau/ZBJC7gtD0pqic6RDX34mdyeXbUfBs1TBf05/57OAaRKyTRIytFSfwPS1bRjkxMX3xBS/8XisiG9L0YsbYk9agi+DZU9YkaJer0+UitFfFMMUud0j1WPbRldhYGges00AqMECTBdGC4Mtn3XWmj0MJqnMCU1GRTE1ToetwqRUZfUJSk+vI+Rzl6Vb9eR3GgDH47EKRbT1y3EcFW/s+z6Oj49VFc10Oo1ut6uKztkKvcVBfT43pCWYmUP1ttVqqbhV0wCXyeiu6068zDC1z9SWJCIHpQztk4NkVtjgrOvZyDrvWlJKOfl5mPpvAlVZvR2ZoSMhJwC2OxgMJvKXSV6GQnJTNFNbfJ4sUMCSsePxWJVjLRQKasKcle+7yIgFaaM8WClpmU4XBIEiLXCaLC3BpGm9ptFZZllTPSZbidKoCHMTSelog5x8TAXO9XbPOoh5r1KbYTyxzS0EYCIVj23oifGmgu+mfstlEQsZMI45n88jn88rQseRrEQsSKvD9uJI2NXVVVSrVbRaLTx48ECta1iWROLJkyfqb9u6Sr+uzZgjM4Io6fVk+LB7kLCl+emqqC4V9X6xphKNaTzXZD2Wk8Asn7Kpv5RyVHHDqvvzWUnpLC2/TA6QkpaSVAeXH3Lt2mq14HkeKpUKXnzxRZTLZdTrdeWfjrLzgOm+F0FljgVpoz5YSdxCoYDj42OV3VGtVkMlre5OMLlCZkFWMzSVOTmrhDVNHvoEEEXNfhpJauufVPdlzrFcv4dJWqmeyzW1PE6vZWzqi3593/fR7XYnVON8Pq8CUEwTaVwQC9LOAl8YS8uUy2VUq1XU63Xlq3McR6V7STBdTZbbjHI9E6SaNwtR1568N+BUEklDje77jdJvXeWMCpMhT/ZXEkySzFQET54n/yYBmU8r3U82DYe+eMdx1K7wnLALhYLysfNnOByi3+9PXT8uOBekBU4Tp5n4vrq6ip2dHZVAzZ3EdVDS6tURZ6m0YYYoSY4wg1EY0UgoGeggjSxy1wO9XQnTfejSyyS5TdBdaLI9TiR6G7bCaibLPDUGea/S0m4jba1WU/dQr9eVT77b7aJcLquNxqiBjUYjta/RPFgUgp8b0kojhGm9Js38EvOoSfNINXnO06pg+lrTpJo+C8yz5tYt4eyPyZh3lv7Jtbt0mZmII4vUcTLTqzlSasuqGFH7sGhYWNJGfWBUH3O5HIrFIlKplPLNdrtdNbN6nqfKkkisrKyodkxWWX7GgQCcbhClI6xWcBSXkYRJVWRBNmnoMflGw56TbB+Y1A6iGsdsxdx834fv+5H80JKUJglumpxsKj3P9zwP3W5XvW+Sluel02lVuZFRVvNIz3mP/6mwsKSNAkmybDar9m5ptVrKKS/XMFzHSLCSn9zcSScvMElanQCEHFxhUkuX7mEDgYNaahLUJvT6Tjb10dSPMNU4inosP+O1pZuF69hZlmj5LKVP19RXW1Qbj6VvVpJVBmzQSDkcDif6Fzecq3zap3kB81pWTeR4Wj/v80YcB7COeV04cURSbiZBgpjhXEnaBAkuAhLSJkgQMyxsuRl93TEej9UHLDdD44zrunjllVdw7do1XhfAqfHI9308fvwYh4eHCIIAOzs7qq0333wz4LG6kd9VCcwAACAASURBVEO3jprw5Zdfqi9v3boVAKfRVSZXjW6BlH+zRAwA/PKXvwwAqLhZafhitM9wOJxI1JfP7eOPP3ZkO/JaJiMQM3scx1FRRI7j4E9/+pPq09/8zd8EAFTRbxlbbHKz6ff3+eefq7befvvtqfFgCh6hUUt/bh9++KFqi6V5ut0uWq2WupfxeIzNzU389V//NTY2NtBqtdBsNtHtdvHll1/i+++/x4/nTY2tMIQtKZ9mzIe1JbGw1uMo/k0OunQ6jWq1io2NDXS7XTx58kSFsJVKJeUuCdu2Qw90t/UhrE8ytnee+4piV5ABEaYgBh4zr/XY9JtW1zDY2tLvz+YmMVms9QlAWsfD7k2PDKM3ATjds7bT6SAIAlWonSGNcbTpLCxpgdnE5YvOZDJYWVnBCy+8gN3dXdy5cwd7e3u4dOkSarWaknzcdEpC+lv17JJ5AzH0YPaw/us+zyjgFhrSTyr7E5W0Osn0v2cRVkJGMunkCYPsu3wHso/0Q5v6KKEXK+BEzh/m0RYKBVQqFVV+JpvNnom0z9tfu/Br2lkPRxI3l8up3b9llcF5BrVJHY76gsJUaf3aZ4nI4UDWiX6WAaRLWFNfZ7m1ngVsz3re+5N9plrNZYNMpM9ms2obzShuvkWUxAstaWeB2RuMK+XLYKUKmTQ9GAwmdhUndPU4bE0myW962fpAM83IHFBSIoWp7XJgca1mkpCzJiRb+2xDRhuZitYBp8EPumST/dSDPWZdX29Pajl6ITkbODn7vq/qQBWLRVWcb39/H0dHR3jppZdQq9UwHA5RKpWQy+Ws42GWOv48sZCkjapacf/ZfD6virmxVIweeyo3HDa1pRuKTCSWRIs6+9s+N6m3YffLwW2q3PCsqjCY1pISfJYmg50+GcnPo0ozvcqjnKzC2pD72LKsECuRDIdDNBoNjEYjXLlyRYUxFgoF43IpDlhI0kYB10Ku6yKfz2M8HqPX66kaQnphN9tai8eHGU9mGVKi9te0Bo0C06xvksSmfsm81ijXsV2PbZsmLhNR9c+jXNf2N9vket7ULwBq3eo4zoTWJcMZ5TlhqYZheN5r2liSlgOwWCxidXUVxWIRvu9jf38fzWZTpWLR0BD2gI+Pj9VLzufzU2qezA+1EWWefpsIa2pLXsNk5DG1ZUo4lwM1TA2XIEFM/ef3s1TWWfenE9u0jJDPPAgCeJ5njB9nv0qlkqoDVS6XkcvllNuv3+9PxI4zT9d2H0+rtfyUiCVp5cxqkrTSxTOLtCzHkkqlFGn19ehZZ1WTW0NaWvVjbOfqBNDbMl2LkHuy6u2FGc3C+jTPgI5ybNjzle+CBkbbNWRCBZdKrDmlW935E0UDsd3X85K2C0naWesgPvB8Po+lpSXk83n4vo+joyN0Oh1ks1mUSiWkUin0ej1lmDIRmIMgl8tNZNDIl6u/2CiS1mQVjbK+MyGK1dm23p4laU3t2O5Pn8Rslt+nkVL6OjgITiszUv219UtP66OFWNacojbCvNqzEi9Rjw0IG+B8Ka7rYmVlBdlsFo1GA0dHRxiNRqrqHrcCoXpkUvm4pjWV1wTMQRe2ukdR1qrzWnp1qaxLYP17vV3p9ppl/JrVJ31NaHpHOnGj3Kt+jq4pcGJl+qWtX3x3nGgZScX3z5xafhfmp9X7Pa8d4qfEwpI2DHImlRUA6dLhd9KKDMCYuC6lj8mVMQ+e9Qs1qchh/TJd32RcimrVjdo/m4FqHpj6Y/o/bF0u19pSYzBNlHq51yj9WxQsHGmjDCS+DBZxY6ja/v4+crkcarUastms2h6R0tc0Sy8vLwM4IWy9XofjnPj4eKz0Xc5yrZhm5jCJHNaWHIiysLe8lj6QwgqMy+NNRjVpTY1yf3pCvG68mzUBysR3qQ3ox3J9yu9N5OFkzO08uYGaDnmvdBfOWr/b7v95YqFIG3Xm5wBjcAVwouY2m00Ui0UsLy+rF9Lr9TAej1GpVIwlVPly2+02jo+PAUCVrtH7FNUiGnXiCWtLfq5LD5taHEZkUxQV1Xz9R79X/f70Puvr4DCVntA1HNtaOkphN1lDqtfrIZVKqeQH26T5tGva54mFD2OMAtNAm2cCIOL4AhOc4lm8v6ddMvwcSCpXJEgQM5wLSZsgwUVCQtoECWKGWFauqNVqQSqVwrvvvovf/va3GI/H+OMf/4iPPvoIKysreOutt7C8vIyHDx/i66+/xmg0wvLysjI6yQoRt2/fDgBMhLU1m020221liZaBFz/2Ra2fPvvsM9XW66+/HrDvJuusrdo+8dFHH6kv33///eDHZzgRPCBrHtN4Y8qG+ctf/uIAwI0bNwIAKiVNrv+lL1O3+PKaX3zxherTBx98EPoOeT7zfmU7elu/+tWv1P2F5e+afNPA5HPf3NwMgJNAmcFggFwuh9dffx03btzA4eEh/vd//xf1eh2vvfYa3n//fWSzWRwcHKDZbAIA/v3f/121lU6nA3kd/e9ZeJoxH9aWxEJZj6NCbiMpLaDApGWQznhaEk07DEgrJkucMKJKbqQlfYD6eTbYAg/CjtGh+x718+T5pkmB7hKGdZKgbE9aeU11jaNAP176R21uGpP/2Ab5fk3PvVQqAYCKS87lcmoLEFaoAIB8Po9qtYpMJoNmsxk52d/k104iouYEBycrE+h7nFKq5PN5FAoFtaG06SVJdwEjb3ielI5yWxGZQTQPTLO3LbpKDmq9UgXv03QvegAJNxZjUrge3SUjiKLmr8rrh1XeCAu0kNcPI66clG3PqlqtKvce77lYLKJUKqHdbqt3WCgUsLq6ikwmg93d3chbXdru/XkRd2FJGyWMUfctyu+lFLa9bBvCfJZRQxDDgit0RAl75N+2Ptmehb4pWFh/nlWfo2CW+ql/NovYVPd17Uuf6OTENk+pn0VyBS0saSX0B6bvsj7rh22YpKOMxpFbMsr1GWEjjamfetQRf5tULRNmxfnqqq08Vv5v2r8nDBzYUScSvW09yMNGNl3Syr7L46kB2SYlHiP7Lis6UjvSCS2TSOZF1In7p8LCk9YWTWOTsPN8DpySUp+BbetIk9SL0md9ApGYteYzwTZwTPce9ZxZE5xsxxTSKQk7i7S2dTV/myK8bO9QHiNjimXiuz6RczKIIuUXDQtP2rNAvnBpcbWRSZ4nf5swjwo5qw1eK6xfJvV4HoOWbvGWP/IaUTWAKAjrp+nYsDWx/tvUPyaEsCYYJ3UaEiXk2j2uiCVpTYndwOkAoFrEQuXD4XCieLiEHLim+kv8zrZOsh1vI0HYellCt+TqRNCtvrb2qPIzMF8nrkki2u5Pl3ryb5tktEG/ri5l+b+tGLtEu90GcFpE3XVdpFIplEoluK6r1GG5ppV9jhtiSdowiWCSslELn80atPOcH1VqhUnOsHXjrD4Q0hBlIqrp/GclccNguzdCPkM5KYVJWlnMT5e0bF9PQJi3n4tgkFpY0oY9HFmJXkoQnkMpS4nMz8Oq+sl13DwSQ++X6ZyzrJtMgQ6mNqJIf9v9hBFhVv909X2WYWpWW2HGNJLStksEM7IYKCLL6coSNKb+zCtpF0EyLxxpoxhpZJDAYDBQ0lTmaA4GgwnLISVv2NqPs7K0Opr6ZCOJae1lUv/kfc1SRU0DLIwoYRJMV4Fty4WwSSLq5/zM5lbRSar3ie+YSxwAKoJLx9LSEoCTneDT6bTKnc5kMoro+hIiKvlmTb7PAwtH2igwDV45AClpuRbVjS62tiRMhJ3HYBO2Zp3HkDSr/ahrZB47a+DNY0TSzzP9PQtR+xTWtlz/8lnI5ZF+HNsJm1AWhaAmLBxpoxCCFkJKRJKTu8t1u111XLFYnEgE1yFjZOUaiuF/0jXBl2wrU2KSqLrBI+pg0I/XjTOErvrpxjQ9gV4eK/sm7ymKsU3203ZvsyYQ/Rg+Y/m51KBmFWsnSUejEbrdroohl8skXnc4HKqd/+KGhSOthO2BkjwyMJ1EcxxHvYzxeAzXdZUaHfbCeT2qybKqgcmAE6YqhoUmmtTCeZ6DjPJiG3Ig6qGas9bmJpXZhlnaw7MgANuQQS9hk5K8tkym8DwP7XZbVS7R7+sswRVRtJmfA/F2WEVEFMnxvF/EWbAIlsxFwNMYluKIpHJFggQxw4WQtAkSnCckpE2QIGZYyMoVprWarFxRLpcDx3Hwzjvv4G//9m+RzWbxww8/YHd3F57n4ejoCP1+HysrK9jY2EAQBNje3sb+/j6CIMD9+/dVW2+//XbwY1+VEYN723IbChqlTL5HVogATitXyHsw+W5tCQ8ff/yx+uDNN98MAChf46ywuyAIJjaZunv3rgOcVoiQMdgm45Tsi8wXln169913J56V/ixmubI+//xz9c8vfvGLgM9C+t1prJOZXHpUEwB8+OGHqq333nsvCIIAjUYD+/v7yGQyePXVV3H9+nW0Wi3cv38f7XYb7733Hn73u98hCAL8x3/8Bz7++GOODdVWKpWauIkzuL7ONOZntSVxriXtPC4IHYto5JnXXXRecZb3uojv86xYaJePDXxJw+FQmfSLxSI2NzfVbNvtdlEulydCGG3uAv6Wbga545otkUCHLUpKOvsJPTl91r3yfBnlJYMA2E/djREWsijboSQ+a5BH2PdR2tR93NL3Ouv5y4QBaiaU1NlsFtVqVVUjkTvoRUmCTyKiIiKKbxE4eUl0nlcqFayvryObzeLu3btot9uo1WqqDe6cZpul+TKByYAF22bEUSOieLwcLHotp6jEZUA8B6MMz6Ra73nexLl6MIXeN6recivIWbCFPspJRPp+Z+Xm6sfqPleTz1ai0WiotjKZjIo7ZsDN6uoqhsMhKpWKmgTmuV/9Pp83FpK0syAd8L7vw/d9lEol5HI5Fclk2o901uCRMMWpynWlTarq/9sIedaXL6+tr0/1IJBZ1zFJ36h9sx1jGthha92wsFBdQ7FFovF7JgjQBsEdEXO5HHK5nPrctr6PCxaGtPOoZVTjer0ejo6OMBgMUK1WUSwWkcvl1GzKkprpdBrFYhFra2tTbUlCSoOHLeonrJ+2LB+9HIokna09k1omwyvDIM9lhUI9skiSYV5V0TQh6tfldRgDrkN/1rp2Je9Tho+aJl4+01KphGq1ilQqhU6ng2+//RbVahUvv/yyGh++78PzPGVsTEj7M4Gk7ff7aLVaAE5efj6fV3uOMmmA66F8Po9arWaVAnLw6VX6oq5rTKSVZJDJC2EWV3m+vLYpU0X+mCYAShtTWpsk7rzGHUkkW395nGlNKjOqTKSVNgV+ZtNc2FaxWMTKygoA4PHjxzg4OABwkgW0tbWlxgQJa5tQTFgkcseStMCp4ajf7yOXy6Hf7ysjTKlUUjvEc01kk1JyHcuXf9awR5NaGnZcVAONqVKHjbx6m7wn29LAtAYN64vpnmxGrjBI95ecyEwEplprk7RsiwXn2R4JTyNUt9tFp9NBr9dTO8MvEhmjYmFIO8v4JMFB7HkeGo0GfN9Hs9lEq9XCeDzG1tYWisWi2vJwNBohl8upbTElqD5yc2rdHyqtv7OkY9Ti12xDSmDT9/q19AoOJgOb3pa+J6/eriS7ba2s35/+bHQpqKv/prb0hAdOwvpak+9Etmtqy3FO9isulUqK4MwGq1arWF1dVXsYt1ottFoto3ps0qoWjdgLQ1qJsAcpj6GkzWQyGAwGE5I2CAK1pmUbYZJWT66eNVBm9VuXXFEkWJTPTO1RGodJWlMQhC7ZZi0DTIM4qktHh00qm969XpxNh8x8ymQyE64rktl1XQAnifLdbjeRtM8Lvu+r3Nlut4ter4cgCLC2toZarYadnR3s7u6qwcxq+xKXLl1Slkam9JnUsDCDiwm6RJSfRbFa6gNZb8ckoaR7hDCVKjXd17yTlK7SyvZ0dd20ebNtAtC3LpFtShuFxMbGBoATK3Gz2YTjOFheXkahUMDm5iaGwyEajQYajQaOjo7QarXUu553Lb8IiCVpORjopx2NRmi32+h0OnBdF5cvX0Yul0Or1cLBwQF834fruqhUKlOD8urVqwCAnZ0dHB4eqsR5Xa3Urcu2fsnj+Vsnhj5YohJYtiMnFdbDMuXT2rb40CeCKLBJar0Wly75TSViZqn/8nyue/v9/pQfGjh9h51OB0dHR0ilUrh69SrW19fhui5838fBwQH29vawt7eHdruNbrc713JmkRAL0tpUNUoZWgSpKtMgIR31MmFeggOKlkoToUyqXBjRTFJRfhdF/TdB9kVXlU1k1vtpW1/arjXvcTZJHkX9j/oMTO3J2HC6+LLZLCqViiqMoFuOo+xXtKhSNxaktUFuGXF0dATXdVGr1bC6uqoq8lUqFRVIX6/Xp9qgusWyJkEQTO02oMMmoaQxSFqi5fe6lTesyoX8LT+n+sjz8vm80fJqalNenxOavCddNZUIG+h6W9xaRK/TpN+fhJx0TFoJI510UPpSs8rn8ygWi3jhhReUhCZpO50OOp2OGjeLSswwxJ60HIDHx8fK0MTqfczScZyTqn706Zra4FqWvznoTESI4s6QscG29Rt/zyKtrkLq1lvet6mtWeto6W8Nu2e9/2zbJGllllAUiSbvTddS5DqduyHqoBW41+vh+PgYhUIBrutiY2MDg8EAR0dH6PV6qi6U53kToZFxw7nJ8oli3LGd93Piaa83z1o4zjgLmUyTit7OWcfJIiEpN5MgQcxwbiRtggQXBQlpEySIGRay3MystvSSIKlUCsViEa7r4tKlS/jggw+wvr6OQqGAUqmEXq+HP/7xj/joo48QBAE8z1Nt/f73vw/YRjqdxng8RrPZRKfTmTIiSesw10pffPGFaoslVMQ9q9+0qEr3A9ui0UaWY3nrrbcCXlcP6qfvk4YaaUziNdnWq6++Gshz9f7L/pn2u5HldF577bUAmCyBI2HKxpEBEf/3f/9nvT8+H74DBoy02200m02k02ncvHkTW1tbAIB/+7d/U21dv349AE4CZa5evYpyuYxXXnkF165dw+7uLv785z9jd3cXe3t72N7enoqGkqWMOE7DjINh+KnGvEQsrccm3+NwOES/30e73cb+/j7G4zHW19exvLwMAFhZWcHm5ubUgz88PARwkiHCPWEYtCF3LbBZSiWkpVaGR9r2kZHBCbPuUX4u/6dFlNfSXSIyIER378g+mPzJNsh7sA1uaZU3HSMnCP4tfcycjPL5PFZWVtSE1+l0jBZyx3GwtLSEGzduoFAoIJ/Po91uo91uq1hjWo0jkm9hLcuxJK0OOtVZjO3JkyfodDpqI6ZcLoeNjQ1cu3Zt6oU9efIEALC+vo5KpQIAynWQzWZVIoG8ls0CSZeJlC76dou6+8c2gEykNQ0k3/fRbrcRBAFc152Kr9aL0rE90yQyK0LKpHXwHNm+vmtDGGlN1l3gNGeX2hI1FJPbjpPi6uoqXn31Vbiui2azqX4YwtjpdKx+47D7XTScC9ICpwOJkiebzU7snJfP5xUpJfgCpcoKnKqtprpMYXjWs/OsaCop1Uaj0ZSknTU56MeapK6pL/rx8v+wc2zXlH3U997hPdlij1kLjEUQMpmMikvvdrvo9/vwfT+ylF10xJ60+ksYDAY4ODhAu91GtVrFkydPkM/nsba2plRliXK5DOBkoOzv7yuCX716Fb7vo9PpoNvtIp/Pq13FbXmdhL4+lNtm8ntKb9ugNkkim/SVNaJ0KSKznHQJKz9nih9wEsRgijzS4531IAg9UCMs4ojk830f/X5/grgMiuEaVwZPMJVS4ve//z0cx8HKyopaB9+/fx/37t1Ds9nE/v4+jo+P50p6X2QsNGnnkVqc8Zlb2+l0cHh4iMPDQ1QqFWxubqqqBhLMsfV9H/V6Hel0Guvr66jVasoIQqNULpdT0tY0qG1RTpQcpk2zZpF21vOQ62ImDkiQiKydxHP065D0pv6bjtXrNsnJIwppSfB+v6/UViKfz6NUKqnwTN4XEwb0Nt977z0Ap/sSe56HJ0+e4JtvvkGv10Oj0TAmGsQVC03as86KVGk9z8Px8TFGo5EaBPpgvHz5MgDg6OgI3W53IlvGcRwVBskZP0zCyiRxkyppMgTx/zDoa0mpEkvpYTNqham7UgXl+tfWH90Qp4dpynuZZbQzpQxy/c8NodPptEoEAYBKpWKcePlOfN9Hq9VSWTysBTVPKGUcsNCknQccLJJ0R0dH+O6771AsFpV1WR+Q/+///T8EQYDPPvsMjx49wmAwQK/XU6rwpUuXAADNZhMHBwcYj0+2z9RT94DTKhimhAFpUZaSSB6j34+JoMxW4nqdaiaP1weorYQOSUfrtkySmGXRloSTx0lNQk/X00ENQErscrmMQqGAbDaLYrGITCaDbreLg4MD5HI5vPfee7h9+7axPeDkHT148ACtVgs7OztoNBrWdXCccW5IKyHL0TSbTTUDmwxRdAPdu3dPnUsyZLNZuK6rBg9Jk81mjWl+Mgg9LO41qtqvrxVliiFJq1tq5005k2o7JW0Ug02YJVk/Jkytl4avTCYD13WV1KchcDAYwHEclMtlbG1tWdsbDAZotVo4Pj5WBqi4VqcIw7khrT5AuF46Pj5Gv9/H4eGhsdwM10jVahVvvPEGut0u2u22SvWj9MnlclhdXQ0dBJS0NKaY+kfIda6JaKYqiXItLYuMywnEZr3V19G6Sq37VU3LAJNxTLZpsq7bpC3Xz6lUCqVSCel0GqVSCcViEePxyabQ9NNeu3YNhUIBq6urarki0Ww2EQQB6vX6RJJ7nGsbh+HckJaQkszzPAyHQ1UjSLfiAqdbSqysrOCv/uqv0Ol08Oc//xkPHjxQUVa0KG9tbWE8HqPVaqkyNxLys3w+r/y2hO4aCZO8cn3MNmTqWi6XUxqBVKFNpNUlv24kk32R5+jgRGRbN4eRVv+cpGXlzEwmg6WlJRSLRfR6PTSbTfT7fayvr+Pq1asolUrY2tqa2OqFODw8RBAE2N3dxaNHj9DpdNBqtSbUYpsVPo44d6SVoDpJt0Kv17MOHhqdpJTkOtL3fZUjysFpqnsk14+SSIRtAM8DXXKaSKnDNjGYJC6AmS4t2Q8beE2ZrK+fLw1gshKmRDabRalUQqlUMj5z4GSypEvI8zyV9K7357zgXJJWWjZpmKrX60Yf36NHjwCcWCar1SrS6TSq1aoqbl2v15WqXCwWkU6nkcvljD5f1irq9/tqIPGH6zW9JOg8BObxch0oa/3yGDlAKW10gxSvy8HNyclxHNVPHZIItt0UgFPtgBZ8TowS1WoVwImfvFKpIJVKYTAYqOdWLBZRKpWwubmJF198URWiPz4+nrrenTt3EAQBHj9+rLY5NV3zvOBckhaYnOmDILCqtAcHB2oQk7TlchkrKyvodDp49OgRut0uSqWSUrULhYLRqLW+vg7gRF1rNpvKv0gfr+u6U7WL5W8Jm0FHqqH8O5vNTqxLJSTRJNG5luY51Eaoepv6JF1WJp8z+yqNZr1eD51OZ6qtUqkE4CTmu1gsIggCpdbmcjlVbJ4x49lsVgW66Hjw4MHUc5cTtw1Po/E8TySpeQgP/uf3Uo00HWP6f9Fh8qmG4VmrmFGel2liM/XDZnB62uXIIiKpXJEgQcyQSNoECWKGhLQJEsQMsaxcobc1a72jr2lkpYJ8Ph84joO1tTVcuXIFxWIRt27dwtWrV9Hr9bC7u4tut4vvv/8ed+/ehe/7KkcXAJ48eaLa+uUvfxkAUJbPVCqlUsI8z8PBwYGypsotSNi/ZrOp2trc3AwAKMOXjI4aDodot9sqQktu58kAkv39fQcA1tfXp54Vc4RZN5qRVczcqVQqKpHi66+/Dq2mIa3QvV5PBXvQTba6uqosxf/1X/+l2vrDH/4QACe+9F6vpyKfhsMhNjc38etf/xqXLl2C53lot9vo9Xr4y1/+gvv37yMIAnz66aeqrUwmo/o1z3JP2Cl+lnH6NG1JnAvr8dOsyzn4Op0O9vb2VKWK4+Nj5Y9lYEWhUIDv+zg8PESr1Zq6bqPRgOM4yuUgrdcMf0ylUnBdV6UE2nyi0lfMDBUOSpJVtxjLGsE6eB0aZvRtN0g+U5UOQm5QLS3RdDsxHpv9YlCKqa2dnR0Ap9tQMiLKdV2USiWMRiM0Gg0cHBzg8ePH6Ha7ePjwoao0ImEyEs5CnG0554K0TwMOcmaHpNNp1Ot1lEol1Go1vPLKK6jValhbW0O1WsVgMMBHH32Er7/+eurFcxNj13VVWRSSIZPJKD8vK2JI94uuDVCSS9+lJCfdSZK0pogv6S7S60rxc0ZuMYnBFhTByYMBESQsE88LhcJEnjADHkz+8R9++AEAsLW1hbW1NZRKJVy/fh2bm5sYDAZoNBrY39/Hd999hy+++AK9Xg+tVkttsma6xzDEmaQ6LjxpgckMHMa9ApjYrLpYLCrVs1AoKN+ihMyxZaCCLt1MQQ5REuol+fQsG5JVVvYnZP0lU+SUKcLKVGtKtkWyUnrzR15fBliYsmw4adF/TdWe1+31eioOvNPpqEineTN2zhNZiYS0mF4TM2Z5PB7j22+/RaFQwMbGBrrdLjKZDC5duoTV1dWpdm7dugXgJID98PAQ4/FYEYDtUeU2pcRJUL3O5XKoVqsqiIJrUbYn449Nm4wxiEHGOctkft57LpdT+ca2sjrcUlIeI9Vu3/fh+77aA5brXFmZgvj1r38NAGrD51QqBc/zcP/+fRwfH+O7777D8fEx6vW6iiO2VZ4I852fRySkNYCDz/d9laI3GAxUHOwLL7yA9fX1qQHCMMYgCLCzswPf95XqyEEt423lmlAHJQrVdIZB5vN5ZUCSsdHj8Ri9Xm8qfI8GJVZ1AE5UdUo6Eo4F1KRU18HQTfk90/jkRMItJ1mvyxRS+MYbbwCAKuMzGo3www8/YG9vD/V6Hd9++y0ajcZEOZrzmLFzFlwI0obNxFLtNJ1H8nQ6vjUv1wAABlBJREFUHdTrdXiep3YV18HPl5eX8cILL6id6EkquX+sDLMzDUSuKW3rN7lZFo+lBJWgcYiqK69LlVoWrZOJ+SbIEEK5jpahkFxecI28urpqLBgga0S1Wi34vo/9/X0cHByodErTfrsJLghpo0DPhOHa0fM8OM7Jrnssq7q7u4tarTbVBkuhrK+v480338RgMMC3336LR48eod/vo16vKwkuDVRhklaSQqq4lNRUT4MgME4m7CfV6NFohE6noww6lJS9Xg/tdnuiTR0sN6v3k31k0jm1h0KhgHfeeQevvPLK1GTAuluHh4fY2dmB53nY2dlRz4hFB6R7ie/loiMhrYAphU0aqLhhseM4RpWPFt9yuYxarYbBYIDDw0McHR1NuCUk+fiZDg5UnbA8VjcCATBKSrpcWF51OByq9SY1CZnCaDNoATAG65tIy/1hM5kMVldXcf369al+8fmxuHyv18PBwQEajcbcqrB8jmGa03nBuSftPC8vLEhDzvh69UDizp07AE4k7sbGhlorvvjii+j3+1hdXVV5ve12W5WNYb0kCbbf7/fRarWQyWRUPWcWP5MuGgBGVZKWcLp52G46nVbElUYsufbWIV03nHTo8nFdFysrK8rvWi6XlV/6wYMHU+199dVXAE4K6h0cHCgDlqliR1QiyuXEeV77nnvSzsKsgaAbXUajEY6Pj42V7j/88EMAJ77H69evo1gsYmtrCzdu3FASbjgcYn9/H48fP1ZlcI6Pj6cGGQdvr9cDgAnrcSaTUaSQEVOUerItXTrKNTH/l4SW62Qd7Au/T6VSWFpaguu6WFtbwzvvvKN8rtVqFcPhEN9++y3u3r071dYnn3wCABNlUWXfbVbieYh7XnGhSHuWl2nLKzW11e124TgOut2uksasbk9jEYMrWMamWCwaC6kVi0UApwYkDkZKQ7mBF/82GW6kFJdtkJzSOGVSM019kueyrlOpVFJ76LDCBG0C3LZEghOAyZXzNKQ774QFLgBpZ73EZ+njI2GOjo7g+z6y2SyePHmCUqmEbDarpCOrYziOg6tXrxoNUfRjcgKgpKZUYm0rWYRcr84InEZpmZ6D3GCMbiCGWXJ9LvG73/1u4pqpVEq5tLgGbjQayr/a7/fx+PFjY+gh17TzVEucRys6zzj3pJ0HUdSvMMiQyFarNWHUYWGyUqmE1dVVbG5uKrXSZIl+8803J6yr9FWydCp3gAMmg/Z10jYajYn7A6DUbNYX5pqUYYgssKY/i3feeQfAqW9Vft/pdLC9vY1Wq4Xt7W18/fXXoaGH1A5MRDvLO7gohAUS0v6k0K2+JuumHlZImMILw64TtS9h7chrPU2f9Hu19e8iEe1ZIqlckSBBzJAkwSdIEDMkpE2QIGY4F5UrnqatVCplbSviWjFSWxIM88tkMhOGKFmN4e/+7u8CAFhaWkKtVlNlRbmFRi6XUzv5DQYDFd1E49Q//dM/OQDwj//4jwEAVS0DON1/loYolmClK0gatP7lX/5F9env//7vA+B0T9nhcKgMTb1eD/v7+8q6bbIKy4ohUZ6V7fn/GGK68GPrWbYlceENUc9yTR/W1ixDkimRQSYURP2x9cnkBzWFVkZt61n5Vc/SxkW3w1x40j4PUJoBUOGMOmm3t7cBQKXMZTIZVKtVlMtlFYyfTqcnQhNNm0rTTyslsoyKop9WSlpbaOXDhw/VdWRGD4/nZ/PEDfN5JIiOhLTPASTWaDRCs9k0hkR+8803AE4DGTKZDGq1GsrlMjKZjCIyAyPYnk5abnvChAAmPuiVNZgFNB6P1TaROpm+/vpr1X+5/aYpuSEqEsLOj4S0PxNsQQQmogGTW0EOh0O1BgWg9u6RdZoATBCIYLigJCVDK6WvVSaxk7Q65FaeMrD/LKpyQtazIyHtc0SUgSslWqvVgud5SrrKki9sT2+TBdSkFNQnCq6deYwt+Vwm8Jt+J/h5kJB2ARBm9OHfsi6x/HxWyN/u7m7oNecJGdSleELW54OEtDGDTpR5EiLO8v1Zj03w0yEJrkiQIGZISJsgQcyQkDZBgpghIW2CBDFDQtoECWKGhLQJEsQMCWkTJIgZEtImSBAzJOVmEiSIGRJJmyBBzJCQNkGCmCEhbYIEMUNC2gQJYoaEtAkSxAwJaRMkiBn+PySHBVa+OnxaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 144 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3H3-dAD8mSl"
      },
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD2hNmnSCx8C"
      },
      "source": [
        "from tensorflow.python.keras.engine.base_layer import Layer\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.python.keras import constraints\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "import string\n",
        "from tensorflow.python.keras.layers import einsum_dense\n",
        "from tensorflow.python.keras.layers import advanced_activations\n",
        "from tensorflow.python.keras.layers import core\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import special_math_ops\n",
        "from tensorflow.python.util.tf_export import keras_export"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxtl1lk1COya"
      },
      "source": [
        "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
        "  input_str = \"\"\n",
        "  kernel_str = \"\"\n",
        "  output_str = \"\"\n",
        "  bias_axes = \"\"\n",
        "  letter_offset = 0\n",
        "  for i in range(free_dims):\n",
        "    char = _CHR_IDX[i + letter_offset]\n",
        "    input_str += char\n",
        "    output_str += char\n",
        "\n",
        "  letter_offset += free_dims\n",
        "  for i in range(bound_dims):\n",
        "    char = _CHR_IDX[i + letter_offset]\n",
        "    input_str += char\n",
        "    kernel_str += char\n",
        "\n",
        "  letter_offset += bound_dims\n",
        "  for i in range(output_dims):\n",
        "    char = _CHR_IDX[i + letter_offset]\n",
        "    kernel_str += char\n",
        "    output_str += char\n",
        "    bias_axes += char\n",
        "  equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
        "\n",
        "  return equation, bias_axes, len(output_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYfEcs8RDrYr"
      },
      "source": [
        "def _get_output_shape(output_rank, known_last_dims):\n",
        "  return [None] * (output_rank - len(known_last_dims)) + list(known_last_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpr95vfmDwLb"
      },
      "source": [
        "def _build_attention_equation(rank, attn_axes):\n",
        "  target_notation = _CHR_IDX[:rank]\n",
        "  batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n",
        "  letter_offset = rank\n",
        "  source_notation = \"\"\n",
        "  for i in range(rank):\n",
        "    if i in batch_dims or i == rank - 1:\n",
        "      source_notation += target_notation[i]\n",
        "    else:\n",
        "      source_notation += _CHR_IDX[letter_offset]\n",
        "      letter_offset += 1\n",
        "\n",
        "  product_notation = \"\".join([target_notation[i] for i in batch_dims] +\n",
        "                             [target_notation[i] for i in attn_axes] +\n",
        "                             [source_notation[i] for i in attn_axes])\n",
        "  dot_product_equation = \"%s,%s->%s\" % (source_notation, target_notation,\n",
        "                                        product_notation)\n",
        "  attn_scores_rank = len(product_notation)\n",
        "  combine_equation = \"%s,%s->%s\" % (product_notation, source_notation,\n",
        "                                    target_notation)\n",
        "  return dot_product_equation, combine_equation, attn_scores_rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA3E6rC9D1cd"
      },
      "source": [
        "_CHR_IDX = string.ascii_lowercase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnbNHxyj-6Xy"
      },
      "source": [
        "@keras_export(\"keras.layers.MultiHeadAttention2\")\n",
        "class MultiHeadAttention2(Layer):\n",
        "  \"\"\"MultiHeadAttention layer.\n",
        "  Nulti-head attention mechanism with the attention mechanism focused\n",
        "  on the query, keys and values. The attention mechanism is crossed to \n",
        "  avoid loss of information in dimension reduction by scalar product.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               num_heads,\n",
        "               key_dim,\n",
        "               value_dim=None,\n",
        "               dropout=0.0,\n",
        "               use_bias=True,\n",
        "               output_shape=None,\n",
        "               attention_axes=None,\n",
        "               kernel_initializer=\"glorot_uniform\",\n",
        "               bias_initializer=\"zeros\",\n",
        "               kernel_regularizer=None,\n",
        "               bias_regularizer=None,\n",
        "               activity_regularizer=None,\n",
        "               kernel_constraint=None,\n",
        "               bias_constraint=None,\n",
        "               **kwargs):\n",
        "    super(MultiHeadAttention2, self).__init__(**kwargs)\n",
        "    self._num_heads = num_heads\n",
        "    self._key_dim = key_dim\n",
        "    self._value_dim = value_dim if value_dim else key_dim\n",
        "    self._dropout = dropout\n",
        "    self._use_bias = use_bias\n",
        "    self._output_shape = output_shape\n",
        "    self._kernel_initializer = initializers.get(kernel_initializer)\n",
        "    self._bias_initializer = initializers.get(bias_initializer)\n",
        "    self._kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "    self._bias_regularizer = regularizers.get(bias_regularizer)\n",
        "    self._kernel_constraint = constraints.get(kernel_constraint)\n",
        "    self._bias_constraint = constraints.get(bias_constraint)\n",
        "    if attention_axes is not None and not isinstance(attention_axes,\n",
        "                                                     collections.abc.Sized):\n",
        "      self._attention_axes = (attention_axes,)\n",
        "    else:\n",
        "      self._attention_axes = attention_axes\n",
        "    self._built_from_signature = False\n",
        "    self._query_shape, self._key_shape, self._value_shape = None, None, None\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        \"num_heads\":\n",
        "            self._num_heads,\n",
        "        \"key_dim\":\n",
        "            self._key_dim,\n",
        "        \"value_dim\":\n",
        "            self._value_dim,\n",
        "        \"dropout\":\n",
        "            self._dropout,\n",
        "        \"use_bias\":\n",
        "            self._use_bias,\n",
        "        \"output_shape\":\n",
        "            self._output_shape,\n",
        "        \"attention_axes\":\n",
        "            self._attention_axes,\n",
        "        \"kernel_initializer\":\n",
        "            initializers.serialize(self._kernel_initializer),\n",
        "        \"bias_initializer\":\n",
        "            initializers.serialize(self._bias_initializer),\n",
        "        \"kernel_regularizer\":\n",
        "            regularizers.serialize(self._kernel_regularizer),\n",
        "        \"bias_regularizer\":\n",
        "            regularizers.serialize(self._bias_regularizer),\n",
        "        \"activity_regularizer\":\n",
        "            regularizers.serialize(self._activity_regularizer),\n",
        "        \"kernel_constraint\":\n",
        "            constraints.serialize(self._kernel_constraint),\n",
        "        \"bias_constraint\":\n",
        "            constraints.serialize(self._bias_constraint),\n",
        "        \"query_shape\": self._query_shape,\n",
        "        \"key_shape\": self._key_shape,\n",
        "        \"value_shape\": self._value_shape,\n",
        "    }\n",
        "    base_config = super(MultiHeadAttention2, self).get_config()\n",
        "    return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "  @classmethod\n",
        "  def from_config(cls, config):\n",
        "    query_shape = config.pop(\"query_shape\")\n",
        "    key_shape = config.pop(\"key_shape\")\n",
        "    value_shape = config.pop(\"value_shape\")\n",
        "    layer = cls(**config)\n",
        "    if None in [query_shape, key_shape, value_shape]:\n",
        "      logging.warning(\n",
        "          \"One of the input shape is missing. They should be \"\n",
        "          \"memorized when the layer was serialized. \"\n",
        "          \"%s is created without weights.\",\n",
        "          str(cls))\n",
        "    else:\n",
        "      layer._build_from_signature(query_shape, value_shape, key_shape)  # pylint: disable=protected-access\n",
        "    return layer\n",
        "\n",
        "  def _build_from_signature(self, query, value, key=None):\n",
        "    self._built_from_signature = True\n",
        "    if hasattr(query, \"shape\"):\n",
        "      self._query_shape = tensor_shape.TensorShape(query.shape)\n",
        "    else:\n",
        "      self._query_shape = tensor_shape.TensorShape(query)\n",
        "    if hasattr(value, \"shape\"):\n",
        "      self._value_shape = tensor_shape.TensorShape(value.shape)\n",
        "    else:\n",
        "      self._value_shape = tensor_shape.TensorShape(value)\n",
        "    if key is None:\n",
        "      self._key_shape = self._value_shape\n",
        "    elif hasattr(key, \"shape\"):\n",
        "      self._key_shape = tensor_shape.TensorShape(key.shape)\n",
        "    else:\n",
        "      self._key_shape = tensor_shape.TensorShape(key)\n",
        "\n",
        "    common_kwargs = dict(\n",
        "        kernel_initializer=self._kernel_initializer,\n",
        "        bias_initializer=self._bias_initializer,\n",
        "        kernel_regularizer=self._kernel_regularizer,\n",
        "        bias_regularizer=self._bias_regularizer,\n",
        "        activity_regularizer=self._activity_regularizer,\n",
        "        kernel_constraint=self._kernel_constraint,\n",
        "        bias_constraint=self._bias_constraint)\n",
        "\n",
        "    with tf_utils.maybe_init_scope(self):\n",
        "      free_dims = self._query_shape.rank - 1\n",
        "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "          free_dims, bound_dims=1, output_dims=2)\n",
        "      self._query_dense = einsum_dense.EinsumDense(\n",
        "          einsum_equation,\n",
        "          output_shape=_get_output_shape(output_rank - 1,\n",
        "                                         [self._num_heads, self._key_dim]),\n",
        "          bias_axes=bias_axes if self._use_bias else None,\n",
        "          name=\"query\",\n",
        "          **common_kwargs)\n",
        "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "          self._key_shape.rank - 1, bound_dims=1, output_dims=2)\n",
        "      self._key_dense = einsum_dense.EinsumDense(\n",
        "          einsum_equation,\n",
        "          output_shape=_get_output_shape(output_rank - 1,\n",
        "                                         [self._num_heads, self._key_dim]),\n",
        "          bias_axes=bias_axes if self._use_bias else None,\n",
        "          name=\"key\",\n",
        "          **common_kwargs)\n",
        "      einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "          self._value_shape.rank - 1, bound_dims=1, output_dims=2)\n",
        "      self._value_dense = einsum_dense.EinsumDense(\n",
        "          einsum_equation,\n",
        "          output_shape=_get_output_shape(output_rank - 1,\n",
        "                                         [self._num_heads, self._value_dim]),\n",
        "          bias_axes=bias_axes if self._use_bias else None,\n",
        "          name=\"value\",\n",
        "          **common_kwargs)\n",
        "      self._build_attention(output_rank)\n",
        "      self._output_dense = self._make_output_dense(\n",
        "          free_dims, common_kwargs, \"attention_output\")\n",
        "\n",
        "  def _make_output_dense(self, free_dims, common_kwargs, name=None):\n",
        "    if self._output_shape:\n",
        "      if not isinstance(self._output_shape, collections.abc.Sized):\n",
        "        output_shape = [self._output_shape]\n",
        "      else:\n",
        "        output_shape = self._output_shape\n",
        "    else:\n",
        "      output_shape = [self._query_shape[-1]]\n",
        "    einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
        "        free_dims, bound_dims=2, output_dims=len(output_shape))\n",
        "    return einsum_dense.EinsumDense(\n",
        "        einsum_equation,\n",
        "        output_shape=_get_output_shape(output_rank - 1, output_shape),\n",
        "        bias_axes=bias_axes if self._use_bias else None,\n",
        "        name=name,\n",
        "        **common_kwargs)\n",
        "\n",
        "  def _build_attention(self, rank):\n",
        "    if self._attention_axes is None:\n",
        "      self._attention_axes = tuple(range(1, rank - 2))\n",
        "    else:\n",
        "      self._attention_axes = tuple(self._attention_axes)\n",
        "    self._dot_product_equation, self._combine_equation, attn_scores_rank = (\n",
        "        _build_attention_equation(rank, attn_axes=self._attention_axes))\n",
        "    norm_axes = tuple(\n",
        "        range(attn_scores_rank - len(self._attention_axes), attn_scores_rank))\n",
        "    self._softmax = advanced_activations.Softmax(axis=norm_axes)\n",
        "    self._dropout_layer = core.Dropout(rate=self._dropout)\n",
        "\n",
        "  def _masked_softmax(self, attention_scores, attention_mask=None):\n",
        "    if attention_mask is not None:\n",
        "      mask_expansion_axes = [-len(self._attention_axes) * 2 - 1]\n",
        "      for _ in range(len(attention_scores.shape) - len(attention_mask.shape)):\n",
        "        attention_mask = array_ops.expand_dims(\n",
        "            attention_mask, axis=mask_expansion_axes)\n",
        "    return self._softmax(attention_scores, attention_mask)\n",
        "\n",
        "  def _compute_attention(self,\n",
        "                         query,\n",
        "                         key,\n",
        "                         value,\n",
        "                         attention_mask=None,\n",
        "                         training=None):\n",
        "\n",
        "    \"\"\"---------------------------Modified section------------------------------\"\"\"\n",
        "    query0 = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n",
        "    attention_scores0 = tf.einsum(self._dot_product_equation, key, query0) #\n",
        "    attention_scores0 = self._masked_softmax(attention_scores0, attention_mask)\n",
        "    attention_scores_dropout0 = self._dropout_layer(attention_scores0, training=training)\n",
        "    attention_output0 = tf.einsum(self._combine_equation, attention_scores_dropout0, value)\n",
        "\n",
        "    value0 = tf.multiply(value, 1.0 / math.sqrt(float(self._key_dim)))\n",
        "    attention_scores1 = tf.einsum(self._dot_product_equation, query, value0)\n",
        "    attention_scores1 = self._masked_softmax(attention_scores1, attention_mask)\n",
        "    attention_scores_dropout1 = self._dropout_layer(attention_scores1, training=training)\n",
        "    attention_output1 = tf.einsum(self._combine_equation, attention_scores_dropout1, key)\n",
        "\n",
        "    key0 = tf.multiply(key, 1.0 / math.sqrt(float(self._key_dim)))\n",
        "    attention_scores2 = tf.einsum(self._dot_product_equation, value, key0)\n",
        "    attention_scores2 = self._masked_softmax(attention_scores2, attention_mask)\n",
        "    attention_scores_dropout2 = self._dropout_layer(attention_scores2, training=training)\n",
        "    attention_output2 = tf.einsum(self._combine_equation, attention_scores_dropout2, query)\n",
        "\n",
        "    attention_output=tf.math.scalar_mul(1/3.,tf.add(tf.add(attention_output0, attention_output1),attention_output2))\n",
        "    attention_scores=tf.math.scalar_mul(1/3.,tf.add(tf.add(attention_scores0, attention_scores1),attention_scores2))\n",
        "\n",
        "    return attention_output, attention_scores\n",
        "\n",
        "  def call(self,\n",
        "           query,\n",
        "           value,\n",
        "           key=None,\n",
        "           attention_mask=None,\n",
        "           return_attention_scores=False,\n",
        "           training=None):\n",
        "    if not self._built_from_signature:\n",
        "      self._build_from_signature(query=query, value=value, key=key)\n",
        "    if key is None:\n",
        "      key = value\n",
        "\n",
        "    query = self._query_dense(query)\n",
        "    key = self._key_dense(key)\n",
        "    value = self._value_dense(value)\n",
        "\n",
        "    attention_output, attention_scores = self._compute_attention(\n",
        "        query, key, value, attention_mask, training)\n",
        "    attention_output = self._output_dense(attention_output)\n",
        "\n",
        "    if return_attention_scores:\n",
        "      return attention_output, attention_scores\n",
        "    return attention_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL4xn7pS8pd4"
      },
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Augment data.\n",
        "    augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = MultiHeadAttention2(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
        "    model.compile(optimizer=optimizer, loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOYZWAyQ1iZw"
      },
      "source": [
        "def get_model(network, opt='adadelta', loss_name='categorical_crossentropy', input_shape=(240,240,3), classes=2, weights='imagenet'):\n",
        "  if network=='Transformer':\n",
        "    model=create_vit_classifier()\n",
        "    return model\n",
        "  else:\n",
        "    try:\n",
        "      del model\n",
        "    except:\n",
        "      print('done')\n",
        "    model=Sequential()\n",
        "    if network=='ResNet50V2':\n",
        "      model.add(ap.ResNet50V2(include_top=False, weights=weights, input_shape=input_shape, pooling='avg', classes=classes)) # The input must have 3 channels\n",
        "    if network=='EfficientNetB7':\n",
        "      model.add(ap.EfficientNetB7(include_top=False, weights=weights, input_shape=input_shape, pooling='avg', classes=classes)) \n",
        "    if network=='InceptionResNetV2':\n",
        "      model.add(ap.InceptionResNetV2(include_top=False, weights=weights, input_shape=input_shape, pooling='avg', classes=classes)) \n",
        "    if network=='InceptionV3':\n",
        "      model.add(ap.InceptionV3(include_top=False, weights=weights, input_shape=input_shape, pooling='avg', classes=classes)) \n",
        "    if network=='NASNetLarge':\n",
        "      model.add(ap.NASNetLarge(include_top=False, weights=weights, input_shape=input_shape, pooling='avg', classes=classes))\n",
        "    if network=='VGG19':\n",
        "      model.add(ap.VGG19(include_top=False, weights=weights, input_shape=input_shape, pooling='avg', classes=classes)) \n",
        "    if network=='Xception':\n",
        "      model.add(ap.Xception(include_top=False, weights=weights, input_shape=input_shape, pooling='avg', classes=classes))\n",
        "    if network=='DenseNet121':\n",
        "      model.add(ap.DenseNet121(include_top=False, weights=weights, input_shape=input_shape, pooling='avg', classes=classes))    \n",
        "    model.add(Dense(classes, activation='softmax'))\n",
        "    model.compile(optimizer=opt, loss=loss_name, metrics=['acc', tf.keras.metrics.Recall(), tf.keras.metrics.FalsePositives()])\n",
        "    clear_output(wait=True)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_2SI7-12Afv"
      },
      "source": [
        "def run_experiment(images, labels, number_class, augm_con, name='', star_run=0, end_run=30, batch_size=4): \n",
        "  for net in Networks:\n",
        "    name_0='_'.join(['clasification_t', name, str(transferlearning), augm_con, net, optimizer])\n",
        "    if not os.path.exists(path2+name_0+'.csv'):\n",
        "      df.to_csv(path2+name_0+'.csv')\n",
        "\n",
        "    train_id, test_id=get_index(labels)\n",
        "    x_test, y_test=get_data_training(test_id, images, labels)\n",
        "    y_test=np.argmax(y_test, axis=1).reshape(np.shape(y_test)[0],1)>=0.5\n",
        "\n",
        "    for i in range(star_run, end_run): #Numero corridas \n",
        "      for j in [loss]: #Funciones de perdida\n",
        "        cntn=True\n",
        "        name_m='_'.join([name_0,j,'run',str(i)])  \n",
        "        print(name_m)\n",
        "\n",
        "        #Obtener imagenes nuevamente\n",
        "        x_train, x_valid, y_train, y_valid=get_data_training(train_id, images, labels, k_fold=i%10, augm=(augm_con!='None'))      \n",
        "        model=get_model(net, classes=number_class, input_shape=list(np.shape(x_train)[1:]),  weights=transferlearning)\n",
        "        \n",
        "        try:\n",
        "          tic = time.time()\n",
        "          results = model.fit(x_train, np.argmax(y_train, axis=1).reshape(np.shape(y_train)[0],1),\n",
        "                              validation_data=(x_valid, np.argmax(y_valid, axis=1).reshape(np.shape(y_valid)[0],1)), batch_size=batch_size, epochs=epochs)\n",
        "          toc=time.time()-tic\n",
        "          model.save_weights(pathW+name_m+\"w.h5\")\n",
        "        except:\n",
        "          print('Training error')\n",
        "          cntn=False\n",
        "\n",
        "        if cntn:\n",
        "          sio.savemat(pathW+name_m+'_r.mat', results.history)\n",
        "\n",
        "          #Validation\n",
        "          y_hat=np.array(model.predict(x_test))\n",
        "          y_hat=np.argmax(y_hat, axis=1)\n",
        "          sio.savemat(pathW+name_m+'_los_8.mat',{'y_hat': y_hat, 'y_test': np.array(y_test)})\n",
        "          y_hat=y_hat>=0.5\n",
        "\n",
        "          for class_i in range(1):\n",
        "            TP.reset_state()\n",
        "            TN.reset_state()\n",
        "            FP.reset_state()\n",
        "            FN.reset_state()        \n",
        "\n",
        "            TP.update_state(y_test, y_hat)\n",
        "            TN.update_state(y_test, y_hat)\n",
        "            FP.update_state(y_test, y_hat)\n",
        "            FN.update_state(y_test, y_hat)\n",
        "            total_p=model.count_params()\n",
        "\n",
        "            #data frame\n",
        "            df2=pd.read_csv(path2+name_0+'.csv')\n",
        "            df2=df2.append({'run_n': i,\n",
        "                            'k_fold': i%10,\n",
        "                            'network': net,\n",
        "                            'optimizer': optimizer,\n",
        "                            'loss': 'categorical_crossentropy',\n",
        "                            'epochs': epochs,\n",
        "                            'total_parameters': total_p,\n",
        "                            'time': toc,\n",
        "                            'transfer': transferlearning,\n",
        "                            'augm': augm_con,\n",
        "                            'Class': class_i,\n",
        "                            'TP': float(TP.result()),\n",
        "                            'TN': float(TN.result()),\n",
        "                            'FP': float(FP.result()),\n",
        "                            'FN': float(FN.result()),\n",
        "                            'result_mat': name_m+'_r.mat'} , ignore_index=True)\n",
        "            df2=df2.drop(df2.columns[:np.where(df2.columns=='run_n')[0][0]], axis=1)\n",
        "            df2.to_csv(path2+name_0+'.csv')\n",
        "          del x_train, y_train, x_valid, y_valid, model\n",
        "          clear_output(wait=True)\n",
        "    star_run=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcFMigCa2FzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee604e8c-eb3d-4481-bb7f-dd665a78b3b7"
      },
      "source": [
        "root='/content/drive/MyDrive/Brain_tumors_v2'\n",
        "\n",
        "Networks=['Transformer']\n",
        "path2=root+'/Results/TCIADetection/TransformerCx/results_csv/'\n",
        "pathW=root+'/Results/TCIADetection/TransformerCx/Weights/'\n",
        "\n",
        "augmentation='None'\n",
        "transferlearning=None\n",
        "optimizer='adadelta'\n",
        "epochs=50\n",
        "loss='categorical_crossentropy'\n",
        "\n",
        "#----------------------------------RUN------------------------------------------\n",
        "seq='FLAIR'#,\n",
        "imas, labs, no_cls=get_data(seq)\n",
        "run_experiment(imas, labs, no_cls, augmentation, seq, star_run=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_2\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_2), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_2), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_9), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_3), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_10), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_1/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_4), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_1/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_11), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_1/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_5), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_1/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_12), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_1/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_6), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_1/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_19), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_1/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_7), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_1/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_20), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_2/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_8), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_2/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_21), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_2/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_9), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_2/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_22), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_2/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_10), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_2/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_29), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_2/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_11), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_2/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_30), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_3/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_12), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_3/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_31), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_3/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_13), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_3/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_32), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_3/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_14), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_3/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_39), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_3/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_15), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_3/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_40), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_4/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_16), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_4/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_41), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_4/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_17), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_4/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_42), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_4/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_18), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_4/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_49), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_4/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_19), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_4/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_50), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_5/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_20), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_5/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_51), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_5/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_21), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_5/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_52), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_5/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_22), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_5/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_59), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_5/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_23), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_5/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_60), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_6/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_24), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_6/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_61), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_6/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_25), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_6/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_62), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_6/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_26), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_6/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_69), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_6/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_27), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_6/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_70), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_7/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_28), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_7/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_71), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_7/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_29), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_7/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_72), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_7/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_30), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_7/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_79), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_7/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_31), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_7/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 29s 30ms/step - loss: 2.4429 - accuracy: 0.6779 - val_loss: 0.4048 - val_accuracy: 0.7930\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5228 - accuracy: 0.7405 - val_loss: 0.4440 - val_accuracy: 0.7834\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4654 - accuracy: 0.7733 - val_loss: 0.3880 - val_accuracy: 0.7930\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4624 - accuracy: 0.7783 - val_loss: 0.3574 - val_accuracy: 0.8344\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4433 - accuracy: 0.7889 - val_loss: 0.3224 - val_accuracy: 0.8344\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4278 - accuracy: 0.8048 - val_loss: 0.3659 - val_accuracy: 0.7771\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4113 - accuracy: 0.8073 - val_loss: 0.3087 - val_accuracy: 0.8790\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3841 - accuracy: 0.8204 - val_loss: 0.3518 - val_accuracy: 0.8535\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3978 - accuracy: 0.8327 - val_loss: 0.3133 - val_accuracy: 0.8599\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3613 - accuracy: 0.8434 - val_loss: 0.3033 - val_accuracy: 0.8726\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3384 - accuracy: 0.8561 - val_loss: 0.2903 - val_accuracy: 0.8631\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3194 - accuracy: 0.8607 - val_loss: 0.3336 - val_accuracy: 0.8344\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3148 - accuracy: 0.8699 - val_loss: 0.2779 - val_accuracy: 0.8535\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2731 - accuracy: 0.8830 - val_loss: 0.2580 - val_accuracy: 0.9108\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2626 - accuracy: 0.8858 - val_loss: 0.3280 - val_accuracy: 0.8917\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2692 - accuracy: 0.8936 - val_loss: 0.2572 - val_accuracy: 0.8949\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3464 - accuracy: 0.8706 - val_loss: 0.3870 - val_accuracy: 0.8312\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2778 - accuracy: 0.8907 - val_loss: 0.2397 - val_accuracy: 0.8981\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3853 - accuracy: 0.8504 - val_loss: 0.3537 - val_accuracy: 0.8631\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2937 - accuracy: 0.8755 - val_loss: 0.2634 - val_accuracy: 0.8885\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1976 - accuracy: 0.9194 - val_loss: 0.2778 - val_accuracy: 0.8949\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1871 - accuracy: 0.9272 - val_loss: 0.2474 - val_accuracy: 0.9299\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1957 - accuracy: 0.9229 - val_loss: 0.2252 - val_accuracy: 0.9172\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2038 - accuracy: 0.9208 - val_loss: 0.1970 - val_accuracy: 0.9299\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1976 - accuracy: 0.9180 - val_loss: 0.2676 - val_accuracy: 0.9045\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2636 - accuracy: 0.8992 - val_loss: 0.2426 - val_accuracy: 0.8885\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1931 - accuracy: 0.9187 - val_loss: 0.2491 - val_accuracy: 0.9172\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1746 - accuracy: 0.9371 - val_loss: 0.2199 - val_accuracy: 0.9013\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1766 - accuracy: 0.9332 - val_loss: 0.2778 - val_accuracy: 0.9108\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2753 - accuracy: 0.9098 - val_loss: 0.3171 - val_accuracy: 0.9013\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.2014 - accuracy: 0.9268 - val_loss: 0.2554 - val_accuracy: 0.9013\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1901 - accuracy: 0.9339 - val_loss: 0.2469 - val_accuracy: 0.9172\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1941 - accuracy: 0.9222 - val_loss: 0.3291 - val_accuracy: 0.8822\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1625 - accuracy: 0.9392 - val_loss: 0.2403 - val_accuracy: 0.9108\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1437 - accuracy: 0.9455 - val_loss: 0.3202 - val_accuracy: 0.9140\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1759 - accuracy: 0.9364 - val_loss: 0.2520 - val_accuracy: 0.8917\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1585 - accuracy: 0.9427 - val_loss: 0.1977 - val_accuracy: 0.9363\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1629 - accuracy: 0.9360 - val_loss: 0.2824 - val_accuracy: 0.8981\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1286 - accuracy: 0.9544 - val_loss: 0.2368 - val_accuracy: 0.9140\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1703 - accuracy: 0.9406 - val_loss: 0.2185 - val_accuracy: 0.8949\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1442 - accuracy: 0.9466 - val_loss: 0.2648 - val_accuracy: 0.8949\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1436 - accuracy: 0.9494 - val_loss: 0.3491 - val_accuracy: 0.9045\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1641 - accuracy: 0.9434 - val_loss: 0.2576 - val_accuracy: 0.9108\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1779 - accuracy: 0.9385 - val_loss: 0.2447 - val_accuracy: 0.9172\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2095 - accuracy: 0.9303 - val_loss: 0.2156 - val_accuracy: 0.9268\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1857 - accuracy: 0.9335 - val_loss: 0.3170 - val_accuracy: 0.8790\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2006 - accuracy: 0.9325 - val_loss: 0.1992 - val_accuracy: 0.9363\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1632 - accuracy: 0.9399 - val_loss: 0.1951 - val_accuracy: 0.9331\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1197 - accuracy: 0.9611 - val_loss: 0.1729 - val_accuracy: 0.9268\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1478 - accuracy: 0.9434 - val_loss: 0.1954 - val_accuracy: 0.9108\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_3\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_80), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_8/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_32), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_8/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_81), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_8/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_33), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_8/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_82), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_8/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_34), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_8/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_89), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_8/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_35), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_8/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_90), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_9/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_36), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_9/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_91), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_9/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_37), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_9/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_92), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_9/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_38), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_9/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_99), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_9/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_39), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_9/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_100), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_10/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_40), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_10/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_101), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_10/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_41), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_10/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_102), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_10/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_42), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_10/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_109), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_10/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_43), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_10/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_110), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_11/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_44), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_11/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_111), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_11/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_45), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_11/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_112), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_11/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_46), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_11/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_119), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_11/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_47), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_11/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_120), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_12/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_48), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_12/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_121), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_12/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_49), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_12/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_122), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_12/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_50), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_12/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_129), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_12/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_51), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_12/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_130), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_13/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_52), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_13/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_131), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_13/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_53), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_13/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_132), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_13/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_54), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_13/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_139), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_13/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_55), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_13/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_140), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_14/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_56), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_14/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_141), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_14/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_57), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_14/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_142), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_14/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_58), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_14/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_149), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_14/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_59), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_14/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_150), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_15/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_60), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_15/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_151), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_15/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_61), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_15/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_152), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_15/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_62), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_15/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_159), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_15/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_63), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_15/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 30s 31ms/step - loss: 1.9329 - accuracy: 0.6853 - val_loss: 0.4638 - val_accuracy: 0.7707\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4975 - accuracy: 0.7670 - val_loss: 0.5630 - val_accuracy: 0.7102\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4713 - accuracy: 0.7804 - val_loss: 0.4733 - val_accuracy: 0.7102\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4769 - accuracy: 0.7829 - val_loss: 0.4069 - val_accuracy: 0.7962\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4403 - accuracy: 0.8083 - val_loss: 0.5463 - val_accuracy: 0.7197\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4247 - accuracy: 0.8112 - val_loss: 0.3904 - val_accuracy: 0.8089\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4099 - accuracy: 0.8179 - val_loss: 0.4525 - val_accuracy: 0.7580\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3930 - accuracy: 0.8303 - val_loss: 0.3933 - val_accuracy: 0.8153\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3437 - accuracy: 0.8409 - val_loss: 0.4201 - val_accuracy: 0.8025\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3394 - accuracy: 0.8589 - val_loss: 0.3946 - val_accuracy: 0.7994\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3169 - accuracy: 0.8649 - val_loss: 0.4257 - val_accuracy: 0.7866\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3366 - accuracy: 0.8518 - val_loss: 0.4299 - val_accuracy: 0.7834\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3042 - accuracy: 0.8787 - val_loss: 0.3353 - val_accuracy: 0.8471\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2797 - accuracy: 0.8844 - val_loss: 0.5514 - val_accuracy: 0.8121\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2943 - accuracy: 0.8861 - val_loss: 0.3577 - val_accuracy: 0.8471\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2452 - accuracy: 0.8982 - val_loss: 0.4120 - val_accuracy: 0.8312\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2356 - accuracy: 0.9006 - val_loss: 0.4518 - val_accuracy: 0.8471\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2845 - accuracy: 0.8837 - val_loss: 0.3286 - val_accuracy: 0.8503\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2127 - accuracy: 0.9148 - val_loss: 0.4372 - val_accuracy: 0.8344\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2061 - accuracy: 0.9155 - val_loss: 0.3877 - val_accuracy: 0.8471\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1851 - accuracy: 0.9254 - val_loss: 0.3936 - val_accuracy: 0.8535\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2321 - accuracy: 0.9137 - val_loss: 0.3838 - val_accuracy: 0.8471\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2221 - accuracy: 0.9215 - val_loss: 0.4372 - val_accuracy: 0.7771\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2351 - accuracy: 0.9056 - val_loss: 0.3393 - val_accuracy: 0.8726\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2067 - accuracy: 0.9325 - val_loss: 0.3249 - val_accuracy: 0.8535\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2051 - accuracy: 0.9279 - val_loss: 0.3642 - val_accuracy: 0.8503\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1879 - accuracy: 0.9272 - val_loss: 0.3642 - val_accuracy: 0.8153\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2203 - accuracy: 0.9148 - val_loss: 0.3333 - val_accuracy: 0.8694\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1749 - accuracy: 0.9399 - val_loss: 0.4508 - val_accuracy: 0.8503\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1553 - accuracy: 0.9441 - val_loss: 0.3344 - val_accuracy: 0.8758\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1424 - accuracy: 0.9420 - val_loss: 0.5493 - val_accuracy: 0.8567\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2379 - accuracy: 0.9215 - val_loss: 0.3184 - val_accuracy: 0.8599\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1769 - accuracy: 0.9353 - val_loss: 0.4080 - val_accuracy: 0.8503\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1642 - accuracy: 0.9371 - val_loss: 0.3379 - val_accuracy: 0.8631\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2015 - accuracy: 0.9303 - val_loss: 0.3225 - val_accuracy: 0.8631\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1422 - accuracy: 0.9491 - val_loss: 0.3646 - val_accuracy: 0.8408\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1410 - accuracy: 0.9526 - val_loss: 0.4498 - val_accuracy: 0.8662\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2069 - accuracy: 0.9282 - val_loss: 0.3280 - val_accuracy: 0.8631\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1454 - accuracy: 0.9470 - val_loss: 0.6461 - val_accuracy: 0.8280\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2033 - accuracy: 0.9307 - val_loss: 0.3526 - val_accuracy: 0.8567\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1992 - accuracy: 0.9399 - val_loss: 0.3147 - val_accuracy: 0.8631\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1443 - accuracy: 0.9466 - val_loss: 0.7352 - val_accuracy: 0.7930\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2626 - accuracy: 0.9074 - val_loss: 0.6827 - val_accuracy: 0.7611\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1333 - accuracy: 0.9480 - val_loss: 0.4595 - val_accuracy: 0.8567\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1609 - accuracy: 0.9409 - val_loss: 0.3972 - val_accuracy: 0.8057\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1415 - accuracy: 0.9448 - val_loss: 0.5048 - val_accuracy: 0.8567\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1971 - accuracy: 0.9314 - val_loss: 0.4273 - val_accuracy: 0.8694\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1224 - accuracy: 0.9533 - val_loss: 0.3924 - val_accuracy: 0.8439\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1478 - accuracy: 0.9516 - val_loss: 0.5077 - val_accuracy: 0.8089\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2153 - accuracy: 0.9197 - val_loss: 0.4161 - val_accuracy: 0.8248\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_4\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_160), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_16/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_64), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_16/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_161), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_16/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_65), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_16/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_162), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_16/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_66), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_16/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_169), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_16/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_67), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_16/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_170), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_17/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_68), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_17/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_171), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_17/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_69), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_17/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_172), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_17/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_70), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_17/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_179), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_17/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_71), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_17/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_180), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_18/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_72), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_18/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_181), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_18/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_73), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_18/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_182), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_18/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_74), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_18/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_189), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_18/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_75), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_18/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_190), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_19/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_76), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_19/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_191), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_19/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_77), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_19/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_192), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_19/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_78), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_19/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_199), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_19/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_79), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_19/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_200), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_20/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_80), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_20/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_201), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_20/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_81), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_20/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_202), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_20/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_82), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_20/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_209), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_20/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_83), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_20/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_210), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_21/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_84), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_21/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_211), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_21/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_85), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_21/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_212), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_21/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_86), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_21/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_219), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_21/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_87), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_21/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_220), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_22/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_88), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_22/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_221), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_22/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_89), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_22/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_222), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_22/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_90), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_22/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_229), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_22/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_91), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_22/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_230), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_23/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_92), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_23/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_231), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_23/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_93), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_23/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_232), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_23/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_94), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_23/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_239), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_23/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_95), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_23/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 42s 43ms/step - loss: 2.3762 - accuracy: 0.6845 - val_loss: 0.4230 - val_accuracy: 0.8127\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.5053 - accuracy: 0.7457 - val_loss: 0.4466 - val_accuracy: 0.8032\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.4662 - accuracy: 0.7690 - val_loss: 0.3690 - val_accuracy: 0.8190\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.4394 - accuracy: 0.7945 - val_loss: 0.5606 - val_accuracy: 0.6952\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.4269 - accuracy: 0.7998 - val_loss: 0.3704 - val_accuracy: 0.8349\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.4166 - accuracy: 0.8097 - val_loss: 0.3939 - val_accuracy: 0.8159\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.3929 - accuracy: 0.8132 - val_loss: 0.3704 - val_accuracy: 0.8349\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.3668 - accuracy: 0.8214 - val_loss: 0.4301 - val_accuracy: 0.8159\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.3433 - accuracy: 0.8486 - val_loss: 0.6310 - val_accuracy: 0.7714\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.3208 - accuracy: 0.8585 - val_loss: 0.3477 - val_accuracy: 0.8476\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.3449 - accuracy: 0.8507 - val_loss: 0.4831 - val_accuracy: 0.8190\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.3029 - accuracy: 0.8794 - val_loss: 0.4726 - val_accuracy: 0.8317\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.2837 - accuracy: 0.8758 - val_loss: 0.3550 - val_accuracy: 0.8381\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2725 - accuracy: 0.8907 - val_loss: 0.4768 - val_accuracy: 0.8508\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2513 - accuracy: 0.8971 - val_loss: 0.2769 - val_accuracy: 0.8857\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.2513 - accuracy: 0.9024 - val_loss: 0.3301 - val_accuracy: 0.8571\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.2306 - accuracy: 0.9038 - val_loss: 0.3212 - val_accuracy: 0.8540\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.2264 - accuracy: 0.9056 - val_loss: 0.2851 - val_accuracy: 0.8889\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.2724 - accuracy: 0.8911 - val_loss: 0.3613 - val_accuracy: 0.8381\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.3025 - accuracy: 0.8935 - val_loss: 0.2716 - val_accuracy: 0.8889\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2153 - accuracy: 0.9169 - val_loss: 0.3101 - val_accuracy: 0.8857\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2414 - accuracy: 0.9119 - val_loss: 0.3253 - val_accuracy: 0.8825\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2501 - accuracy: 0.9056 - val_loss: 0.3532 - val_accuracy: 0.8603\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1575 - accuracy: 0.9342 - val_loss: 0.3919 - val_accuracy: 0.8794\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1968 - accuracy: 0.9264 - val_loss: 0.3208 - val_accuracy: 0.8635\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1748 - accuracy: 0.9307 - val_loss: 0.3401 - val_accuracy: 0.8603\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.2206 - accuracy: 0.9229 - val_loss: 0.3741 - val_accuracy: 0.8159\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2888 - accuracy: 0.8985 - val_loss: 0.3426 - val_accuracy: 0.8603\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.2245 - accuracy: 0.9176 - val_loss: 0.2927 - val_accuracy: 0.8635\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1593 - accuracy: 0.9406 - val_loss: 0.2766 - val_accuracy: 0.8762\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1636 - accuracy: 0.9473 - val_loss: 0.2714 - val_accuracy: 0.8952\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1749 - accuracy: 0.9331 - val_loss: 0.3825 - val_accuracy: 0.8667\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1291 - accuracy: 0.9466 - val_loss: 0.3063 - val_accuracy: 0.8794\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.2199 - accuracy: 0.9307 - val_loss: 0.3922 - val_accuracy: 0.8381\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1777 - accuracy: 0.9321 - val_loss: 0.4214 - val_accuracy: 0.8698\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1997 - accuracy: 0.9339 - val_loss: 0.3642 - val_accuracy: 0.8444\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1499 - accuracy: 0.9459 - val_loss: 0.2993 - val_accuracy: 0.8635\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1333 - accuracy: 0.9462 - val_loss: 0.3734 - val_accuracy: 0.8889\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1455 - accuracy: 0.9441 - val_loss: 0.3121 - val_accuracy: 0.8825\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.1351 - accuracy: 0.9473 - val_loss: 0.3746 - val_accuracy: 0.8508\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1551 - accuracy: 0.9452 - val_loss: 0.4899 - val_accuracy: 0.8127\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1645 - accuracy: 0.9402 - val_loss: 0.3216 - val_accuracy: 0.8794\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1637 - accuracy: 0.9399 - val_loss: 0.3121 - val_accuracy: 0.8635\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1230 - accuracy: 0.9554 - val_loss: 0.5450 - val_accuracy: 0.8794\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1755 - accuracy: 0.9406 - val_loss: 0.3432 - val_accuracy: 0.8794\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1675 - accuracy: 0.9434 - val_loss: 0.3637 - val_accuracy: 0.8857\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.2010 - accuracy: 0.9388 - val_loss: 0.3247 - val_accuracy: 0.8889\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.3358 - accuracy: 0.8903 - val_loss: 0.3166 - val_accuracy: 0.8730\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1922 - accuracy: 0.9271 - val_loss: 0.3579 - val_accuracy: 0.8667\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1414 - accuracy: 0.9448 - val_loss: 0.4187 - val_accuracy: 0.8730\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_5\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_240), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_24/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_96), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_24/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_241), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_24/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_97), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_24/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_242), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_24/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_98), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_24/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_249), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_24/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_99), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_24/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_250), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_25/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_100), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_25/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_251), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_25/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_101), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_25/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_252), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_25/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_102), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_25/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_259), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_25/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_103), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_25/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_260), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_26/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_104), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_26/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_261), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_26/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_105), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_26/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_262), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_26/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_106), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_26/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_269), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_26/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_107), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_26/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_270), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_27/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_108), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_27/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_271), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_27/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_109), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_27/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_272), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_27/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_110), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_27/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_279), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_27/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_111), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_27/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_280), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_28/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_112), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_28/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_281), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_28/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_113), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_28/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_282), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_28/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_114), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_28/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_289), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_28/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_115), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_28/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_290), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_29/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_116), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_29/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_291), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_29/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_117), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_29/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_292), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_29/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_118), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_29/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_299), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_29/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_119), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_29/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_300), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_30/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_120), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_30/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_301), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_30/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_121), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_30/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_302), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_30/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_122), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_30/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_309), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_30/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_123), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_30/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_310), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_31/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_124), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_31/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_311), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_31/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_125), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_31/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_312), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_31/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_126), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_31/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_319), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_31/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_127), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_31/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 30s 31ms/step - loss: 2.5218 - accuracy: 0.6821 - val_loss: 0.5040 - val_accuracy: 0.7548\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.5269 - accuracy: 0.7454 - val_loss: 0.4114 - val_accuracy: 0.7675\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4795 - accuracy: 0.7797 - val_loss: 0.3684 - val_accuracy: 0.8185\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4298 - accuracy: 0.7984 - val_loss: 0.3750 - val_accuracy: 0.8344\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4351 - accuracy: 0.8002 - val_loss: 0.3604 - val_accuracy: 0.8153\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4030 - accuracy: 0.8144 - val_loss: 0.3611 - val_accuracy: 0.8153\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3816 - accuracy: 0.8228 - val_loss: 0.4674 - val_accuracy: 0.8121\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3746 - accuracy: 0.8310 - val_loss: 0.4521 - val_accuracy: 0.7643\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3771 - accuracy: 0.8345 - val_loss: 0.4548 - val_accuracy: 0.7898\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3792 - accuracy: 0.8359 - val_loss: 0.3311 - val_accuracy: 0.8153\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3261 - accuracy: 0.8653 - val_loss: 0.5186 - val_accuracy: 0.7580\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.3139 - accuracy: 0.8674 - val_loss: 0.3178 - val_accuracy: 0.8344\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2973 - accuracy: 0.8692 - val_loss: 0.3128 - val_accuracy: 0.8280\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2891 - accuracy: 0.8777 - val_loss: 0.3412 - val_accuracy: 0.8217\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2733 - accuracy: 0.8858 - val_loss: 0.3401 - val_accuracy: 0.8822\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2826 - accuracy: 0.8900 - val_loss: 0.2767 - val_accuracy: 0.8790\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2890 - accuracy: 0.8900 - val_loss: 0.2921 - val_accuracy: 0.8790\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2414 - accuracy: 0.8936 - val_loss: 0.4779 - val_accuracy: 0.8280\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2734 - accuracy: 0.8978 - val_loss: 0.2850 - val_accuracy: 0.9172\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2237 - accuracy: 0.9059 - val_loss: 0.2918 - val_accuracy: 0.8790\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2407 - accuracy: 0.9081 - val_loss: 0.2721 - val_accuracy: 0.8726\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2535 - accuracy: 0.9017 - val_loss: 0.2836 - val_accuracy: 0.8917\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1793 - accuracy: 0.9342 - val_loss: 0.3132 - val_accuracy: 0.8885\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2300 - accuracy: 0.9102 - val_loss: 0.2766 - val_accuracy: 0.8822\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2450 - accuracy: 0.9074 - val_loss: 0.6203 - val_accuracy: 0.8599\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2881 - accuracy: 0.9059 - val_loss: 0.5576 - val_accuracy: 0.8121\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4032 - accuracy: 0.8575 - val_loss: 0.2597 - val_accuracy: 0.8726\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1914 - accuracy: 0.9247 - val_loss: 0.3248 - val_accuracy: 0.8758\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2137 - accuracy: 0.9229 - val_loss: 0.2606 - val_accuracy: 0.8885\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2145 - accuracy: 0.9282 - val_loss: 0.2945 - val_accuracy: 0.8567\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2186 - accuracy: 0.9158 - val_loss: 0.3173 - val_accuracy: 0.8790\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1994 - accuracy: 0.9356 - val_loss: 0.4235 - val_accuracy: 0.7962\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1856 - accuracy: 0.9233 - val_loss: 0.4079 - val_accuracy: 0.8790\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1711 - accuracy: 0.9399 - val_loss: 0.2908 - val_accuracy: 0.9108\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2588 - accuracy: 0.8953 - val_loss: 0.3052 - val_accuracy: 0.8439\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1834 - accuracy: 0.9342 - val_loss: 0.3416 - val_accuracy: 0.8949\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1403 - accuracy: 0.9473 - val_loss: 0.2844 - val_accuracy: 0.8822\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1469 - accuracy: 0.9484 - val_loss: 0.3810 - val_accuracy: 0.8949\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1434 - accuracy: 0.9445 - val_loss: 0.3042 - val_accuracy: 0.8917\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2638 - accuracy: 0.9088 - val_loss: 0.3578 - val_accuracy: 0.8535\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1283 - accuracy: 0.9519 - val_loss: 0.6779 - val_accuracy: 0.8503\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4955 - accuracy: 0.9035 - val_loss: 0.5799 - val_accuracy: 0.6815\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2897 - accuracy: 0.8670 - val_loss: 0.2725 - val_accuracy: 0.8854\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1391 - accuracy: 0.9480 - val_loss: 0.4084 - val_accuracy: 0.8567\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2327 - accuracy: 0.9335 - val_loss: 0.2765 - val_accuracy: 0.8854\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1321 - accuracy: 0.9530 - val_loss: 0.2551 - val_accuracy: 0.8822\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1832 - accuracy: 0.9356 - val_loss: 0.5021 - val_accuracy: 0.8535\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2326 - accuracy: 0.9243 - val_loss: 0.3758 - val_accuracy: 0.8726\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1311 - accuracy: 0.9526 - val_loss: 0.2809 - val_accuracy: 0.8917\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1146 - accuracy: 0.9576 - val_loss: 0.3329 - val_accuracy: 0.9013\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_6\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_320), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_32/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_128), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_32/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_321), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_32/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_129), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_32/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_322), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_32/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_130), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_32/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_329), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_32/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_131), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_32/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_330), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_33/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_132), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_33/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_331), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_33/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_133), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_33/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_332), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_33/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_134), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_33/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_339), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_33/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_135), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_33/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_340), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_34/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_136), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_34/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_341), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_34/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_137), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_34/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_342), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_34/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_138), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_34/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_349), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_34/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_139), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_34/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_350), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_35/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_140), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_35/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_351), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_35/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_141), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_35/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_352), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_35/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_142), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_35/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_359), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_35/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_143), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_35/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_360), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_36/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_144), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_36/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_361), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_36/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_145), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_36/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_362), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_36/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_146), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_36/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_369), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_36/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_147), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_36/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_370), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_37/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_148), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_37/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_371), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_37/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_149), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_37/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_372), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_37/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_150), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_37/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_379), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_37/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_151), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_37/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_380), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_38/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_152), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_38/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_381), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_38/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_153), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_38/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_382), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_38/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_154), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_38/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_389), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_38/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_155), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_38/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_390), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_39/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_156), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_39/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_391), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_39/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_157), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_39/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_392), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_39/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_158), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_39/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_399), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_39/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_159), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_39/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 28s 29ms/step - loss: 2.5364 - accuracy: 0.6906 - val_loss: 0.4904 - val_accuracy: 0.7580\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5163 - accuracy: 0.7475 - val_loss: 0.5107 - val_accuracy: 0.7229\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4773 - accuracy: 0.7705 - val_loss: 0.4546 - val_accuracy: 0.7580\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4453 - accuracy: 0.7889 - val_loss: 0.4557 - val_accuracy: 0.8089\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4302 - accuracy: 0.7924 - val_loss: 0.4333 - val_accuracy: 0.8217\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4090 - accuracy: 0.8080 - val_loss: 0.4289 - val_accuracy: 0.8248\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4279 - accuracy: 0.8030 - val_loss: 0.4406 - val_accuracy: 0.7898\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3926 - accuracy: 0.8119 - val_loss: 0.4355 - val_accuracy: 0.8121\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3788 - accuracy: 0.8335 - val_loss: 0.3728 - val_accuracy: 0.8248\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3543 - accuracy: 0.8331 - val_loss: 0.4398 - val_accuracy: 0.8057\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3375 - accuracy: 0.8476 - val_loss: 0.4313 - val_accuracy: 0.8089\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3119 - accuracy: 0.8610 - val_loss: 0.3461 - val_accuracy: 0.8631\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3315 - accuracy: 0.8607 - val_loss: 0.3408 - val_accuracy: 0.8439\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.3221 - accuracy: 0.8681 - val_loss: 0.4186 - val_accuracy: 0.8025\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2930 - accuracy: 0.8822 - val_loss: 0.3742 - val_accuracy: 0.8599\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2568 - accuracy: 0.8971 - val_loss: 0.3538 - val_accuracy: 0.8344\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2768 - accuracy: 0.8982 - val_loss: 0.4261 - val_accuracy: 0.7962\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2862 - accuracy: 0.8936 - val_loss: 0.3914 - val_accuracy: 0.8599\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2224 - accuracy: 0.9105 - val_loss: 0.4258 - val_accuracy: 0.8535\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.2535 - accuracy: 0.9042 - val_loss: 0.4233 - val_accuracy: 0.8535\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2201 - accuracy: 0.9158 - val_loss: 0.3762 - val_accuracy: 0.8535\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2194 - accuracy: 0.9144 - val_loss: 0.3304 - val_accuracy: 0.8885\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1792 - accuracy: 0.9264 - val_loss: 0.4626 - val_accuracy: 0.8599\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1887 - accuracy: 0.9279 - val_loss: 0.3257 - val_accuracy: 0.8790\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1934 - accuracy: 0.9325 - val_loss: 0.3718 - val_accuracy: 0.8631\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2476 - accuracy: 0.9158 - val_loss: 0.3812 - val_accuracy: 0.8694\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1887 - accuracy: 0.9219 - val_loss: 0.5178 - val_accuracy: 0.8408\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2419 - accuracy: 0.9272 - val_loss: 0.5412 - val_accuracy: 0.7930\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1998 - accuracy: 0.9219 - val_loss: 0.3252 - val_accuracy: 0.8758\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1493 - accuracy: 0.9470 - val_loss: 0.3610 - val_accuracy: 0.8599\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1668 - accuracy: 0.9349 - val_loss: 0.4052 - val_accuracy: 0.8503\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1473 - accuracy: 0.9427 - val_loss: 0.3084 - val_accuracy: 0.8885\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1758 - accuracy: 0.9399 - val_loss: 0.7981 - val_accuracy: 0.8089\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2138 - accuracy: 0.9247 - val_loss: 0.2984 - val_accuracy: 0.8790\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2367 - accuracy: 0.9303 - val_loss: 0.4695 - val_accuracy: 0.8662\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1589 - accuracy: 0.9388 - val_loss: 0.5339 - val_accuracy: 0.8503\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1469 - accuracy: 0.9445 - val_loss: 0.4674 - val_accuracy: 0.8662\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1873 - accuracy: 0.9399 - val_loss: 0.4709 - val_accuracy: 0.8726\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1490 - accuracy: 0.9438 - val_loss: 0.5028 - val_accuracy: 0.8599\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1254 - accuracy: 0.9501 - val_loss: 0.3391 - val_accuracy: 0.8790\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1979 - accuracy: 0.9392 - val_loss: 0.4745 - val_accuracy: 0.8567\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1151 - accuracy: 0.9593 - val_loss: 0.3758 - val_accuracy: 0.8662\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2333 - accuracy: 0.9300 - val_loss: 0.3622 - val_accuracy: 0.8471\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1681 - accuracy: 0.9342 - val_loss: 0.4188 - val_accuracy: 0.8758\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1264 - accuracy: 0.9519 - val_loss: 0.4866 - val_accuracy: 0.8631\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1673 - accuracy: 0.9431 - val_loss: 0.4370 - val_accuracy: 0.8408\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1986 - accuracy: 0.9303 - val_loss: 0.3578 - val_accuracy: 0.8917\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1314 - accuracy: 0.9544 - val_loss: 0.3330 - val_accuracy: 0.8854\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1354 - accuracy: 0.9491 - val_loss: 0.3778 - val_accuracy: 0.8694\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1901 - accuracy: 0.9346 - val_loss: 0.3258 - val_accuracy: 0.8758\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_7\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_400), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_40/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_160), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_40/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_401), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_40/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_161), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_40/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_402), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_40/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_162), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_40/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_409), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_40/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_163), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_40/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_410), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_41/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_164), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_41/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_411), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_41/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_165), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_41/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_412), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_41/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_166), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_41/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_419), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_41/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_167), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_41/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_420), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_42/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_168), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_42/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_421), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_42/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_169), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_42/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_422), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_42/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_170), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_42/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_429), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_42/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_171), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_42/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_430), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_43/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_172), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_43/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_431), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_43/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_173), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_43/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_432), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_43/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_174), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_43/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_439), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_43/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_175), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_43/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_440), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_44/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_176), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_44/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_441), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_44/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_177), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_44/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_442), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_44/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_178), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_44/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_449), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_44/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_179), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_44/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_450), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_45/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_180), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_45/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_451), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_45/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_181), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_45/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_452), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_45/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_182), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_45/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_459), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_45/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_183), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_45/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_460), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_46/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_184), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_46/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_461), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_46/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_185), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_46/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_462), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_46/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_186), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_46/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_469), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_46/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_187), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_46/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_470), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_47/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_188), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_47/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_471), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_47/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_189), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_47/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_472), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_47/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_190), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_47/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_479), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_47/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_191), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_47/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 29s 29ms/step - loss: 2.1720 - accuracy: 0.6881 - val_loss: 0.5894 - val_accuracy: 0.6242\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.5167 - accuracy: 0.7468 - val_loss: 0.4678 - val_accuracy: 0.7229\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5147 - accuracy: 0.7627 - val_loss: 0.4142 - val_accuracy: 0.7930\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4756 - accuracy: 0.7758 - val_loss: 0.4790 - val_accuracy: 0.7389\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4619 - accuracy: 0.7797 - val_loss: 0.3869 - val_accuracy: 0.7834\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4258 - accuracy: 0.8009 - val_loss: 0.3582 - val_accuracy: 0.8344\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4128 - accuracy: 0.8091 - val_loss: 0.3869 - val_accuracy: 0.8280\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3834 - accuracy: 0.8281 - val_loss: 0.3720 - val_accuracy: 0.8153\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3809 - accuracy: 0.8306 - val_loss: 0.3499 - val_accuracy: 0.8248\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.3467 - accuracy: 0.8469 - val_loss: 0.3317 - val_accuracy: 0.8471\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3667 - accuracy: 0.8391 - val_loss: 0.3286 - val_accuracy: 0.8535\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3040 - accuracy: 0.8709 - val_loss: 0.3180 - val_accuracy: 0.8408\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3422 - accuracy: 0.8586 - val_loss: 0.3256 - val_accuracy: 0.8535\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2933 - accuracy: 0.8752 - val_loss: 0.3323 - val_accuracy: 0.8599\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2935 - accuracy: 0.8734 - val_loss: 0.3755 - val_accuracy: 0.8439\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2563 - accuracy: 0.8992 - val_loss: 0.4044 - val_accuracy: 0.8439\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2409 - accuracy: 0.9013 - val_loss: 0.4408 - val_accuracy: 0.8217\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2224 - accuracy: 0.9056 - val_loss: 0.3045 - val_accuracy: 0.8439\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2294 - accuracy: 0.9070 - val_loss: 0.3161 - val_accuracy: 0.8567\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2296 - accuracy: 0.9130 - val_loss: 0.4600 - val_accuracy: 0.8312\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2445 - accuracy: 0.8960 - val_loss: 0.3721 - val_accuracy: 0.8726\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2483 - accuracy: 0.9098 - val_loss: 0.3382 - val_accuracy: 0.8662\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1714 - accuracy: 0.9346 - val_loss: 0.2754 - val_accuracy: 0.8854\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1778 - accuracy: 0.9303 - val_loss: 0.3323 - val_accuracy: 0.8758\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1729 - accuracy: 0.9310 - val_loss: 0.4878 - val_accuracy: 0.8312\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2073 - accuracy: 0.9275 - val_loss: 0.3540 - val_accuracy: 0.8790\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1768 - accuracy: 0.9339 - val_loss: 0.3348 - val_accuracy: 0.8631\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1790 - accuracy: 0.9300 - val_loss: 0.2735 - val_accuracy: 0.8822\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1847 - accuracy: 0.9275 - val_loss: 0.3250 - val_accuracy: 0.8790\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1681 - accuracy: 0.9385 - val_loss: 0.4669 - val_accuracy: 0.8439\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1595 - accuracy: 0.9409 - val_loss: 0.4324 - val_accuracy: 0.8662\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1443 - accuracy: 0.9434 - val_loss: 0.4242 - val_accuracy: 0.8535\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1591 - accuracy: 0.9364 - val_loss: 0.3623 - val_accuracy: 0.8854\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1405 - accuracy: 0.9593 - val_loss: 0.3401 - val_accuracy: 0.8662\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2347 - accuracy: 0.9169 - val_loss: 0.3655 - val_accuracy: 0.8535\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1125 - accuracy: 0.9551 - val_loss: 0.3905 - val_accuracy: 0.8790\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1464 - accuracy: 0.9455 - val_loss: 0.3591 - val_accuracy: 0.8822\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1818 - accuracy: 0.9431 - val_loss: 0.3286 - val_accuracy: 0.8981\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1465 - accuracy: 0.9484 - val_loss: 0.3595 - val_accuracy: 0.8822\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1674 - accuracy: 0.9385 - val_loss: 0.3600 - val_accuracy: 0.8280\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1556 - accuracy: 0.9455 - val_loss: 0.3086 - val_accuracy: 0.8758\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1700 - accuracy: 0.9395 - val_loss: 0.3346 - val_accuracy: 0.8694\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1487 - accuracy: 0.9466 - val_loss: 0.4083 - val_accuracy: 0.8376\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2249 - accuracy: 0.9183 - val_loss: 0.3080 - val_accuracy: 0.9013\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1489 - accuracy: 0.9480 - val_loss: 0.3352 - val_accuracy: 0.8758\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1214 - accuracy: 0.9505 - val_loss: 0.3246 - val_accuracy: 0.8662\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1698 - accuracy: 0.9402 - val_loss: 0.3004 - val_accuracy: 0.8822\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1460 - accuracy: 0.9530 - val_loss: 0.2958 - val_accuracy: 0.8981\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1739 - accuracy: 0.9353 - val_loss: 0.3426 - val_accuracy: 0.8599\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1219 - accuracy: 0.9551 - val_loss: 0.2873 - val_accuracy: 0.8822\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_8\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_480), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_48/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_192), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_48/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_481), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_48/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_193), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_48/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_482), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_48/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_194), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_48/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_489), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_48/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_195), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_48/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_490), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_49/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_196), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_49/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_491), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_49/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_197), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_49/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_492), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_49/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_198), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_49/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_499), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_49/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_199), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_49/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_500), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_50/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_200), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_50/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_501), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_50/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_201), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_50/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_502), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_50/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_202), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_50/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_509), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_50/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_203), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_50/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_510), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_51/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_204), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_51/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_511), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_51/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_205), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_51/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_512), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_51/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_206), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_51/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_519), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_51/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_207), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_51/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_520), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_52/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_208), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_52/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_521), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_52/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_209), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_52/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_522), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_52/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_210), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_52/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_529), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_52/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_211), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_52/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_530), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_53/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_212), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_53/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_531), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_53/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_213), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_53/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_532), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_53/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_214), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_53/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_539), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_53/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_215), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_53/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_540), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_54/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_216), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_54/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_541), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_54/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_217), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_54/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_542), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_54/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_218), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_54/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_549), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_54/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_219), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_54/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_550), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_55/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_220), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_55/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_551), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_55/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_221), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_55/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_552), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_55/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_222), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_55/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_559), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_55/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_223), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_55/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 29s 31ms/step - loss: 2.0264 - accuracy: 0.6839 - val_loss: 0.5253 - val_accuracy: 0.7261\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5054 - accuracy: 0.7553 - val_loss: 0.3861 - val_accuracy: 0.8185\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4765 - accuracy: 0.7652 - val_loss: 0.3947 - val_accuracy: 0.7548\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4638 - accuracy: 0.7832 - val_loss: 0.3978 - val_accuracy: 0.8121\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4401 - accuracy: 0.7995 - val_loss: 0.3435 - val_accuracy: 0.8376\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4424 - accuracy: 0.8030 - val_loss: 0.3762 - val_accuracy: 0.8280\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4240 - accuracy: 0.8158 - val_loss: 0.3276 - val_accuracy: 0.8280\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3863 - accuracy: 0.8292 - val_loss: 0.2875 - val_accuracy: 0.8726\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3704 - accuracy: 0.8356 - val_loss: 0.2928 - val_accuracy: 0.8567\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3477 - accuracy: 0.8444 - val_loss: 0.3454 - val_accuracy: 0.8248\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3261 - accuracy: 0.8586 - val_loss: 0.4174 - val_accuracy: 0.8153\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3066 - accuracy: 0.8692 - val_loss: 0.3089 - val_accuracy: 0.8599\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3165 - accuracy: 0.8752 - val_loss: 0.3006 - val_accuracy: 0.8439\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2987 - accuracy: 0.8716 - val_loss: 0.3172 - val_accuracy: 0.8631\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2763 - accuracy: 0.8858 - val_loss: 0.2906 - val_accuracy: 0.8599\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2620 - accuracy: 0.8943 - val_loss: 0.2551 - val_accuracy: 0.8822\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3045 - accuracy: 0.8851 - val_loss: 0.2516 - val_accuracy: 0.8854\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2326 - accuracy: 0.9091 - val_loss: 0.2493 - val_accuracy: 0.8822\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2335 - accuracy: 0.9141 - val_loss: 0.2860 - val_accuracy: 0.8344\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2435 - accuracy: 0.9074 - val_loss: 0.2863 - val_accuracy: 0.8822\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1948 - accuracy: 0.9254 - val_loss: 0.2652 - val_accuracy: 0.8917\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2860 - accuracy: 0.9013 - val_loss: 0.3110 - val_accuracy: 0.8567\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2099 - accuracy: 0.9151 - val_loss: 0.2923 - val_accuracy: 0.8758\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1918 - accuracy: 0.9257 - val_loss: 0.2481 - val_accuracy: 0.9013\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1671 - accuracy: 0.9381 - val_loss: 0.2486 - val_accuracy: 0.9108\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1740 - accuracy: 0.9356 - val_loss: 0.2789 - val_accuracy: 0.8758\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1758 - accuracy: 0.9339 - val_loss: 0.2267 - val_accuracy: 0.9045\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1865 - accuracy: 0.9303 - val_loss: 0.2911 - val_accuracy: 0.8758\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1657 - accuracy: 0.9392 - val_loss: 0.2531 - val_accuracy: 0.9045\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1524 - accuracy: 0.9441 - val_loss: 0.2825 - val_accuracy: 0.8758\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1549 - accuracy: 0.9420 - val_loss: 0.2697 - val_accuracy: 0.8885\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2478 - accuracy: 0.9187 - val_loss: 0.2746 - val_accuracy: 0.8822\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1657 - accuracy: 0.9392 - val_loss: 0.2893 - val_accuracy: 0.8981\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1675 - accuracy: 0.9434 - val_loss: 0.2437 - val_accuracy: 0.9045\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1380 - accuracy: 0.9484 - val_loss: 0.3035 - val_accuracy: 0.8949\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2406 - accuracy: 0.9130 - val_loss: 0.2965 - val_accuracy: 0.8981\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1308 - accuracy: 0.9487 - val_loss: 0.3420 - val_accuracy: 0.8758\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1791 - accuracy: 0.9360 - val_loss: 0.3037 - val_accuracy: 0.8694\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1241 - accuracy: 0.9526 - val_loss: 0.2739 - val_accuracy: 0.8599\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1405 - accuracy: 0.9455 - val_loss: 0.3322 - val_accuracy: 0.8885\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1551 - accuracy: 0.9466 - val_loss: 0.2241 - val_accuracy: 0.8981\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1870 - accuracy: 0.9356 - val_loss: 0.3110 - val_accuracy: 0.8917\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1690 - accuracy: 0.9420 - val_loss: 0.2340 - val_accuracy: 0.9013\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1608 - accuracy: 0.9374 - val_loss: 0.3617 - val_accuracy: 0.8854\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1263 - accuracy: 0.9540 - val_loss: 0.3837 - val_accuracy: 0.9013\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2242 - accuracy: 0.9307 - val_loss: 0.3944 - val_accuracy: 0.8726\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1608 - accuracy: 0.9533 - val_loss: 0.2977 - val_accuracy: 0.8726\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1631 - accuracy: 0.9399 - val_loss: 0.4519 - val_accuracy: 0.8758\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1391 - accuracy: 0.9512 - val_loss: 0.2492 - val_accuracy: 0.8981\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2162 - accuracy: 0.9325 - val_loss: 0.2936 - val_accuracy: 0.9013\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_9\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_560), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_56/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_224), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_56/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_561), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_56/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_225), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_56/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_562), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_56/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_226), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_56/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_569), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_56/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_227), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_56/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_570), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_57/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_228), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_57/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_571), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_57/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_229), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_57/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_572), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_57/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_230), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_57/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_579), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_57/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_231), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_57/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_580), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_58/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_232), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_58/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_581), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_58/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_233), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_58/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_582), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_58/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_234), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_58/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_589), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_58/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_235), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_58/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_590), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_59/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_236), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_59/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_591), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_59/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_237), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_59/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_592), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_59/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_238), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_59/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_599), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_59/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_239), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_59/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_600), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_60/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_240), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_60/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_601), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_60/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_241), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_60/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_602), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_60/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_242), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_60/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_609), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_60/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_243), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_60/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_610), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_61/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_244), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_61/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_611), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_61/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_245), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_61/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_612), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_61/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_246), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_61/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_619), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_61/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_247), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_61/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_620), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_62/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_248), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_62/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_621), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_62/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_249), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_62/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_622), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_62/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_250), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_62/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_629), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_62/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_251), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_62/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_630), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_63/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_252), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_63/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_631), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_63/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_253), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_63/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_632), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_63/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_254), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_63/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_639), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_63/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_255), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_63/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 43s 45ms/step - loss: 2.1783 - accuracy: 0.6898 - val_loss: 0.4403 - val_accuracy: 0.7968\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.5225 - accuracy: 0.7584 - val_loss: 0.4246 - val_accuracy: 0.8032\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.4535 - accuracy: 0.7733 - val_loss: 0.3891 - val_accuracy: 0.8000\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.4484 - accuracy: 0.7881 - val_loss: 0.4108 - val_accuracy: 0.8000\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 31s 43ms/step - loss: 0.4179 - accuracy: 0.7991 - val_loss: 0.4282 - val_accuracy: 0.8063\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.4048 - accuracy: 0.8154 - val_loss: 0.3654 - val_accuracy: 0.8222\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 31s 43ms/step - loss: 0.3853 - accuracy: 0.8299 - val_loss: 0.4338 - val_accuracy: 0.7492\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.4054 - accuracy: 0.8320 - val_loss: 0.4412 - val_accuracy: 0.7905\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 31s 43ms/step - loss: 0.3426 - accuracy: 0.8461 - val_loss: 0.4022 - val_accuracy: 0.8286\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.3359 - accuracy: 0.8521 - val_loss: 0.3395 - val_accuracy: 0.8413\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.3332 - accuracy: 0.8603 - val_loss: 0.3825 - val_accuracy: 0.8095\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.3098 - accuracy: 0.8691 - val_loss: 0.4509 - val_accuracy: 0.8317\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.2937 - accuracy: 0.8709 - val_loss: 0.3254 - val_accuracy: 0.8603\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2609 - accuracy: 0.8854 - val_loss: 0.3599 - val_accuracy: 0.8444\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.2701 - accuracy: 0.8928 - val_loss: 0.3057 - val_accuracy: 0.8762\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.3043 - accuracy: 0.8755 - val_loss: 0.3577 - val_accuracy: 0.8571\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.3410 - accuracy: 0.8659 - val_loss: 0.3645 - val_accuracy: 0.8349\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2277 - accuracy: 0.9048 - val_loss: 0.3911 - val_accuracy: 0.8540\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2108 - accuracy: 0.9179 - val_loss: 0.3023 - val_accuracy: 0.8603\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2058 - accuracy: 0.9208 - val_loss: 0.2763 - val_accuracy: 0.8921\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1768 - accuracy: 0.9296 - val_loss: 0.3722 - val_accuracy: 0.8571\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.2420 - accuracy: 0.9084 - val_loss: 0.4547 - val_accuracy: 0.7714\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.2478 - accuracy: 0.9010 - val_loss: 0.4514 - val_accuracy: 0.8508\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 28s 39ms/step - loss: 0.1992 - accuracy: 0.9215 - val_loss: 0.3246 - val_accuracy: 0.8794\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.1659 - accuracy: 0.9370 - val_loss: 0.4742 - val_accuracy: 0.8635\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1628 - accuracy: 0.9339 - val_loss: 0.4009 - val_accuracy: 0.8762\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.2156 - accuracy: 0.9275 - val_loss: 0.3158 - val_accuracy: 0.8635\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1986 - accuracy: 0.9254 - val_loss: 0.3801 - val_accuracy: 0.8603\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.2135 - accuracy: 0.9271 - val_loss: 0.3606 - val_accuracy: 0.8730\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1371 - accuracy: 0.9455 - val_loss: 0.3451 - val_accuracy: 0.8667\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1683 - accuracy: 0.9363 - val_loss: 0.4941 - val_accuracy: 0.8381\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1485 - accuracy: 0.9427 - val_loss: 0.3939 - val_accuracy: 0.8921\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1674 - accuracy: 0.9349 - val_loss: 0.4263 - val_accuracy: 0.8698\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1408 - accuracy: 0.9494 - val_loss: 0.3137 - val_accuracy: 0.8952\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.1474 - accuracy: 0.9441 - val_loss: 0.3524 - val_accuracy: 0.8667\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1561 - accuracy: 0.9438 - val_loss: 0.4908 - val_accuracy: 0.8698\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1580 - accuracy: 0.9430 - val_loss: 0.5950 - val_accuracy: 0.8476\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1750 - accuracy: 0.9353 - val_loss: 0.3347 - val_accuracy: 0.8730\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.2359 - accuracy: 0.9158 - val_loss: 0.3059 - val_accuracy: 0.8889\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1937 - accuracy: 0.9353 - val_loss: 0.3341 - val_accuracy: 0.8635\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1659 - accuracy: 0.9416 - val_loss: 0.2869 - val_accuracy: 0.8730\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1205 - accuracy: 0.9593 - val_loss: 0.3578 - val_accuracy: 0.8794\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1430 - accuracy: 0.9498 - val_loss: 0.4177 - val_accuracy: 0.8667\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2604 - accuracy: 0.9229 - val_loss: 0.4808 - val_accuracy: 0.8571\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2379 - accuracy: 0.9144 - val_loss: 0.3013 - val_accuracy: 0.8794\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1287 - accuracy: 0.9515 - val_loss: 0.3823 - val_accuracy: 0.8889\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.1312 - accuracy: 0.9522 - val_loss: 0.4185 - val_accuracy: 0.8317\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.1274 - accuracy: 0.9561 - val_loss: 0.4528 - val_accuracy: 0.8698\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1300 - accuracy: 0.9498 - val_loss: 0.2804 - val_accuracy: 0.8857\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1438 - accuracy: 0.9469 - val_loss: 0.3358 - val_accuracy: 0.8730\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_10\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_640), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_64/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_256), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_64/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_641), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_64/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_257), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_64/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_642), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_64/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_258), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_64/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_649), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_64/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_259), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_64/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_650), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_65/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_260), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_65/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_651), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_65/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_261), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_65/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_652), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_65/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_262), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_65/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_659), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_65/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_263), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_65/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_660), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_66/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_264), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_66/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_661), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_66/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_265), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_66/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_662), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_66/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_266), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_66/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_669), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_66/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_267), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_66/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_670), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_67/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_268), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_67/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_671), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_67/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_269), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_67/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_672), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_67/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_270), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_67/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_679), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_67/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_271), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_67/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_680), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_68/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_272), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_68/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_681), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_68/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_273), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_68/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_682), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_68/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_274), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_68/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_689), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_68/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_275), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_68/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_690), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_69/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_276), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_69/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_691), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_69/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_277), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_69/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_692), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_69/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_278), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_69/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_699), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_69/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_279), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_69/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_700), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_70/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_280), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_70/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_701), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_70/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_281), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_70/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_702), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_70/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_282), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_70/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_709), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_70/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_283), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_70/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_710), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_71/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_284), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_71/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_711), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_71/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_285), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_71/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_712), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_71/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_286), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_71/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_719), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_71/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_287), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_71/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 29s 30ms/step - loss: 2.3585 - accuracy: 0.6609 - val_loss: 0.6774 - val_accuracy: 0.6688\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5169 - accuracy: 0.7514 - val_loss: 0.4535 - val_accuracy: 0.7325\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5013 - accuracy: 0.7595 - val_loss: 0.4790 - val_accuracy: 0.7548\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4539 - accuracy: 0.7871 - val_loss: 0.4770 - val_accuracy: 0.6943\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4339 - accuracy: 0.7953 - val_loss: 0.4104 - val_accuracy: 0.7898\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4323 - accuracy: 0.8016 - val_loss: 0.3777 - val_accuracy: 0.7994\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.3782 - accuracy: 0.8214 - val_loss: 0.4024 - val_accuracy: 0.8089\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3578 - accuracy: 0.8296 - val_loss: 0.3935 - val_accuracy: 0.7898\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3764 - accuracy: 0.8331 - val_loss: 0.3522 - val_accuracy: 0.8567\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3462 - accuracy: 0.8483 - val_loss: 0.3575 - val_accuracy: 0.8312\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3224 - accuracy: 0.8614 - val_loss: 0.3897 - val_accuracy: 0.8312\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3190 - accuracy: 0.8571 - val_loss: 0.3773 - val_accuracy: 0.8408\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3226 - accuracy: 0.8632 - val_loss: 0.4085 - val_accuracy: 0.8376\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.3720 - accuracy: 0.8423 - val_loss: 0.4544 - val_accuracy: 0.8217\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3158 - accuracy: 0.8685 - val_loss: 0.3610 - val_accuracy: 0.8503\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2964 - accuracy: 0.8674 - val_loss: 0.3802 - val_accuracy: 0.8439\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2361 - accuracy: 0.8985 - val_loss: 0.3371 - val_accuracy: 0.8631\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2281 - accuracy: 0.9013 - val_loss: 0.4202 - val_accuracy: 0.8408\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2532 - accuracy: 0.8971 - val_loss: 0.4303 - val_accuracy: 0.8471\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2299 - accuracy: 0.9109 - val_loss: 0.3322 - val_accuracy: 0.8726\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2474 - accuracy: 0.8943 - val_loss: 0.3194 - val_accuracy: 0.8726\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.2177 - accuracy: 0.9127 - val_loss: 0.3017 - val_accuracy: 0.8694\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2180 - accuracy: 0.9095 - val_loss: 0.3412 - val_accuracy: 0.8631\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1932 - accuracy: 0.9215 - val_loss: 0.4200 - val_accuracy: 0.8471\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3316 - accuracy: 0.8695 - val_loss: 0.4009 - val_accuracy: 0.8408\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1901 - accuracy: 0.9211 - val_loss: 0.3554 - val_accuracy: 0.8790\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1758 - accuracy: 0.9279 - val_loss: 0.4364 - val_accuracy: 0.8344\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1756 - accuracy: 0.9219 - val_loss: 0.4683 - val_accuracy: 0.8439\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1853 - accuracy: 0.9321 - val_loss: 0.3952 - val_accuracy: 0.8408\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1773 - accuracy: 0.9303 - val_loss: 0.3384 - val_accuracy: 0.8599\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1826 - accuracy: 0.9328 - val_loss: 0.3508 - val_accuracy: 0.8503\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2034 - accuracy: 0.9318 - val_loss: 0.3699 - val_accuracy: 0.8662\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1518 - accuracy: 0.9420 - val_loss: 0.2833 - val_accuracy: 0.9076\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2085 - accuracy: 0.9215 - val_loss: 0.3686 - val_accuracy: 0.8631\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1974 - accuracy: 0.9247 - val_loss: 0.3062 - val_accuracy: 0.8758\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1413 - accuracy: 0.9459 - val_loss: 0.3218 - val_accuracy: 0.8854\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1520 - accuracy: 0.9409 - val_loss: 0.5335 - val_accuracy: 0.8471\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2343 - accuracy: 0.9165 - val_loss: 0.4126 - val_accuracy: 0.8631\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1516 - accuracy: 0.9455 - val_loss: 0.3676 - val_accuracy: 0.8408\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2294 - accuracy: 0.9183 - val_loss: 0.3249 - val_accuracy: 0.8822\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1252 - accuracy: 0.9494 - val_loss: 0.3125 - val_accuracy: 0.8822\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1378 - accuracy: 0.9494 - val_loss: 0.4178 - val_accuracy: 0.8790\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1998 - accuracy: 0.9229 - val_loss: 0.3549 - val_accuracy: 0.8631\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1582 - accuracy: 0.9417 - val_loss: 0.3936 - val_accuracy: 0.8471\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1471 - accuracy: 0.9470 - val_loss: 0.4093 - val_accuracy: 0.8694\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1464 - accuracy: 0.9501 - val_loss: 0.4287 - val_accuracy: 0.8631\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1263 - accuracy: 0.9565 - val_loss: 0.3120 - val_accuracy: 0.8981\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2007 - accuracy: 0.9289 - val_loss: 0.3831 - val_accuracy: 0.8631\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1633 - accuracy: 0.9346 - val_loss: 0.3555 - val_accuracy: 0.8439\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2365 - accuracy: 0.9134 - val_loss: 0.3060 - val_accuracy: 0.8535\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_11\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_720), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_72/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_288), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_72/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_721), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_72/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_289), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_72/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_722), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_72/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_290), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_72/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_729), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_72/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_291), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_72/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_730), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_73/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_292), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_73/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_731), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_73/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_293), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_73/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_732), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_73/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_294), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_73/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_739), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_73/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_295), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_73/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_740), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_74/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_296), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_74/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_741), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_74/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_297), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_74/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_742), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_74/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_298), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_74/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_749), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_74/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_299), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_74/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_750), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_75/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_300), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_75/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_751), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_75/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_301), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_75/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_752), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_75/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_302), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_75/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_759), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_75/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_303), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_75/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_760), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_76/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_304), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_76/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_761), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_76/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_305), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_76/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_762), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_76/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_306), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_76/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_769), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_76/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_307), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_76/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_770), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_77/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_308), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_77/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_771), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_77/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_309), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_77/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_772), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_77/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_310), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_77/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_779), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_77/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_311), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_77/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_780), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_78/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_312), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_78/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_781), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_78/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_313), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_78/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_782), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_78/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_314), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_78/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_789), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_78/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_315), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_78/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_790), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_79/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_316), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_79/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_791), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_79/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_317), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_79/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_792), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_79/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_318), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_79/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_799), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_79/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_319), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_79/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 28s 29ms/step - loss: 2.6597 - accuracy: 0.6793 - val_loss: 0.5232 - val_accuracy: 0.7580\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5114 - accuracy: 0.7443 - val_loss: 0.4882 - val_accuracy: 0.7070\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4648 - accuracy: 0.7666 - val_loss: 0.4364 - val_accuracy: 0.7580\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4668 - accuracy: 0.7705 - val_loss: 0.4303 - val_accuracy: 0.7898\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4489 - accuracy: 0.7885 - val_loss: 0.5638 - val_accuracy: 0.7548\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4105 - accuracy: 0.8136 - val_loss: 0.4416 - val_accuracy: 0.7580\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4268 - accuracy: 0.8122 - val_loss: 0.3725 - val_accuracy: 0.8217\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4048 - accuracy: 0.8154 - val_loss: 0.4491 - val_accuracy: 0.7611\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3548 - accuracy: 0.8416 - val_loss: 0.4875 - val_accuracy: 0.7803\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3490 - accuracy: 0.8490 - val_loss: 0.4152 - val_accuracy: 0.7994\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3239 - accuracy: 0.8533 - val_loss: 0.3931 - val_accuracy: 0.7930\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3166 - accuracy: 0.8663 - val_loss: 0.3621 - val_accuracy: 0.8121\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2821 - accuracy: 0.8773 - val_loss: 0.3750 - val_accuracy: 0.8662\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2884 - accuracy: 0.8745 - val_loss: 0.5717 - val_accuracy: 0.7611\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2752 - accuracy: 0.8844 - val_loss: 0.3551 - val_accuracy: 0.8312\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2842 - accuracy: 0.8939 - val_loss: 0.3416 - val_accuracy: 0.8344\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2656 - accuracy: 0.8936 - val_loss: 0.3831 - val_accuracy: 0.8503\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2489 - accuracy: 0.9024 - val_loss: 0.9107 - val_accuracy: 0.7166\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2496 - accuracy: 0.9042 - val_loss: 0.4835 - val_accuracy: 0.7771\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2334 - accuracy: 0.9109 - val_loss: 0.3510 - val_accuracy: 0.8694\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1996 - accuracy: 0.9162 - val_loss: 0.3174 - val_accuracy: 0.8790\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2097 - accuracy: 0.9151 - val_loss: 0.5116 - val_accuracy: 0.8726\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2442 - accuracy: 0.9098 - val_loss: 0.4276 - val_accuracy: 0.8439\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2130 - accuracy: 0.9233 - val_loss: 0.3755 - val_accuracy: 0.8917\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2146 - accuracy: 0.9190 - val_loss: 0.4471 - val_accuracy: 0.8312\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1968 - accuracy: 0.9208 - val_loss: 0.5288 - val_accuracy: 0.8567\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1672 - accuracy: 0.9339 - val_loss: 0.4938 - val_accuracy: 0.8439\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1911 - accuracy: 0.9356 - val_loss: 0.2762 - val_accuracy: 0.8917\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2383 - accuracy: 0.9169 - val_loss: 0.2850 - val_accuracy: 0.8726\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1540 - accuracy: 0.9399 - val_loss: 0.3511 - val_accuracy: 0.8662\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2846 - accuracy: 0.9081 - val_loss: 0.3242 - val_accuracy: 0.8631\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1358 - accuracy: 0.9470 - val_loss: 0.3712 - val_accuracy: 0.8917\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1466 - accuracy: 0.9424 - val_loss: 0.4125 - val_accuracy: 0.8790\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1425 - accuracy: 0.9459 - val_loss: 0.3630 - val_accuracy: 0.8822\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1458 - accuracy: 0.9424 - val_loss: 0.3496 - val_accuracy: 0.8981\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2511 - accuracy: 0.9137 - val_loss: 0.3977 - val_accuracy: 0.8726\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1875 - accuracy: 0.9402 - val_loss: 0.4511 - val_accuracy: 0.8312\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1982 - accuracy: 0.9314 - val_loss: 0.6106 - val_accuracy: 0.8057\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1432 - accuracy: 0.9466 - val_loss: 0.3448 - val_accuracy: 0.8885\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1541 - accuracy: 0.9448 - val_loss: 0.3919 - val_accuracy: 0.8662\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1411 - accuracy: 0.9473 - val_loss: 0.4595 - val_accuracy: 0.8726\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1576 - accuracy: 0.9371 - val_loss: 0.3110 - val_accuracy: 0.8917\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1505 - accuracy: 0.9466 - val_loss: 0.4791 - val_accuracy: 0.8121\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2021 - accuracy: 0.9261 - val_loss: 0.3626 - val_accuracy: 0.8822\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1303 - accuracy: 0.9533 - val_loss: 0.4136 - val_accuracy: 0.8694\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2220 - accuracy: 0.9275 - val_loss: 0.4569 - val_accuracy: 0.8312\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1387 - accuracy: 0.9470 - val_loss: 0.6225 - val_accuracy: 0.8185\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1521 - accuracy: 0.9491 - val_loss: 0.4066 - val_accuracy: 0.8790\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1297 - accuracy: 0.9487 - val_loss: 0.3235 - val_accuracy: 0.8790\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1737 - accuracy: 0.9434 - val_loss: 0.3240 - val_accuracy: 0.8885\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_12\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_800), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_80/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_320), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_80/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_801), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_80/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_321), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_80/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_802), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_80/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_322), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_80/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_809), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_80/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_323), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_80/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_810), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_81/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_324), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_81/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_811), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_81/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_325), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_81/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_812), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_81/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_326), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_81/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_819), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_81/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_327), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_81/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_820), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_82/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_328), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_82/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_821), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_82/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_329), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_82/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_822), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_82/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_330), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_82/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_829), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_82/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_331), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_82/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_830), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_83/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_332), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_83/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_831), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_83/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_333), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_83/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_832), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_83/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_334), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_83/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_839), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_83/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_335), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_83/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_840), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_84/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_336), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_84/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_841), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_84/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_337), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_84/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_842), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_84/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_338), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_84/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_849), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_84/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_339), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_84/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_850), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_85/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_340), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_85/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_851), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_85/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_341), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_85/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_852), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_85/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_342), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_85/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_859), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_85/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_343), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_85/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_860), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_86/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_344), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_86/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_861), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_86/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_345), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_86/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_862), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_86/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_346), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_86/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_869), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_86/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_347), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_86/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_870), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_87/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_348), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_87/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_871), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_87/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_349), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_87/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_872), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_87/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_350), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_87/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_879), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_87/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_351), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_87/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 29s 30ms/step - loss: 2.3444 - accuracy: 0.6750 - val_loss: 0.4244 - val_accuracy: 0.8057\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.5249 - accuracy: 0.7436 - val_loss: 0.4069 - val_accuracy: 0.8025\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4739 - accuracy: 0.7641 - val_loss: 0.3854 - val_accuracy: 0.8121\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4536 - accuracy: 0.7850 - val_loss: 0.3401 - val_accuracy: 0.8567\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4380 - accuracy: 0.7896 - val_loss: 0.3500 - val_accuracy: 0.8280\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4687 - accuracy: 0.7829 - val_loss: 0.3052 - val_accuracy: 0.8758\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4236 - accuracy: 0.8062 - val_loss: 0.3636 - val_accuracy: 0.8057\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3959 - accuracy: 0.8175 - val_loss: 0.3351 - val_accuracy: 0.8439\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3717 - accuracy: 0.8246 - val_loss: 0.3500 - val_accuracy: 0.8471\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3585 - accuracy: 0.8419 - val_loss: 0.3171 - val_accuracy: 0.8790\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3338 - accuracy: 0.8494 - val_loss: 0.4206 - val_accuracy: 0.8344\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3263 - accuracy: 0.8557 - val_loss: 0.2814 - val_accuracy: 0.8758\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3056 - accuracy: 0.8755 - val_loss: 0.3087 - val_accuracy: 0.8408\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3027 - accuracy: 0.8716 - val_loss: 0.3427 - val_accuracy: 0.8599\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3051 - accuracy: 0.8777 - val_loss: 0.3101 - val_accuracy: 0.8631\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2697 - accuracy: 0.8837 - val_loss: 0.2570 - val_accuracy: 0.8790\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2571 - accuracy: 0.8960 - val_loss: 0.3375 - val_accuracy: 0.8439\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2400 - accuracy: 0.9010 - val_loss: 0.3129 - val_accuracy: 0.8790\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2653 - accuracy: 0.9010 - val_loss: 0.2582 - val_accuracy: 0.8726\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2011 - accuracy: 0.9169 - val_loss: 0.2803 - val_accuracy: 0.9013\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2426 - accuracy: 0.9021 - val_loss: 0.2282 - val_accuracy: 0.9236\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1691 - accuracy: 0.9247 - val_loss: 0.2966 - val_accuracy: 0.8854\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.2314 - accuracy: 0.9162 - val_loss: 0.3253 - val_accuracy: 0.8662\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.2838 - accuracy: 0.8960 - val_loss: 0.2973 - val_accuracy: 0.8917\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1960 - accuracy: 0.9349 - val_loss: 0.2615 - val_accuracy: 0.9013\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2436 - accuracy: 0.9162 - val_loss: 0.2649 - val_accuracy: 0.9045\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1704 - accuracy: 0.9378 - val_loss: 0.7257 - val_accuracy: 0.8057\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2199 - accuracy: 0.9215 - val_loss: 0.2272 - val_accuracy: 0.8949\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1595 - accuracy: 0.9385 - val_loss: 0.2274 - val_accuracy: 0.9204\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1989 - accuracy: 0.9264 - val_loss: 0.2804 - val_accuracy: 0.8981\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1552 - accuracy: 0.9452 - val_loss: 0.2316 - val_accuracy: 0.9140\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1761 - accuracy: 0.9399 - val_loss: 0.2063 - val_accuracy: 0.9172\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1500 - accuracy: 0.9470 - val_loss: 0.1977 - val_accuracy: 0.9299\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2541 - accuracy: 0.9151 - val_loss: 0.3414 - val_accuracy: 0.8758\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1486 - accuracy: 0.9441 - val_loss: 0.1704 - val_accuracy: 0.9490\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1424 - accuracy: 0.9441 - val_loss: 0.2787 - val_accuracy: 0.8885\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1485 - accuracy: 0.9480 - val_loss: 0.1975 - val_accuracy: 0.9204\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1594 - accuracy: 0.9420 - val_loss: 0.2281 - val_accuracy: 0.9299\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1478 - accuracy: 0.9477 - val_loss: 0.7374 - val_accuracy: 0.8694\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2051 - accuracy: 0.9279 - val_loss: 0.1964 - val_accuracy: 0.9108\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1246 - accuracy: 0.9526 - val_loss: 0.2681 - val_accuracy: 0.9236\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1564 - accuracy: 0.9473 - val_loss: 0.1958 - val_accuracy: 0.9204\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1827 - accuracy: 0.9346 - val_loss: 0.2145 - val_accuracy: 0.9236\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1690 - accuracy: 0.9448 - val_loss: 0.2991 - val_accuracy: 0.8981\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1374 - accuracy: 0.9498 - val_loss: 0.2617 - val_accuracy: 0.9268\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1846 - accuracy: 0.9392 - val_loss: 0.2399 - val_accuracy: 0.9299\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1997 - accuracy: 0.9364 - val_loss: 0.2590 - val_accuracy: 0.8981\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1168 - accuracy: 0.9554 - val_loss: 0.2663 - val_accuracy: 0.8949\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1993 - accuracy: 0.9303 - val_loss: 0.2967 - val_accuracy: 0.8854\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1710 - accuracy: 0.9420 - val_loss: 0.2878 - val_accuracy: 0.8854\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_13\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_880), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_88/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_352), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_88/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_881), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_88/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_353), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_88/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_882), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_88/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_354), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_88/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_889), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_88/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_355), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_88/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_890), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_89/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_356), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_89/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_891), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_89/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_357), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_89/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_892), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_89/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_358), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_89/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_899), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_89/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_359), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_89/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_900), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_90/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_360), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_90/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_901), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_90/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_361), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_90/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_902), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_90/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_362), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_90/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_909), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_90/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_363), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_90/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_910), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_91/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_364), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_91/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_911), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_91/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_365), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_91/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_912), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_91/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_366), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_91/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_919), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_91/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_367), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_91/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_920), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_92/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_368), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_92/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_921), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_92/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_369), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_92/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_922), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_92/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_370), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_92/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_929), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_92/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_371), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_92/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_930), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_93/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_372), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_93/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_931), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_93/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_373), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_93/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_932), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_93/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_374), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_93/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_939), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_93/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_375), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_93/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_940), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_94/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_376), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_94/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_941), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_94/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_377), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_94/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_942), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_94/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_378), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_94/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_949), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_94/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_379), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_94/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_950), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_95/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_380), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_95/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_951), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_95/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_381), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_95/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_952), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_95/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_382), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_95/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_959), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_95/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_383), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_95/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 29s 29ms/step - loss: 2.1832 - accuracy: 0.6871 - val_loss: 0.4584 - val_accuracy: 0.7516\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.5105 - accuracy: 0.7493 - val_loss: 0.4584 - val_accuracy: 0.7516\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4651 - accuracy: 0.7832 - val_loss: 0.5322 - val_accuracy: 0.7643\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4385 - accuracy: 0.7974 - val_loss: 0.4206 - val_accuracy: 0.7930\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4460 - accuracy: 0.7977 - val_loss: 0.3685 - val_accuracy: 0.8408\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4264 - accuracy: 0.8066 - val_loss: 0.3994 - val_accuracy: 0.7898\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.4002 - accuracy: 0.8197 - val_loss: 0.3892 - val_accuracy: 0.8057\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3635 - accuracy: 0.8356 - val_loss: 0.4353 - val_accuracy: 0.7675\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3653 - accuracy: 0.8476 - val_loss: 0.5579 - val_accuracy: 0.7739\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.3413 - accuracy: 0.8568 - val_loss: 0.4798 - val_accuracy: 0.8280\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3451 - accuracy: 0.8536 - val_loss: 0.3659 - val_accuracy: 0.8535\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2723 - accuracy: 0.8822 - val_loss: 0.3516 - val_accuracy: 0.8312\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2821 - accuracy: 0.8837 - val_loss: 0.3976 - val_accuracy: 0.7930\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2848 - accuracy: 0.8780 - val_loss: 0.3957 - val_accuracy: 0.8376\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2641 - accuracy: 0.8921 - val_loss: 0.4163 - val_accuracy: 0.8439\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2359 - accuracy: 0.9070 - val_loss: 0.3862 - val_accuracy: 0.8439\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2325 - accuracy: 0.9095 - val_loss: 0.5925 - val_accuracy: 0.8153\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1977 - accuracy: 0.9208 - val_loss: 0.3817 - val_accuracy: 0.8439\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1839 - accuracy: 0.9286 - val_loss: 0.3573 - val_accuracy: 0.8503\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1877 - accuracy: 0.9190 - val_loss: 0.4915 - val_accuracy: 0.8439\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1869 - accuracy: 0.9264 - val_loss: 0.4032 - val_accuracy: 0.8344\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1793 - accuracy: 0.9321 - val_loss: 0.3943 - val_accuracy: 0.8439\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1845 - accuracy: 0.9367 - val_loss: 0.4438 - val_accuracy: 0.8057\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2654 - accuracy: 0.8975 - val_loss: 0.4637 - val_accuracy: 0.8535\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2184 - accuracy: 0.9261 - val_loss: 0.4664 - val_accuracy: 0.7771\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1443 - accuracy: 0.9385 - val_loss: 0.3806 - val_accuracy: 0.8694\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1368 - accuracy: 0.9463 - val_loss: 0.4711 - val_accuracy: 0.8567\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2525 - accuracy: 0.8989 - val_loss: 0.3125 - val_accuracy: 0.8567\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1572 - accuracy: 0.9434 - val_loss: 0.4709 - val_accuracy: 0.8503\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1281 - accuracy: 0.9508 - val_loss: 0.3644 - val_accuracy: 0.8599\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1516 - accuracy: 0.9434 - val_loss: 0.3809 - val_accuracy: 0.8631\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1417 - accuracy: 0.9470 - val_loss: 0.4896 - val_accuracy: 0.8599\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1653 - accuracy: 0.9402 - val_loss: 0.3810 - val_accuracy: 0.8217\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2422 - accuracy: 0.9219 - val_loss: 0.4095 - val_accuracy: 0.8503\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2072 - accuracy: 0.9332 - val_loss: 0.5029 - val_accuracy: 0.8471\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1913 - accuracy: 0.9424 - val_loss: 0.5119 - val_accuracy: 0.7771\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1452 - accuracy: 0.9399 - val_loss: 0.3644 - val_accuracy: 0.8439\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 19s 28ms/step - loss: 0.1306 - accuracy: 0.9516 - val_loss: 0.3833 - val_accuracy: 0.8567\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2706 - accuracy: 0.9236 - val_loss: 0.3783 - val_accuracy: 0.8567\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1314 - accuracy: 0.9487 - val_loss: 0.4824 - val_accuracy: 0.8535\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1189 - accuracy: 0.9569 - val_loss: 0.4659 - val_accuracy: 0.8439\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1156 - accuracy: 0.9572 - val_loss: 0.6235 - val_accuracy: 0.8503\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1362 - accuracy: 0.9544 - val_loss: 0.4916 - val_accuracy: 0.8057\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1891 - accuracy: 0.9424 - val_loss: 0.3832 - val_accuracy: 0.8344\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1487 - accuracy: 0.9420 - val_loss: 0.4457 - val_accuracy: 0.8631\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 19s 26ms/step - loss: 0.1668 - accuracy: 0.9420 - val_loss: 0.5386 - val_accuracy: 0.8408\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1462 - accuracy: 0.9466 - val_loss: 0.3643 - val_accuracy: 0.8726\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1381 - accuracy: 0.9537 - val_loss: 0.4539 - val_accuracy: 0.8503\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1397 - accuracy: 0.9445 - val_loss: 0.4511 - val_accuracy: 0.8599\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2680 - accuracy: 0.9190 - val_loss: 0.3923 - val_accuracy: 0.8344\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_14\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_960), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_96/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_384), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_96/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_961), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_96/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_385), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_96/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_962), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_96/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_386), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_96/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_969), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_96/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_387), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_96/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_970), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_97/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_388), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_97/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_971), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_97/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_389), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_97/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_972), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_97/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_390), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_97/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_979), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_97/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_391), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_97/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_980), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_98/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_392), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_98/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_981), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_98/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_393), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_98/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_982), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_98/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_394), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_98/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_989), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_98/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_395), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_98/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_990), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_99/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_396), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_99/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_991), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_99/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_397), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_99/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_992), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_99/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_398), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_99/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_999), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_99/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_399), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_99/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1000), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_100/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_400), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_100/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1001), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_100/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_401), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_100/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1002), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_100/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_402), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_100/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1009), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_100/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_403), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_100/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1010), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_101/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_404), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_101/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1011), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_101/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_405), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_101/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1012), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_101/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_406), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_101/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1019), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_101/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_407), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_101/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1020), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_102/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_408), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_102/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1021), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_102/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_409), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_102/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1022), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_102/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_410), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_102/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1029), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_102/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_411), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_102/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1030), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_103/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_412), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_103/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1031), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_103/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_413), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_103/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1032), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_103/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_414), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_103/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1039), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_103/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_415), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_103/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 41s 43ms/step - loss: 2.1669 - accuracy: 0.6845 - val_loss: 0.4868 - val_accuracy: 0.7270\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.5242 - accuracy: 0.7520 - val_loss: 0.3846 - val_accuracy: 0.8286\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.4963 - accuracy: 0.7733 - val_loss: 0.3816 - val_accuracy: 0.8159\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.4611 - accuracy: 0.7895 - val_loss: 0.4658 - val_accuracy: 0.7556\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.4552 - accuracy: 0.7973 - val_loss: 0.4414 - val_accuracy: 0.8159\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.4550 - accuracy: 0.7934 - val_loss: 0.3688 - val_accuracy: 0.8063\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.4107 - accuracy: 0.8111 - val_loss: 0.3851 - val_accuracy: 0.8286\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.3861 - accuracy: 0.8284 - val_loss: 0.3381 - val_accuracy: 0.8508\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.3467 - accuracy: 0.8444 - val_loss: 0.3976 - val_accuracy: 0.8317\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 28s 39ms/step - loss: 0.3571 - accuracy: 0.8419 - val_loss: 0.4571 - val_accuracy: 0.8095\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 28s 39ms/step - loss: 0.3321 - accuracy: 0.8550 - val_loss: 0.2962 - val_accuracy: 0.8413\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 27s 39ms/step - loss: 0.3116 - accuracy: 0.8656 - val_loss: 0.4296 - val_accuracy: 0.7746\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.3165 - accuracy: 0.8741 - val_loss: 0.3833 - val_accuracy: 0.8317\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 29s 40ms/step - loss: 0.3195 - accuracy: 0.8613 - val_loss: 0.4336 - val_accuracy: 0.8095\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 29s 40ms/step - loss: 0.2917 - accuracy: 0.8868 - val_loss: 0.2971 - val_accuracy: 0.8635\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2904 - accuracy: 0.8836 - val_loss: 0.3465 - val_accuracy: 0.8476\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 29s 40ms/step - loss: 0.2617 - accuracy: 0.8819 - val_loss: 0.3387 - val_accuracy: 0.8190\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2632 - accuracy: 0.8893 - val_loss: 0.4945 - val_accuracy: 0.7016\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.2475 - accuracy: 0.8964 - val_loss: 0.4041 - val_accuracy: 0.8571\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2549 - accuracy: 0.9056 - val_loss: 0.2841 - val_accuracy: 0.8667\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2189 - accuracy: 0.9091 - val_loss: 0.4430 - val_accuracy: 0.8762\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.3172 - accuracy: 0.8949 - val_loss: 0.3020 - val_accuracy: 0.8730\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.2403 - accuracy: 0.9140 - val_loss: 0.2802 - val_accuracy: 0.8762\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1924 - accuracy: 0.9271 - val_loss: 0.3533 - val_accuracy: 0.8857\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1962 - accuracy: 0.9268 - val_loss: 0.2692 - val_accuracy: 0.8762\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.1726 - accuracy: 0.9328 - val_loss: 0.4178 - val_accuracy: 0.8730\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.1710 - accuracy: 0.9289 - val_loss: 0.3559 - val_accuracy: 0.8889\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2456 - accuracy: 0.9091 - val_loss: 0.2727 - val_accuracy: 0.8921\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1668 - accuracy: 0.9399 - val_loss: 0.2821 - val_accuracy: 0.8794\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1761 - accuracy: 0.9399 - val_loss: 0.3285 - val_accuracy: 0.8825\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1538 - accuracy: 0.9469 - val_loss: 0.3410 - val_accuracy: 0.8508\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1759 - accuracy: 0.9321 - val_loss: 0.2691 - val_accuracy: 0.9048\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1899 - accuracy: 0.9303 - val_loss: 0.2578 - val_accuracy: 0.8571\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1842 - accuracy: 0.9296 - val_loss: 0.3665 - val_accuracy: 0.8952\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.1234 - accuracy: 0.9487 - val_loss: 0.3475 - val_accuracy: 0.8921\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1506 - accuracy: 0.9462 - val_loss: 0.4676 - val_accuracy: 0.8254\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1680 - accuracy: 0.9484 - val_loss: 0.2978 - val_accuracy: 0.8825\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.1739 - accuracy: 0.9370 - val_loss: 0.4202 - val_accuracy: 0.8508\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 30s 42ms/step - loss: 0.2405 - accuracy: 0.9183 - val_loss: 0.3519 - val_accuracy: 0.8730\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1348 - accuracy: 0.9476 - val_loss: 0.4079 - val_accuracy: 0.8794\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1393 - accuracy: 0.9476 - val_loss: 0.3139 - val_accuracy: 0.8889\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 29s 42ms/step - loss: 0.1982 - accuracy: 0.9254 - val_loss: 0.5506 - val_accuracy: 0.8540\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1225 - accuracy: 0.9576 - val_loss: 0.2867 - val_accuracy: 0.8857\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1652 - accuracy: 0.9430 - val_loss: 0.2993 - val_accuracy: 0.8952\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1471 - accuracy: 0.9469 - val_loss: 0.4083 - val_accuracy: 0.8571\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1701 - accuracy: 0.9402 - val_loss: 0.4197 - val_accuracy: 0.8984\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1860 - accuracy: 0.9374 - val_loss: 0.2740 - val_accuracy: 0.8984\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 28s 40ms/step - loss: 0.2000 - accuracy: 0.9239 - val_loss: 0.3695 - val_accuracy: 0.8540\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.1235 - accuracy: 0.9614 - val_loss: 0.4763 - val_accuracy: 0.8286\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 29s 41ms/step - loss: 0.2646 - accuracy: 0.9271 - val_loss: 0.2238 - val_accuracy: 0.9270\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_15\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1040), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_104/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_416), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_104/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1041), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_104/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_417), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_104/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1042), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_104/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_418), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_104/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1049), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_104/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_419), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_104/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1050), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_105/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_420), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_105/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1051), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_105/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_421), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_105/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1052), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_105/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_422), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_105/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1059), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_105/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_423), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_105/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1060), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_106/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_424), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_106/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1061), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_106/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_425), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_106/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1062), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_106/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_426), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_106/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1069), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_106/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_427), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_106/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1070), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_107/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_428), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_107/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1071), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_107/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_429), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_107/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1072), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_107/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_430), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_107/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1079), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_107/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_431), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_107/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1080), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_108/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_432), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_108/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1081), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_108/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_433), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_108/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1082), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_108/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_434), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_108/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1089), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_108/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_435), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_108/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1090), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_109/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_436), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_109/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1091), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_109/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_437), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_109/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1092), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_109/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_438), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_109/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1099), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_109/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_439), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_109/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1100), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_110/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_440), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_110/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1101), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_110/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_441), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_110/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1102), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_110/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_442), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_110/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1109), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_110/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_443), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_110/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1110), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_111/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_444), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_111/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1111), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_111/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_445), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_111/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1112), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_111/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_446), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_111/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1119), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_111/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_447), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_111/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 29s 30ms/step - loss: 2.2842 - accuracy: 0.6892 - val_loss: 0.4314 - val_accuracy: 0.7771\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5022 - accuracy: 0.7641 - val_loss: 0.5825 - val_accuracy: 0.7070\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4824 - accuracy: 0.7755 - val_loss: 0.3980 - val_accuracy: 0.7866\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4659 - accuracy: 0.7779 - val_loss: 0.3547 - val_accuracy: 0.8248\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4240 - accuracy: 0.7974 - val_loss: 0.3198 - val_accuracy: 0.8376\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4055 - accuracy: 0.8154 - val_loss: 0.3323 - val_accuracy: 0.8344\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3916 - accuracy: 0.8257 - val_loss: 0.4011 - val_accuracy: 0.8344\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3835 - accuracy: 0.8289 - val_loss: 0.4181 - val_accuracy: 0.8025\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3573 - accuracy: 0.8434 - val_loss: 0.3574 - val_accuracy: 0.8471\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3506 - accuracy: 0.8494 - val_loss: 0.3569 - val_accuracy: 0.8280\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3286 - accuracy: 0.8635 - val_loss: 0.3840 - val_accuracy: 0.8376\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3132 - accuracy: 0.8632 - val_loss: 0.2994 - val_accuracy: 0.8631\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3149 - accuracy: 0.8660 - val_loss: 0.3126 - val_accuracy: 0.8567\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2874 - accuracy: 0.8798 - val_loss: 0.3418 - val_accuracy: 0.8471\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2646 - accuracy: 0.8918 - val_loss: 0.3029 - val_accuracy: 0.8471\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2398 - accuracy: 0.8971 - val_loss: 0.2941 - val_accuracy: 0.8790\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2123 - accuracy: 0.9151 - val_loss: 0.3739 - val_accuracy: 0.8885\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2221 - accuracy: 0.9063 - val_loss: 0.4929 - val_accuracy: 0.8344\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2401 - accuracy: 0.9017 - val_loss: 0.3332 - val_accuracy: 0.8758\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2587 - accuracy: 0.9095 - val_loss: 0.3910 - val_accuracy: 0.8248\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2553 - accuracy: 0.9109 - val_loss: 0.4072 - val_accuracy: 0.8599\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2635 - accuracy: 0.9116 - val_loss: 0.3006 - val_accuracy: 0.8567\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1888 - accuracy: 0.9268 - val_loss: 0.2717 - val_accuracy: 0.8694\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2458 - accuracy: 0.9134 - val_loss: 0.2925 - val_accuracy: 0.8822\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1570 - accuracy: 0.9417 - val_loss: 0.3208 - val_accuracy: 0.8790\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1396 - accuracy: 0.9448 - val_loss: 0.4213 - val_accuracy: 0.8790\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2036 - accuracy: 0.9275 - val_loss: 0.2506 - val_accuracy: 0.8949\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1835 - accuracy: 0.9332 - val_loss: 0.3105 - val_accuracy: 0.9045\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2209 - accuracy: 0.9151 - val_loss: 0.3383 - val_accuracy: 0.8599\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2340 - accuracy: 0.9208 - val_loss: 0.3143 - val_accuracy: 0.8822\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1483 - accuracy: 0.9512 - val_loss: 0.2979 - val_accuracy: 0.8790\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1532 - accuracy: 0.9413 - val_loss: 0.3407 - val_accuracy: 0.8726\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1438 - accuracy: 0.9477 - val_loss: 0.3195 - val_accuracy: 0.9045\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2632 - accuracy: 0.9385 - val_loss: 0.9714 - val_accuracy: 0.8790\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3390 - accuracy: 0.8822 - val_loss: 0.4238 - val_accuracy: 0.8535\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2069 - accuracy: 0.9321 - val_loss: 0.4097 - val_accuracy: 0.7898\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1896 - accuracy: 0.9293 - val_loss: 0.2706 - val_accuracy: 0.8694\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2146 - accuracy: 0.9356 - val_loss: 0.2958 - val_accuracy: 0.8854\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1565 - accuracy: 0.9491 - val_loss: 0.3447 - val_accuracy: 0.8758\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1940 - accuracy: 0.9395 - val_loss: 0.3194 - val_accuracy: 0.8822\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1676 - accuracy: 0.9459 - val_loss: 0.3728 - val_accuracy: 0.8790\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1901 - accuracy: 0.9268 - val_loss: 0.2827 - val_accuracy: 0.8822\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1614 - accuracy: 0.9424 - val_loss: 0.3117 - val_accuracy: 0.8503\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1519 - accuracy: 0.9406 - val_loss: 0.4147 - val_accuracy: 0.9045\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.2518 - accuracy: 0.9148 - val_loss: 0.3539 - val_accuracy: 0.8885\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 19s 27ms/step - loss: 0.1250 - accuracy: 0.9512 - val_loss: 0.2973 - val_accuracy: 0.8917\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2521 - accuracy: 0.9247 - val_loss: 0.2782 - val_accuracy: 0.9108\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1920 - accuracy: 0.9307 - val_loss: 0.3399 - val_accuracy: 0.8790\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1991 - accuracy: 0.9349 - val_loss: 0.4522 - val_accuracy: 0.8822\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1257 - accuracy: 0.9579 - val_loss: 0.3187 - val_accuracy: 0.8981\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_16\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1120), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_112/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_448), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_112/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1121), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_112/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_449), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_112/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1122), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_112/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_450), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_112/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1129), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_112/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_451), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_112/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1130), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_113/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_452), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_113/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1131), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_113/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_453), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_113/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1132), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_113/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_454), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_113/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1139), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_113/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_455), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_113/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1140), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_114/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_456), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_114/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1141), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_114/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_457), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_114/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1142), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_114/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_458), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_114/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1149), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_114/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_459), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_114/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1150), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_115/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_460), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_115/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1151), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_115/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_461), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_115/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1152), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_115/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_462), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_115/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1159), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_115/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_463), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_115/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1160), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_116/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_464), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_116/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1161), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_116/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_465), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_116/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1162), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_116/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_466), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_116/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1169), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_116/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_467), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_116/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1170), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_117/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_468), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_117/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1171), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_117/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_469), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_117/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1172), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_117/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_470), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_117/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1179), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_117/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_471), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_117/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1180), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_118/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_472), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_118/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1181), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_118/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_473), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_118/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1182), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_118/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_474), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_118/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1189), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_118/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_475), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_118/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1190), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_119/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_476), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_119/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1191), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_119/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_477), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_119/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1192), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_119/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_478), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_119/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1199), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_119/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_479), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_119/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 30s 31ms/step - loss: 2.1654 - accuracy: 0.6665 - val_loss: 0.4918 - val_accuracy: 0.7611\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.5194 - accuracy: 0.7493 - val_loss: 0.4770 - val_accuracy: 0.7325\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4673 - accuracy: 0.7677 - val_loss: 0.4006 - val_accuracy: 0.8089\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.4418 - accuracy: 0.7903 - val_loss: 0.5729 - val_accuracy: 0.7389\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4316 - accuracy: 0.7942 - val_loss: 0.3982 - val_accuracy: 0.8248\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4097 - accuracy: 0.8034 - val_loss: 0.6175 - val_accuracy: 0.7866\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4303 - accuracy: 0.8133 - val_loss: 0.3997 - val_accuracy: 0.8248\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3972 - accuracy: 0.8207 - val_loss: 0.3937 - val_accuracy: 0.7834\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3899 - accuracy: 0.8221 - val_loss: 0.4487 - val_accuracy: 0.8057\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.3462 - accuracy: 0.8465 - val_loss: 0.4555 - val_accuracy: 0.8344\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3446 - accuracy: 0.8455 - val_loss: 0.4231 - val_accuracy: 0.8376\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3178 - accuracy: 0.8603 - val_loss: 0.3888 - val_accuracy: 0.8344\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3228 - accuracy: 0.8706 - val_loss: 0.3982 - val_accuracy: 0.8503\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2850 - accuracy: 0.8723 - val_loss: 0.3716 - val_accuracy: 0.8344\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3278 - accuracy: 0.8667 - val_loss: 0.4064 - val_accuracy: 0.8344\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2282 - accuracy: 0.8996 - val_loss: 0.3500 - val_accuracy: 0.8567\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2519 - accuracy: 0.8943 - val_loss: 0.3882 - val_accuracy: 0.8503\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2339 - accuracy: 0.9084 - val_loss: 0.3568 - val_accuracy: 0.8631\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2343 - accuracy: 0.9031 - val_loss: 0.3979 - val_accuracy: 0.8599\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2089 - accuracy: 0.9116 - val_loss: 0.3988 - val_accuracy: 0.8599\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2186 - accuracy: 0.9144 - val_loss: 0.3524 - val_accuracy: 0.8280\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2740 - accuracy: 0.8868 - val_loss: 0.4753 - val_accuracy: 0.7293\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2265 - accuracy: 0.9070 - val_loss: 0.3827 - val_accuracy: 0.8599\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1678 - accuracy: 0.9325 - val_loss: 0.4099 - val_accuracy: 0.8726\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1714 - accuracy: 0.9392 - val_loss: 0.4054 - val_accuracy: 0.8631\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2027 - accuracy: 0.9254 - val_loss: 0.3631 - val_accuracy: 0.8694\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1688 - accuracy: 0.9360 - val_loss: 0.4167 - val_accuracy: 0.8885\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2135 - accuracy: 0.9226 - val_loss: 0.2997 - val_accuracy: 0.8631\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3754 - accuracy: 0.8833 - val_loss: 0.3137 - val_accuracy: 0.8631\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1604 - accuracy: 0.9328 - val_loss: 0.3617 - val_accuracy: 0.8662\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1778 - accuracy: 0.9399 - val_loss: 0.4243 - val_accuracy: 0.8248\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1941 - accuracy: 0.9314 - val_loss: 0.4057 - val_accuracy: 0.8758\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2375 - accuracy: 0.9236 - val_loss: 0.3634 - val_accuracy: 0.8822\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1700 - accuracy: 0.9399 - val_loss: 0.4862 - val_accuracy: 0.7930\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1933 - accuracy: 0.9367 - val_loss: 0.4645 - val_accuracy: 0.8376\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1505 - accuracy: 0.9466 - val_loss: 0.4106 - val_accuracy: 0.8694\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 22s 30ms/step - loss: 0.1414 - accuracy: 0.9491 - val_loss: 0.4710 - val_accuracy: 0.8567\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1258 - accuracy: 0.9540 - val_loss: 0.5826 - val_accuracy: 0.8694\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1771 - accuracy: 0.9346 - val_loss: 0.3608 - val_accuracy: 0.9108\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1392 - accuracy: 0.9491 - val_loss: 0.4409 - val_accuracy: 0.8854\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2078 - accuracy: 0.9243 - val_loss: 0.4736 - val_accuracy: 0.8822\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1532 - accuracy: 0.9498 - val_loss: 0.5446 - val_accuracy: 0.8790\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1434 - accuracy: 0.9441 - val_loss: 0.3557 - val_accuracy: 0.9013\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1509 - accuracy: 0.9441 - val_loss: 0.3701 - val_accuracy: 0.8822\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1694 - accuracy: 0.9409 - val_loss: 0.5651 - val_accuracy: 0.8822\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.1745 - accuracy: 0.9356 - val_loss: 0.5621 - val_accuracy: 0.8344\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1286 - accuracy: 0.9544 - val_loss: 0.4824 - val_accuracy: 0.8599\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1623 - accuracy: 0.9402 - val_loss: 0.3953 - val_accuracy: 0.8854\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1540 - accuracy: 0.9466 - val_loss: 0.3503 - val_accuracy: 0.8758\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1364 - accuracy: 0.9512 - val_loss: 0.3586 - val_accuracy: 0.8694\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_17\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1200), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_120/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_480), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_120/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1201), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_120/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_481), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_120/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1202), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_120/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_482), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_120/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1209), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_120/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_483), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_120/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1210), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_121/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_484), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_121/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1211), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_121/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_485), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_121/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1212), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_121/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_486), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_121/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1219), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_121/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_487), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_121/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1220), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_122/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_488), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_122/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1221), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_122/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_489), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_122/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1222), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_122/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_490), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_122/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1229), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_122/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_491), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_122/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1230), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_123/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_492), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_123/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1231), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_123/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_493), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_123/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1232), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_123/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_494), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_123/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1239), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_123/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_495), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_123/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1240), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_124/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_496), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_124/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1241), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_124/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_497), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_124/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1242), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_124/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_498), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_124/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1249), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_124/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_499), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_124/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1250), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_125/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_500), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_125/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1251), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_125/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_501), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_125/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1252), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_125/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_502), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_125/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1259), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_125/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_503), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_125/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1260), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_126/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_504), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_126/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1261), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_126/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_505), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_126/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1262), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_126/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_506), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_126/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1269), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_126/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_507), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_126/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1270), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_127/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_508), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_127/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1271), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_127/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_509), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_127/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1272), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_127/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_510), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_127/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1279), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_127/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_511), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_127/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 31s 32ms/step - loss: 2.1956 - accuracy: 0.6846 - val_loss: 0.4488 - val_accuracy: 0.7675\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.5030 - accuracy: 0.7535 - val_loss: 0.4085 - val_accuracy: 0.8121\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4860 - accuracy: 0.7719 - val_loss: 0.3875 - val_accuracy: 0.7898\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4532 - accuracy: 0.7988 - val_loss: 0.4213 - val_accuracy: 0.8089\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4587 - accuracy: 0.7843 - val_loss: 0.3652 - val_accuracy: 0.8057\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4351 - accuracy: 0.8069 - val_loss: 0.4610 - val_accuracy: 0.6879\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3973 - accuracy: 0.8115 - val_loss: 0.3826 - val_accuracy: 0.8057\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3908 - accuracy: 0.8129 - val_loss: 0.4186 - val_accuracy: 0.7643\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3705 - accuracy: 0.8437 - val_loss: 0.3154 - val_accuracy: 0.8694\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3464 - accuracy: 0.8490 - val_loss: 0.3365 - val_accuracy: 0.8408\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3233 - accuracy: 0.8617 - val_loss: 0.4061 - val_accuracy: 0.8057\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2985 - accuracy: 0.8670 - val_loss: 0.3261 - val_accuracy: 0.8567\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2831 - accuracy: 0.8822 - val_loss: 0.3072 - val_accuracy: 0.8726\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2633 - accuracy: 0.8925 - val_loss: 0.3236 - val_accuracy: 0.8344\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2479 - accuracy: 0.8957 - val_loss: 0.3101 - val_accuracy: 0.8694\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2484 - accuracy: 0.8992 - val_loss: 0.2584 - val_accuracy: 0.8981\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2253 - accuracy: 0.9112 - val_loss: 0.3174 - val_accuracy: 0.8535\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2305 - accuracy: 0.9120 - val_loss: 0.4133 - val_accuracy: 0.8439\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.2488 - accuracy: 0.9102 - val_loss: 0.3445 - val_accuracy: 0.8567\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2225 - accuracy: 0.9141 - val_loss: 0.4090 - val_accuracy: 0.8726\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2515 - accuracy: 0.9010 - val_loss: 0.3255 - val_accuracy: 0.8662\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1822 - accuracy: 0.9339 - val_loss: 0.3666 - val_accuracy: 0.8376\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2034 - accuracy: 0.9233 - val_loss: 0.3132 - val_accuracy: 0.8694\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2864 - accuracy: 0.9130 - val_loss: 0.4667 - val_accuracy: 0.8599\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1890 - accuracy: 0.9257 - val_loss: 0.5109 - val_accuracy: 0.8312\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2047 - accuracy: 0.9243 - val_loss: 0.3283 - val_accuracy: 0.8726\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1838 - accuracy: 0.9325 - val_loss: 0.4482 - val_accuracy: 0.8185\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1272 - accuracy: 0.9530 - val_loss: 0.3378 - val_accuracy: 0.8662\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1615 - accuracy: 0.9356 - val_loss: 0.3538 - val_accuracy: 0.8439\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1662 - accuracy: 0.9455 - val_loss: 0.3248 - val_accuracy: 0.8726\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.1588 - accuracy: 0.9409 - val_loss: 0.3911 - val_accuracy: 0.8376\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1253 - accuracy: 0.9547 - val_loss: 0.3343 - val_accuracy: 0.8503\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1728 - accuracy: 0.9367 - val_loss: 0.4058 - val_accuracy: 0.8535\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1556 - accuracy: 0.9367 - val_loss: 0.5435 - val_accuracy: 0.8535\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1637 - accuracy: 0.9402 - val_loss: 0.3319 - val_accuracy: 0.8885\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1591 - accuracy: 0.9484 - val_loss: 0.3275 - val_accuracy: 0.8599\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1597 - accuracy: 0.9431 - val_loss: 0.3380 - val_accuracy: 0.8376\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1693 - accuracy: 0.9395 - val_loss: 0.3549 - val_accuracy: 0.8567\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1732 - accuracy: 0.9399 - val_loss: 0.4604 - val_accuracy: 0.8567\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1476 - accuracy: 0.9477 - val_loss: 0.6009 - val_accuracy: 0.8471\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1239 - accuracy: 0.9600 - val_loss: 0.8911 - val_accuracy: 0.8535\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1512 - accuracy: 0.9441 - val_loss: 0.4516 - val_accuracy: 0.8439\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1515 - accuracy: 0.9455 - val_loss: 1.0672 - val_accuracy: 0.8248\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2052 - accuracy: 0.9307 - val_loss: 0.4683 - val_accuracy: 0.8503\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1486 - accuracy: 0.9480 - val_loss: 0.4954 - val_accuracy: 0.8185\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2592 - accuracy: 0.9219 - val_loss: 0.2915 - val_accuracy: 0.8726\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.0942 - accuracy: 0.9650 - val_loss: 0.3931 - val_accuracy: 0.8885\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1136 - accuracy: 0.9562 - val_loss: 0.4133 - val_accuracy: 0.8344\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1464 - accuracy: 0.9463 - val_loss: 0.3389 - val_accuracy: 0.8949\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2252 - accuracy: 0.9342 - val_loss: 0.3398 - val_accuracy: 0.8408\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_18\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1280), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_128/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_512), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_128/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1281), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_128/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_513), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_128/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1282), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_128/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_514), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_128/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1289), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_128/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_515), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_128/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1290), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_129/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_516), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_129/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1291), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_129/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_517), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_129/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1292), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_129/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_518), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_129/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1299), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_129/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_519), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_129/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1300), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_130/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_520), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_130/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1301), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_130/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_521), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_130/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1302), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_130/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_522), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_130/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1309), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_130/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_523), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_130/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1310), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_131/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_524), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_131/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1311), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_131/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_525), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_131/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1312), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_131/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_526), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_131/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1319), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_131/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_527), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_131/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1320), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_132/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_528), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_132/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1321), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_132/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_529), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_132/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1322), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_132/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_530), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_132/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1329), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_132/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_531), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_132/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1330), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_133/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_532), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_133/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1331), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_133/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_533), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_133/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1332), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_133/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_534), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_133/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1339), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_133/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_535), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_133/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1340), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_134/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_536), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_134/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1341), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_134/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_537), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_134/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1342), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_134/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_538), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_134/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1349), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_134/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_539), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_134/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1350), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_135/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_540), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_135/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1351), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_135/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_541), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_135/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1352), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_135/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_542), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_135/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1359), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_135/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_543), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_135/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 30s 31ms/step - loss: 2.0545 - accuracy: 0.6853 - val_loss: 0.6263 - val_accuracy: 0.6306\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.5157 - accuracy: 0.7606 - val_loss: 0.4522 - val_accuracy: 0.8025\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4574 - accuracy: 0.7801 - val_loss: 0.3638 - val_accuracy: 0.7994\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4595 - accuracy: 0.7917 - val_loss: 0.3429 - val_accuracy: 0.8185\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4472 - accuracy: 0.7953 - val_loss: 0.3700 - val_accuracy: 0.8089\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4476 - accuracy: 0.8048 - val_loss: 0.5580 - val_accuracy: 0.7834\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4079 - accuracy: 0.8133 - val_loss: 0.3396 - val_accuracy: 0.8408\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3878 - accuracy: 0.8264 - val_loss: 0.9309 - val_accuracy: 0.6911\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3685 - accuracy: 0.8370 - val_loss: 0.3255 - val_accuracy: 0.8376\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.3556 - accuracy: 0.8458 - val_loss: 0.3919 - val_accuracy: 0.8089\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.3706 - accuracy: 0.8479 - val_loss: 0.2660 - val_accuracy: 0.8854\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 23s 32ms/step - loss: 0.3456 - accuracy: 0.8632 - val_loss: 0.2804 - val_accuracy: 0.8567\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.3108 - accuracy: 0.8748 - val_loss: 0.4095 - val_accuracy: 0.7930\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3243 - accuracy: 0.8628 - val_loss: 0.2242 - val_accuracy: 0.8885\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2653 - accuracy: 0.8851 - val_loss: 0.4193 - val_accuracy: 0.8121\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2637 - accuracy: 0.8911 - val_loss: 0.2708 - val_accuracy: 0.8758\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2510 - accuracy: 0.8943 - val_loss: 0.2513 - val_accuracy: 0.8854\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2321 - accuracy: 0.9081 - val_loss: 0.4885 - val_accuracy: 0.8694\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2219 - accuracy: 0.9095 - val_loss: 0.2974 - val_accuracy: 0.8790\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2292 - accuracy: 0.9070 - val_loss: 0.3408 - val_accuracy: 0.8439\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2639 - accuracy: 0.9017 - val_loss: 0.2606 - val_accuracy: 0.8917\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1980 - accuracy: 0.9197 - val_loss: 0.2252 - val_accuracy: 0.8854\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1911 - accuracy: 0.9272 - val_loss: 0.2466 - val_accuracy: 0.8790\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1860 - accuracy: 0.9293 - val_loss: 0.3313 - val_accuracy: 0.8790\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1956 - accuracy: 0.9250 - val_loss: 0.3183 - val_accuracy: 0.8822\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2071 - accuracy: 0.9282 - val_loss: 0.2704 - val_accuracy: 0.8885\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2430 - accuracy: 0.9173 - val_loss: 0.3409 - val_accuracy: 0.8885\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1653 - accuracy: 0.9399 - val_loss: 0.2198 - val_accuracy: 0.9140\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1694 - accuracy: 0.9367 - val_loss: 0.2528 - val_accuracy: 0.9013\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.1616 - accuracy: 0.9388 - val_loss: 0.2647 - val_accuracy: 0.8885\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1475 - accuracy: 0.9459 - val_loss: 0.3297 - val_accuracy: 0.8631\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2559 - accuracy: 0.9102 - val_loss: 0.2506 - val_accuracy: 0.8949\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1455 - accuracy: 0.9466 - val_loss: 0.2119 - val_accuracy: 0.9013\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1656 - accuracy: 0.9409 - val_loss: 0.2971 - val_accuracy: 0.8854\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2405 - accuracy: 0.9190 - val_loss: 0.2573 - val_accuracy: 0.9076\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1874 - accuracy: 0.9282 - val_loss: 0.2485 - val_accuracy: 0.8981\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1802 - accuracy: 0.9335 - val_loss: 0.2302 - val_accuracy: 0.8949\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1474 - accuracy: 0.9452 - val_loss: 0.2250 - val_accuracy: 0.9108\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1917 - accuracy: 0.9346 - val_loss: 0.3398 - val_accuracy: 0.8535\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2454 - accuracy: 0.9162 - val_loss: 0.3054 - val_accuracy: 0.8822\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1899 - accuracy: 0.9402 - val_loss: 0.2360 - val_accuracy: 0.8790\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1492 - accuracy: 0.9431 - val_loss: 0.2651 - val_accuracy: 0.8981\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1741 - accuracy: 0.9318 - val_loss: 0.2616 - val_accuracy: 0.8949\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1704 - accuracy: 0.9406 - val_loss: 0.2486 - val_accuracy: 0.9013\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2027 - accuracy: 0.9318 - val_loss: 0.2423 - val_accuracy: 0.9140\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1077 - accuracy: 0.9576 - val_loss: 0.3535 - val_accuracy: 0.8854\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1224 - accuracy: 0.9537 - val_loss: 0.4838 - val_accuracy: 0.8471\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1463 - accuracy: 0.9487 - val_loss: 0.2436 - val_accuracy: 0.9013\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1766 - accuracy: 0.9392 - val_loss: 0.2298 - val_accuracy: 0.8854\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1279 - accuracy: 0.9558 - val_loss: 0.2515 - val_accuracy: 0.9108\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_19\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1360), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_136/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_544), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_136/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1361), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_136/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_545), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_136/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1362), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_136/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_546), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_136/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1369), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_136/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_547), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_136/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1370), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_137/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_548), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_137/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1371), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_137/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_549), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_137/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1372), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_137/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_550), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_137/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1379), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_137/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_551), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_137/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1380), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_138/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_552), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_138/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1381), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_138/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_553), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_138/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1382), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_138/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_554), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_138/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1389), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_138/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_555), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_138/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1390), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_139/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_556), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_139/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1391), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_139/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_557), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_139/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1392), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_139/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_558), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_139/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1399), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_139/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_559), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_139/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1400), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_140/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_560), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_140/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1401), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_140/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_561), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_140/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1402), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_140/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_562), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_140/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1409), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_140/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_563), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_140/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1410), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_141/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_564), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_141/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1411), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_141/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_565), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_141/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1412), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_141/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_566), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_141/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1419), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_141/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_567), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_141/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1420), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_142/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_568), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_142/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1421), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_142/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_569), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_142/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1422), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_142/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_570), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_142/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1429), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_142/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_571), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_142/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1430), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_143/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_572), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_143/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1431), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_143/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_573), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_143/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1432), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_143/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_574), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_143/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1439), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_143/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_575), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_143/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 45s 46ms/step - loss: 2.3863 - accuracy: 0.6838 - val_loss: 0.4737 - val_accuracy: 0.7873\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.5233 - accuracy: 0.7503 - val_loss: 0.4248 - val_accuracy: 0.8032\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.4748 - accuracy: 0.7750 - val_loss: 0.4133 - val_accuracy: 0.7524\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.4643 - accuracy: 0.7842 - val_loss: 0.4719 - val_accuracy: 0.7333\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.4481 - accuracy: 0.7867 - val_loss: 0.4055 - val_accuracy: 0.8000\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 31s 45ms/step - loss: 0.4317 - accuracy: 0.8132 - val_loss: 0.3420 - val_accuracy: 0.8222\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.3881 - accuracy: 0.8242 - val_loss: 0.3465 - val_accuracy: 0.8603\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.3761 - accuracy: 0.8267 - val_loss: 0.3762 - val_accuracy: 0.8571\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.3600 - accuracy: 0.8352 - val_loss: 0.3675 - val_accuracy: 0.8540\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.3393 - accuracy: 0.8475 - val_loss: 0.3923 - val_accuracy: 0.8381\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 31s 43ms/step - loss: 0.3199 - accuracy: 0.8585 - val_loss: 0.3186 - val_accuracy: 0.8571\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.3005 - accuracy: 0.8705 - val_loss: 0.3948 - val_accuracy: 0.8222\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 30s 43ms/step - loss: 0.3103 - accuracy: 0.8681 - val_loss: 0.3802 - val_accuracy: 0.8222\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 31s 43ms/step - loss: 0.3030 - accuracy: 0.8801 - val_loss: 0.3842 - val_accuracy: 0.8349\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 31s 43ms/step - loss: 0.2913 - accuracy: 0.8847 - val_loss: 0.3275 - val_accuracy: 0.8571\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.2077 - accuracy: 0.9109 - val_loss: 0.2897 - val_accuracy: 0.8794\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.2653 - accuracy: 0.8988 - val_loss: 0.2979 - val_accuracy: 0.8698\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.2542 - accuracy: 0.8988 - val_loss: 0.2679 - val_accuracy: 0.8889\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 31s 43ms/step - loss: 0.2306 - accuracy: 0.9066 - val_loss: 0.4507 - val_accuracy: 0.8254\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.2082 - accuracy: 0.9222 - val_loss: 0.3391 - val_accuracy: 0.8381\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1839 - accuracy: 0.9254 - val_loss: 0.3025 - val_accuracy: 0.8921\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1890 - accuracy: 0.9261 - val_loss: 0.3857 - val_accuracy: 0.8317\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1880 - accuracy: 0.9328 - val_loss: 0.3675 - val_accuracy: 0.8698\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1829 - accuracy: 0.9310 - val_loss: 0.3165 - val_accuracy: 0.8921\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 31s 45ms/step - loss: 0.1870 - accuracy: 0.9307 - val_loss: 0.3918 - val_accuracy: 0.8190\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.1615 - accuracy: 0.9416 - val_loss: 0.4107 - val_accuracy: 0.8508\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1912 - accuracy: 0.9317 - val_loss: 0.3625 - val_accuracy: 0.8698\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.2204 - accuracy: 0.9229 - val_loss: 0.3285 - val_accuracy: 0.8635\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1280 - accuracy: 0.9537 - val_loss: 0.4059 - val_accuracy: 0.8635\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1611 - accuracy: 0.9395 - val_loss: 0.4198 - val_accuracy: 0.8540\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1932 - accuracy: 0.9321 - val_loss: 0.3391 - val_accuracy: 0.8476\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.2034 - accuracy: 0.9339 - val_loss: 0.3589 - val_accuracy: 0.8381\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1495 - accuracy: 0.9374 - val_loss: 0.3654 - val_accuracy: 0.8730\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1684 - accuracy: 0.9367 - val_loss: 0.5639 - val_accuracy: 0.8571\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.1536 - accuracy: 0.9526 - val_loss: 0.2774 - val_accuracy: 0.8984\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.1549 - accuracy: 0.9540 - val_loss: 0.4173 - val_accuracy: 0.8730\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.1411 - accuracy: 0.9462 - val_loss: 0.4554 - val_accuracy: 0.8635\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1284 - accuracy: 0.9551 - val_loss: 0.2807 - val_accuracy: 0.8889\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.1442 - accuracy: 0.9430 - val_loss: 0.4220 - val_accuracy: 0.8540\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.1345 - accuracy: 0.9505 - val_loss: 0.3754 - val_accuracy: 0.8667\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1505 - accuracy: 0.9445 - val_loss: 0.4129 - val_accuracy: 0.8508\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 33s 46ms/step - loss: 0.1887 - accuracy: 0.9409 - val_loss: 0.3533 - val_accuracy: 0.8476\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.1625 - accuracy: 0.9445 - val_loss: 0.3174 - val_accuracy: 0.8698\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 32s 46ms/step - loss: 0.1849 - accuracy: 0.9374 - val_loss: 0.3036 - val_accuracy: 0.8825\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.0996 - accuracy: 0.9632 - val_loss: 0.3667 - val_accuracy: 0.8635\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.1471 - accuracy: 0.9533 - val_loss: 0.5461 - val_accuracy: 0.8476\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 32s 46ms/step - loss: 0.1123 - accuracy: 0.9558 - val_loss: 0.6313 - val_accuracy: 0.8381\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 31s 44ms/step - loss: 0.2490 - accuracy: 0.9098 - val_loss: 0.2866 - val_accuracy: 0.8857\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 32s 45ms/step - loss: 0.1642 - accuracy: 0.9416 - val_loss: 0.2884 - val_accuracy: 0.8984\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 33s 46ms/step - loss: 0.1276 - accuracy: 0.9579 - val_loss: 0.4164 - val_accuracy: 0.8857\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_20\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1440), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_144/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_576), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_144/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1441), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_144/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_577), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_144/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1442), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_144/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_578), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_144/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1449), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_144/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_579), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_144/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1450), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_145/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_580), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_145/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1451), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_145/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_581), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_145/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1452), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_145/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_582), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_145/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1459), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_145/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_583), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_145/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1460), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_146/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_584), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_146/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1461), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_146/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_585), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_146/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1462), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_146/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_586), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_146/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1469), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_146/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_587), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_146/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1470), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_147/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_588), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_147/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1471), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_147/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_589), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_147/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1472), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_147/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_590), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_147/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1479), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_147/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_591), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_147/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1480), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_148/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_592), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_148/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1481), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_148/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_593), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_148/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1482), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_148/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_594), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_148/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1489), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_148/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_595), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_148/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1490), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_149/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_596), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_149/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1491), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_149/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_597), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_149/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1492), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_149/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_598), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_149/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1499), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_149/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_599), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_149/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1500), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_150/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_600), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_150/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1501), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_150/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_601), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_150/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1502), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_150/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_602), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_150/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1509), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_150/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_603), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_150/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1510), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_151/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_604), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_151/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1511), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_151/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_605), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_151/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1512), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_151/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_606), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_151/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1519), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_151/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_607), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_151/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 30s 32ms/step - loss: 2.0788 - accuracy: 0.6839 - val_loss: 0.4867 - val_accuracy: 0.7866\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.5277 - accuracy: 0.7429 - val_loss: 0.4561 - val_accuracy: 0.7516\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4614 - accuracy: 0.7744 - val_loss: 0.4524 - val_accuracy: 0.7580\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4561 - accuracy: 0.7882 - val_loss: 0.9011 - val_accuracy: 0.6911\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4456 - accuracy: 0.7956 - val_loss: 0.4014 - val_accuracy: 0.7898\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 22s 30ms/step - loss: 0.4119 - accuracy: 0.8158 - val_loss: 0.5046 - val_accuracy: 0.7643\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 22s 30ms/step - loss: 0.4126 - accuracy: 0.8190 - val_loss: 0.4022 - val_accuracy: 0.8089\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3925 - accuracy: 0.8292 - val_loss: 0.4134 - val_accuracy: 0.7771\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3699 - accuracy: 0.8388 - val_loss: 0.4442 - val_accuracy: 0.8089\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3892 - accuracy: 0.8317 - val_loss: 0.4575 - val_accuracy: 0.8408\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3665 - accuracy: 0.8476 - val_loss: 0.3565 - val_accuracy: 0.8439\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 22s 30ms/step - loss: 0.2944 - accuracy: 0.8685 - val_loss: 0.4772 - val_accuracy: 0.8439\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2982 - accuracy: 0.8780 - val_loss: 0.3758 - val_accuracy: 0.8567\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.2666 - accuracy: 0.8847 - val_loss: 0.4304 - val_accuracy: 0.8185\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2680 - accuracy: 0.8890 - val_loss: 0.4213 - val_accuracy: 0.8248\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.2675 - accuracy: 0.8893 - val_loss: 0.3503 - val_accuracy: 0.8535\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2515 - accuracy: 0.8989 - val_loss: 0.3915 - val_accuracy: 0.8153\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.2246 - accuracy: 0.9081 - val_loss: 0.3272 - val_accuracy: 0.8439\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2249 - accuracy: 0.9077 - val_loss: 0.4407 - val_accuracy: 0.8280\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2109 - accuracy: 0.9187 - val_loss: 0.4821 - val_accuracy: 0.8185\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2118 - accuracy: 0.9204 - val_loss: 0.3135 - val_accuracy: 0.8694\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1921 - accuracy: 0.9282 - val_loss: 0.3713 - val_accuracy: 0.8567\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2238 - accuracy: 0.9074 - val_loss: 0.4297 - val_accuracy: 0.8471\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2799 - accuracy: 0.9042 - val_loss: 0.3293 - val_accuracy: 0.8535\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1833 - accuracy: 0.9335 - val_loss: 0.3823 - val_accuracy: 0.8471\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1773 - accuracy: 0.9303 - val_loss: 0.2803 - val_accuracy: 0.8694\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1775 - accuracy: 0.9296 - val_loss: 0.3744 - val_accuracy: 0.8439\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.1597 - accuracy: 0.9364 - val_loss: 0.3843 - val_accuracy: 0.8758\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1707 - accuracy: 0.9346 - val_loss: 0.3954 - val_accuracy: 0.8662\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 22s 30ms/step - loss: 0.1495 - accuracy: 0.9463 - val_loss: 0.5178 - val_accuracy: 0.8217\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2114 - accuracy: 0.9300 - val_loss: 0.4696 - val_accuracy: 0.7930\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2100 - accuracy: 0.9236 - val_loss: 0.3304 - val_accuracy: 0.9013\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1597 - accuracy: 0.9385 - val_loss: 0.3784 - val_accuracy: 0.8631\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1815 - accuracy: 0.9402 - val_loss: 0.2925 - val_accuracy: 0.8726\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1290 - accuracy: 0.9516 - val_loss: 0.2744 - val_accuracy: 0.8758\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1460 - accuracy: 0.9501 - val_loss: 0.3512 - val_accuracy: 0.8758\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2570 - accuracy: 0.9063 - val_loss: 0.3762 - val_accuracy: 0.8726\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1684 - accuracy: 0.9420 - val_loss: 0.2868 - val_accuracy: 0.8694\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2097 - accuracy: 0.9254 - val_loss: 0.3983 - val_accuracy: 0.8471\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1567 - accuracy: 0.9463 - val_loss: 0.7684 - val_accuracy: 0.8025\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1682 - accuracy: 0.9441 - val_loss: 0.4647 - val_accuracy: 0.8854\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1271 - accuracy: 0.9533 - val_loss: 0.3820 - val_accuracy: 0.8694\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1423 - accuracy: 0.9445 - val_loss: 0.4718 - val_accuracy: 0.8662\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1632 - accuracy: 0.9508 - val_loss: 1.4200 - val_accuracy: 0.3854\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1979 - accuracy: 0.9197 - val_loss: 0.3185 - val_accuracy: 0.8917\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2070 - accuracy: 0.9190 - val_loss: 0.4040 - val_accuracy: 0.8631\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2059 - accuracy: 0.9307 - val_loss: 0.3084 - val_accuracy: 0.8854\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1115 - accuracy: 0.9569 - val_loss: 0.3263 - val_accuracy: 0.8535\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1244 - accuracy: 0.9554 - val_loss: 0.4221 - val_accuracy: 0.8535\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1681 - accuracy: 0.9342 - val_loss: 0.3598 - val_accuracy: 0.9108\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_21\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1520), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_152/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_608), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_152/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1521), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_152/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_609), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_152/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1522), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_152/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_610), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_152/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1529), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_152/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_611), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_152/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1530), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_153/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_612), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_153/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1531), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_153/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_613), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_153/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1532), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_153/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_614), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_153/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1539), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_153/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_615), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_153/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1540), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_154/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_616), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_154/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1541), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_154/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_617), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_154/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1542), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_154/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_618), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_154/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1549), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_154/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_619), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_154/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1550), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_155/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_620), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_155/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1551), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_155/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_621), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_155/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1552), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_155/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_622), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_155/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1559), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_155/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_623), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_155/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1560), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_156/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_624), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_156/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1561), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_156/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_625), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_156/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1562), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_156/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_626), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_156/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1569), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_156/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_627), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_156/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1570), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_157/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_628), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_157/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1571), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_157/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_629), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_157/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1572), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_157/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_630), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_157/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1579), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_157/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_631), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_157/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1580), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_158/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_632), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_158/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1581), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_158/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_633), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_158/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1582), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_158/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_634), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_158/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1589), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_158/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_635), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_158/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1590), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_159/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_636), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_159/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1591), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_159/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_637), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_159/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1592), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_159/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_638), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_159/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1599), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_159/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_639), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_159/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 30s 31ms/step - loss: 2.5095 - accuracy: 0.6828 - val_loss: 0.4823 - val_accuracy: 0.7611\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.5306 - accuracy: 0.7542 - val_loss: 0.4328 - val_accuracy: 0.7771\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4776 - accuracy: 0.7726 - val_loss: 0.4525 - val_accuracy: 0.7707\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4375 - accuracy: 0.7967 - val_loss: 0.4945 - val_accuracy: 0.7707\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4392 - accuracy: 0.7921 - val_loss: 0.4400 - val_accuracy: 0.7866\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4181 - accuracy: 0.8083 - val_loss: 0.3893 - val_accuracy: 0.7898\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4024 - accuracy: 0.8204 - val_loss: 0.4313 - val_accuracy: 0.7611\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.3938 - accuracy: 0.8260 - val_loss: 0.4020 - val_accuracy: 0.7803\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 22s 30ms/step - loss: 0.3365 - accuracy: 0.8434 - val_loss: 0.4865 - val_accuracy: 0.7420\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3416 - accuracy: 0.8529 - val_loss: 0.5223 - val_accuracy: 0.7930\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3295 - accuracy: 0.8561 - val_loss: 0.4128 - val_accuracy: 0.8376\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3133 - accuracy: 0.8663 - val_loss: 0.3616 - val_accuracy: 0.8312\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2812 - accuracy: 0.8794 - val_loss: 0.3771 - val_accuracy: 0.8089\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2705 - accuracy: 0.8808 - val_loss: 0.3911 - val_accuracy: 0.8312\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2914 - accuracy: 0.8791 - val_loss: 0.4538 - val_accuracy: 0.8153\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2781 - accuracy: 0.8911 - val_loss: 0.4328 - val_accuracy: 0.8185\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2901 - accuracy: 0.8925 - val_loss: 0.6586 - val_accuracy: 0.8057\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2663 - accuracy: 0.9006 - val_loss: 0.4814 - val_accuracy: 0.7834\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2361 - accuracy: 0.9070 - val_loss: 0.3099 - val_accuracy: 0.8662\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2116 - accuracy: 0.9183 - val_loss: 0.4204 - val_accuracy: 0.8471\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2281 - accuracy: 0.9162 - val_loss: 0.4183 - val_accuracy: 0.8376\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2134 - accuracy: 0.9137 - val_loss: 0.4160 - val_accuracy: 0.8025\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2026 - accuracy: 0.9240 - val_loss: 0.4118 - val_accuracy: 0.8471\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2125 - accuracy: 0.9183 - val_loss: 0.3763 - val_accuracy: 0.8599\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1989 - accuracy: 0.9240 - val_loss: 0.4529 - val_accuracy: 0.8471\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1774 - accuracy: 0.9349 - val_loss: 0.3054 - val_accuracy: 0.8790\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1506 - accuracy: 0.9417 - val_loss: 0.5807 - val_accuracy: 0.7994\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2280 - accuracy: 0.9208 - val_loss: 0.4365 - val_accuracy: 0.8535\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1656 - accuracy: 0.9381 - val_loss: 0.3507 - val_accuracy: 0.8662\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1611 - accuracy: 0.9406 - val_loss: 0.4251 - val_accuracy: 0.8662\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1604 - accuracy: 0.9364 - val_loss: 0.3972 - val_accuracy: 0.8503\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1434 - accuracy: 0.9487 - val_loss: 0.4134 - val_accuracy: 0.8439\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2223 - accuracy: 0.9233 - val_loss: 0.3720 - val_accuracy: 0.8471\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1692 - accuracy: 0.9353 - val_loss: 0.4282 - val_accuracy: 0.8439\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1493 - accuracy: 0.9448 - val_loss: 0.4041 - val_accuracy: 0.8694\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2102 - accuracy: 0.9257 - val_loss: 0.4163 - val_accuracy: 0.8758\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1309 - accuracy: 0.9508 - val_loss: 0.4297 - val_accuracy: 0.8503\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1596 - accuracy: 0.9470 - val_loss: 0.3012 - val_accuracy: 0.8567\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1647 - accuracy: 0.9378 - val_loss: 0.2611 - val_accuracy: 0.8917\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2055 - accuracy: 0.9275 - val_loss: 0.3102 - val_accuracy: 0.8694\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1340 - accuracy: 0.9551 - val_loss: 0.3783 - val_accuracy: 0.8854\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1431 - accuracy: 0.9470 - val_loss: 0.3460 - val_accuracy: 0.8439\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1798 - accuracy: 0.9434 - val_loss: 0.3449 - val_accuracy: 0.8822\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1530 - accuracy: 0.9452 - val_loss: 0.2727 - val_accuracy: 0.8854\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2610 - accuracy: 0.9169 - val_loss: 0.3076 - val_accuracy: 0.8726\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1981 - accuracy: 0.9261 - val_loss: 0.2994 - val_accuracy: 0.8885\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1626 - accuracy: 0.9463 - val_loss: 0.3494 - val_accuracy: 0.8662\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1562 - accuracy: 0.9473 - val_loss: 0.3017 - val_accuracy: 0.8822\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1641 - accuracy: 0.9473 - val_loss: 0.5039 - val_accuracy: 0.8567\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.1468 - accuracy: 0.9498 - val_loss: 0.5052 - val_accuracy: 0.8567\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_22\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1600), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_160/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_640), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_160/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1601), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_160/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_641), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_160/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1602), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_160/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_642), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_160/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1609), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_160/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_643), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_160/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1610), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_161/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_644), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_161/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1611), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_161/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_645), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_161/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1612), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_161/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_646), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_161/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1619), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_161/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_647), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_161/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1620), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_162/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_648), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_162/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1621), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_162/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_649), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_162/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1622), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_162/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_650), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_162/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1629), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_162/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_651), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_162/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1630), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_163/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_652), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_163/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1631), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_163/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_653), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_163/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1632), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_163/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_654), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_163/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1639), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_163/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_655), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_163/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1640), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_164/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_656), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_164/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1641), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_164/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_657), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_164/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1642), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_164/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_658), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_164/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1649), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_164/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_659), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_164/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1650), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_165/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_660), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_165/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1651), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_165/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_661), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_165/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1652), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_165/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_662), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_165/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1659), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_165/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_663), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_165/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1660), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_166/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_664), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_166/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1661), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_166/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_665), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_166/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1662), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_166/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_666), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_166/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1669), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_166/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_667), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_166/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1670), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_167/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_668), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_167/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1671), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_167/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_669), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_167/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1672), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_167/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_670), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_167/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1679), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_167/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_671), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_167/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 30s 32ms/step - loss: 2.1515 - accuracy: 0.6729 - val_loss: 0.4756 - val_accuracy: 0.7707\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.5355 - accuracy: 0.7458 - val_loss: 0.4016 - val_accuracy: 0.7994\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.4720 - accuracy: 0.7680 - val_loss: 0.4325 - val_accuracy: 0.7898\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4775 - accuracy: 0.7815 - val_loss: 0.5270 - val_accuracy: 0.7548\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4446 - accuracy: 0.7917 - val_loss: 0.3484 - val_accuracy: 0.8631\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4150 - accuracy: 0.8073 - val_loss: 0.3070 - val_accuracy: 0.8567\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3898 - accuracy: 0.8197 - val_loss: 0.3846 - val_accuracy: 0.8503\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3888 - accuracy: 0.8200 - val_loss: 0.3128 - val_accuracy: 0.8535\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3691 - accuracy: 0.8317 - val_loss: 0.2745 - val_accuracy: 0.8662\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.3363 - accuracy: 0.8536 - val_loss: 0.3760 - val_accuracy: 0.8344\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3506 - accuracy: 0.8561 - val_loss: 0.2703 - val_accuracy: 0.8885\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3107 - accuracy: 0.8617 - val_loss: 0.2952 - val_accuracy: 0.8726\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3138 - accuracy: 0.8649 - val_loss: 0.2761 - val_accuracy: 0.8758\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2899 - accuracy: 0.8798 - val_loss: 0.2561 - val_accuracy: 0.9013\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2715 - accuracy: 0.8872 - val_loss: 0.3346 - val_accuracy: 0.8121\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3088 - accuracy: 0.8755 - val_loss: 0.2749 - val_accuracy: 0.8790\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2635 - accuracy: 0.8918 - val_loss: 0.2275 - val_accuracy: 0.9172\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2340 - accuracy: 0.9042 - val_loss: 0.2597 - val_accuracy: 0.8949\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2139 - accuracy: 0.9098 - val_loss: 0.2519 - val_accuracy: 0.8854\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2343 - accuracy: 0.9017 - val_loss: 0.2637 - val_accuracy: 0.8949\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2195 - accuracy: 0.9158 - val_loss: 0.2526 - val_accuracy: 0.9108\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.2045 - accuracy: 0.9190 - val_loss: 0.2382 - val_accuracy: 0.9076\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3119 - accuracy: 0.8964 - val_loss: 0.2917 - val_accuracy: 0.8885\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1824 - accuracy: 0.9300 - val_loss: 0.2033 - val_accuracy: 0.9299\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 20s 28ms/step - loss: 0.1797 - accuracy: 0.9328 - val_loss: 0.2714 - val_accuracy: 0.8949\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.4021 - accuracy: 0.8854 - val_loss: 0.2116 - val_accuracy: 0.9236\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1663 - accuracy: 0.9279 - val_loss: 0.3030 - val_accuracy: 0.8822\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1896 - accuracy: 0.9346 - val_loss: 0.2047 - val_accuracy: 0.9268\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2106 - accuracy: 0.9169 - val_loss: 0.2041 - val_accuracy: 0.9045\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1773 - accuracy: 0.9353 - val_loss: 0.2143 - val_accuracy: 0.9395\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1498 - accuracy: 0.9477 - val_loss: 0.2063 - val_accuracy: 0.9236\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1579 - accuracy: 0.9427 - val_loss: 0.4224 - val_accuracy: 0.9172\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2309 - accuracy: 0.9165 - val_loss: 0.3228 - val_accuracy: 0.8917\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.1988 - accuracy: 0.9321 - val_loss: 0.2186 - val_accuracy: 0.9236\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1660 - accuracy: 0.9424 - val_loss: 0.2541 - val_accuracy: 0.9076\n",
            "Epoch 36/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1472 - accuracy: 0.9427 - val_loss: 0.2219 - val_accuracy: 0.9299\n",
            "Epoch 37/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1591 - accuracy: 0.9484 - val_loss: 0.2674 - val_accuracy: 0.8981\n",
            "Epoch 38/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1533 - accuracy: 0.9431 - val_loss: 0.2257 - val_accuracy: 0.9331\n",
            "Epoch 39/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1753 - accuracy: 0.9364 - val_loss: 0.2153 - val_accuracy: 0.9108\n",
            "Epoch 40/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1625 - accuracy: 0.9406 - val_loss: 0.2229 - val_accuracy: 0.9140\n",
            "Epoch 41/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.1738 - accuracy: 0.9332 - val_loss: 0.2295 - val_accuracy: 0.9204\n",
            "Epoch 42/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2681 - accuracy: 0.9148 - val_loss: 0.1900 - val_accuracy: 0.9108\n",
            "Epoch 43/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1442 - accuracy: 0.9537 - val_loss: 0.2410 - val_accuracy: 0.9268\n",
            "Epoch 44/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1154 - accuracy: 0.9576 - val_loss: 0.3094 - val_accuracy: 0.8885\n",
            "Epoch 45/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2257 - accuracy: 0.9349 - val_loss: 0.2851 - val_accuracy: 0.8694\n",
            "Epoch 46/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1742 - accuracy: 0.9395 - val_loss: 0.3171 - val_accuracy: 0.9204\n",
            "Epoch 47/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1827 - accuracy: 0.9385 - val_loss: 0.2198 - val_accuracy: 0.9172\n",
            "Epoch 48/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1997 - accuracy: 0.9356 - val_loss: 0.2229 - val_accuracy: 0.9204\n",
            "Epoch 49/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.1728 - accuracy: 0.9470 - val_loss: 0.3108 - val_accuracy: 0.8631\n",
            "Epoch 50/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2525 - accuracy: 0.9211 - val_loss: 0.2626 - val_accuracy: 0.8854\n",
            "clasification_t_FLAIR_None_None_Transformer_adadelta_categorical_crossentropy_run_23\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1680), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_168/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_672), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_168/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1681), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_168/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_673), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_168/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1682), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_168/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_674), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_168/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1689), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_168/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_675), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_168/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1690), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_169/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_676), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_169/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1691), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_169/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_677), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_169/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1692), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_169/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_678), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_169/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1699), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_169/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_679), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_169/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1700), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_170/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_680), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_170/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1701), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_170/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_681), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_170/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1702), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_170/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_682), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_170/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1709), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_170/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_683), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_170/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1710), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_171/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_684), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_171/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1711), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_171/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_685), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_171/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1712), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_171/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_686), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_171/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1719), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_171/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_687), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_171/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1720), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_172/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_688), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_172/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1721), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_172/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_689), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_172/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1722), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_172/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_690), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_172/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1729), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_172/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_691), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_172/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1730), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_173/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_692), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_173/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1731), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_173/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_693), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_173/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1732), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_173/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_694), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_173/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1739), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_173/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_695), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_173/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1740), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_174/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_696), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_174/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1741), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_174/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_697), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_174/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1742), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_174/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_698), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_174/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1749), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_174/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_699), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_174/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1750), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_175/query/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_700), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_175/query/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1751), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_175/key/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_701), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_175/key/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1752), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_175/value/kernel:0' shape=(64, 4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_702), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_175/value/bias:0' shape=(4, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.einsum_1759), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_175/attention_output/kernel:0' shape=(4, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_703), but are not present in its tracked objects:   <tf.Variable 'multi_head_attention2_175/attention_output/bias:0' shape=(64,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "Epoch 1/50\n",
            "707/707 [==============================] - 30s 32ms/step - loss: 2.3405 - accuracy: 0.6885 - val_loss: 0.5061 - val_accuracy: 0.7293\n",
            "Epoch 2/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.5292 - accuracy: 0.7532 - val_loss: 0.4373 - val_accuracy: 0.7803\n",
            "Epoch 3/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4900 - accuracy: 0.7744 - val_loss: 0.4670 - val_accuracy: 0.7580\n",
            "Epoch 4/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4570 - accuracy: 0.7889 - val_loss: 0.4198 - val_accuracy: 0.7803\n",
            "Epoch 5/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.4337 - accuracy: 0.7988 - val_loss: 0.5578 - val_accuracy: 0.7580\n",
            "Epoch 6/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.4241 - accuracy: 0.8009 - val_loss: 0.5331 - val_accuracy: 0.7357\n",
            "Epoch 7/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.4091 - accuracy: 0.8151 - val_loss: 0.4303 - val_accuracy: 0.7930\n",
            "Epoch 8/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3880 - accuracy: 0.8214 - val_loss: 0.3989 - val_accuracy: 0.8217\n",
            "Epoch 9/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3862 - accuracy: 0.8285 - val_loss: 0.5025 - val_accuracy: 0.7930\n",
            "Epoch 10/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.3631 - accuracy: 0.8444 - val_loss: 0.4132 - val_accuracy: 0.8344\n",
            "Epoch 11/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.3322 - accuracy: 0.8472 - val_loss: 0.4868 - val_accuracy: 0.7866\n",
            "Epoch 12/50\n",
            "707/707 [==============================] - 22s 30ms/step - loss: 0.2993 - accuracy: 0.8727 - val_loss: 0.3888 - val_accuracy: 0.7962\n",
            "Epoch 13/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2969 - accuracy: 0.8826 - val_loss: 0.3731 - val_accuracy: 0.8280\n",
            "Epoch 14/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2568 - accuracy: 0.8897 - val_loss: 0.4486 - val_accuracy: 0.8503\n",
            "Epoch 15/50\n",
            "707/707 [==============================] - 20s 29ms/step - loss: 0.2651 - accuracy: 0.8865 - val_loss: 0.4531 - val_accuracy: 0.8217\n",
            "Epoch 16/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2331 - accuracy: 0.9049 - val_loss: 0.5290 - val_accuracy: 0.8376\n",
            "Epoch 17/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2473 - accuracy: 0.8978 - val_loss: 0.4303 - val_accuracy: 0.7739\n",
            "Epoch 18/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2298 - accuracy: 0.9120 - val_loss: 0.3005 - val_accuracy: 0.8662\n",
            "Epoch 19/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2649 - accuracy: 0.9042 - val_loss: 0.3890 - val_accuracy: 0.8280\n",
            "Epoch 20/50\n",
            "707/707 [==============================] - 21s 29ms/step - loss: 0.2405 - accuracy: 0.9035 - val_loss: 0.4344 - val_accuracy: 0.8439\n",
            "Epoch 21/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.3036 - accuracy: 0.9031 - val_loss: 0.4503 - val_accuracy: 0.8025\n",
            "Epoch 22/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1842 - accuracy: 0.9243 - val_loss: 0.3702 - val_accuracy: 0.8758\n",
            "Epoch 23/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1695 - accuracy: 0.9378 - val_loss: 0.4054 - val_accuracy: 0.8408\n",
            "Epoch 24/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.1513 - accuracy: 0.9438 - val_loss: 0.3563 - val_accuracy: 0.8535\n",
            "Epoch 25/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2105 - accuracy: 0.9240 - val_loss: 1.1784 - val_accuracy: 0.6847\n",
            "Epoch 26/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.2462 - accuracy: 0.9254 - val_loss: 0.6134 - val_accuracy: 0.8153\n",
            "Epoch 27/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1477 - accuracy: 0.9445 - val_loss: 0.7398 - val_accuracy: 0.8217\n",
            "Epoch 28/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1321 - accuracy: 0.9494 - val_loss: 0.5170 - val_accuracy: 0.8248\n",
            "Epoch 29/50\n",
            "707/707 [==============================] - 22s 31ms/step - loss: 0.1608 - accuracy: 0.9360 - val_loss: 0.4582 - val_accuracy: 0.8503\n",
            "Epoch 30/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1570 - accuracy: 0.9413 - val_loss: 0.4155 - val_accuracy: 0.8726\n",
            "Epoch 31/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2027 - accuracy: 0.9335 - val_loss: 0.4483 - val_accuracy: 0.8567\n",
            "Epoch 32/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.2432 - accuracy: 0.9116 - val_loss: 0.5367 - val_accuracy: 0.8408\n",
            "Epoch 33/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1951 - accuracy: 0.9300 - val_loss: 0.3956 - val_accuracy: 0.8376\n",
            "Epoch 34/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1456 - accuracy: 0.9420 - val_loss: 0.4177 - val_accuracy: 0.8599\n",
            "Epoch 35/50\n",
            "707/707 [==============================] - 21s 30ms/step - loss: 0.1222 - accuracy: 0.9565 - val_loss: 0.4081 - val_accuracy: 0.8790\n",
            "Epoch 36/50\n",
            "620/707 [=========================>....] - ETA: 2s - loss: 0.2235 - accuracy: 0.9278"
          ]
        }
      ]
    }
  ]
}